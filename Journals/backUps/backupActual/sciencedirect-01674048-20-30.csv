sep=|
volume|issue|url|title|abstract
21|1|http://www.sciencedirect.com/science/journal/01674048/21/1|From the editor-in-chief|
21|1||Security Views|
21|1||2001: A Privacy Odyssey Revisited|
21|1||What InfoSec professionals should know about information warfare tactics by terrorists|
21|1||In brief|
21|1||Abstracts of recent articles and literature|
21|1||Calendar of forthcoming conferences and events|
21|1||ACSAC 2001 review|
21|1||International board of referees|
21|1||Guide for Authors|
21|1||Insider Threat Prediction Tool: Evaluating the probability of IT misuse|Despite the well documented and emerging insider threat to information systems, there is currently no substantial effort devoted to addressing the problem of internal IT misuse. In fact, the great majority of misuse counter measures address forms of abuse originating from external factors (i.e. the perceived threat from unauthorized users). This paper suggests a new and innovative approach of dealing with insiders that abuse IT systems. The proposed solution estimates the level of threat that is likely to originate from a particular insider by introducing a threat evaluation system based on certain profiles of user behaviour. However, a substantial amount of work is required, in order to materialize and validate the proposed solutions.
21|1||Cryptanalysis of a Timestamp-Based Password Authentication Scheme|In this paper, we present a cryptanalysis of a timestamp-based password authentication scheme which is based on the concepts of ID-based schemes and smart cards. We show that the scheme is breakable. An intruder is able to construct forged login request from intercepted login requests and then he can impersonate other legal users and pass the system authentication.
21|1||Hybrid Key Escrow: A New Paradigm*,â |A new paradigm for the design of key recovery systems called hybrid key escrow will be presented. It will be shown that such a design can guard the privacy of system users and at the same time enable authorized key recovery. The system will be analyzed against the three fundamental properties of any robust key recovery system: compliance, enforceability and traceability.
21|1||IFIP technical committee 11|
21|2|http://www.sciencedirect.com/science/journal/01674048/21/2|From the Editor-in-chief|In my last editorial I discussed the foundations of information security, focusing on some of the things that information security professionals need to do to be successful in securing systems and networks. Although we can choose our approach and focus, the truth is that we have less control over other factors that affect our success. The information security function may, for example, be placed in a lower tier of an organization chart (giving it little leverage and influence within the organization to which it belongs) or it may be allocated too few resources to enable it to accomplish what needs to get done.
21|2||Security Views|
21|2||What InfoSec Professionals Should Know About Information Warfare Tactics by Terrorists|
21|2||Security Insights|Predicting the future is never easy, but in this first of a series of interviews with infosecurity professionals we ask them to do just that. In this issue, Bob Blakley, Chief Scientist for Tivoli, an IBM company, speaks about his priorities for the year ahead.
21|2||In brief|
21|2||Abstracts of Recent Articles & Literature|
21|2||Events|
21|2||Author Index for Volume 20|
21|2||Subject Index for Volume 20|
21|2||Refereed Papers|
21|2||Guide for Authors|
21|2||The Benefits of a Notification Process in Addressing the Worsening Computer Virus Problem: Results of a Survey and a Simulation Model|Computer viruses present an increasing risk to the integrity of information systems and the functions of a modern business enterprise. Systematic study of this problem can yield better indicators of the impact of computer viruses as well as a better understanding of strategies to reduce that impact.
21|2||Erratum|
21|2||A Novel Key Management Scheme Based on Discrete Logarithms and Polynomial Interpolations|The authors present a novel cryptographic key assignment scheme in order to solve the dynamic access control problems in a partially ordered user hierarchy. By using the scheme based on the discrete logarithms and Newton’s polynomial interpolations, each security class is assigned a secret key that can be used to derive his successors’ secret keys efficiently. The dynamic key management problems, such as adding/deleting classes, adding/deleting relationships, and changing secret keys, are discussed in detail. Moreover, it is unnecessary to consider the status of the security class at lower levels because any user can freely change his/her own secret key for some security reasons.
21|2||Development of Information Security Baselines for Healthcare Information Systems in New Zealand|In 1996 New Zealand had introduced security standard AS/NZCS 4444 based on the British Standard BS 7799, which has recently been accepted as an international standard ISO 17799. This standard is very often referred to as the ‘baseline lane approach’ to the issue of managing information security. On the other hand the health information systems (HIS) are undergoing rapid development both in the number of installed systems as in the law and regulations governing HIS developments and deployment. The project was aimed at reviewing the AS/NZCS 4444 standard from the HIS requirements point of view. In this paper, we began with an overview of healthcare information systems (HIS) infrastructure in New Zealand and associated security issues around privacy and confidentiality, followed by a general review of the security baseline approach. We analyzed each clause of the AS/NZS 4444 with the information collected about technical and non-technical approaches to protecting HIS, consisting of a series of multi-case studies of healthcare organizations that collect, process, store and transmit electronic medical records. Finally, we proposed a new set of information security baselines based on the research to build an information security model for healthcare organizations.
21|2||IFIP Technical Committee|
21|3|http://www.sciencedirect.com/science/journal/01674048/21/3|From the editor-in-chief|
21|3||Security Views|
21|3||It Was DÃ©jÃ  vu all Over Again|
21|3||Acceptance of Subscriber Authentication Methods For Mobile Telephony Devices|Mobile phones are now an accepted part of everyday life, with users becoming more reliant on the services that they can provide. In the vast majority of systems, the only security to prevent unauthorized use of the handset is a four digit Personal Identification Number (PIN). This paper presents the findings of a survey into the opinions of subscribers regarding the need for security in mobile devices, their use of current methods, and their attitudes towards alternative approaches that could be employed in the future. It is concluded that, although the need for security is understood and appreciated, the current PIN-based approach is under-utilized and can, therefore, be considered to provide inadequate protection in many cases. Surveyed users responded positively towards alternative methods of authentication, such as fingerprint scanning and voice verification. Based upon these findings, the paper concludes that a non-intrusive, and possibly hybrid, method of authentication (using a combination of techniques) would best satisfy the needs of future subscribers.
21|3||On Bricks and Walls: Why Building Secure Software is Hard|
21|3||Anti-Terrorism Legislation: The Impact on The Processing of Data|
21|3||In brief|
21|3||Abstracts of Recent Articles & Literature|
21|3||Events|
21|3||International Board of Referees|
21|3||Guide for authors|
21|3||On the Security of Todayâs Online Electronic Banking Systems|Current technology is evolving fast and is constantly bringing new dimensions to our daily life. Electronic banking systems provide us with easy access to banking services. The interaction between user and bank has been substantially improved by deploying ATMs, phone banking, Internet banking, and more recently, mobile banking. This paper discusses the security of today’s electronic banking systems. We focus on Internet and mobile banking and present an overview and evaluation of the techniques that are used in the current systems. The best practice is indicated, together with improvements for the future. The issues discussed in this paper are generally applicable in other electronic services such as E-commerce and E-government.
21|3||A Prototype for Assessing Information Technology Risks in Health Care|Although a vast number of risk-management methodologies have been proposed thus far and even though these methodologies are being applied to all types of organizations quite effectively, a few concerns are raised when the self-same risk-management methodologies are applied to the health-care environment. The authors, therefore, developed a risk-management methodology, entitled “Risk Management in Health Care — using cognitive fuzzy techniques” (RiMaHCoF), that is specifically tailored for the health-care environment. The methodology comprises five successive stages in all, namely initiation, domain analysis, risk assessment, risk analysis and domain monitoring. In the present paper, however, the authors will focus only on the third stage, viz. the risk assessment stage.
21|3||IFIP technical committee 11|
21|4|http://www.sciencedirect.com/science/journal/01674048/21/4|The Sorry State of Law Enforcement|
21|4||Security views|
21|4||Why access control is difficult|
21|4||Security surveys spring crop|What does the latest round of IT security surveys have to tell us? Stephen Hinde samples, savours and pronounces.
21|4||The Klez.H worm dissected|
21|4||The emergence of a comprehensive obligation towards computer security|
21|4||Syndicated crime and international terrorism: the lessons of â9â11â|
21|4||In Brief|
21|4||Digest of recent IT security press coverage|
21|4||Calendar of forthcoming conferences and events|
21|4||Guide for authors|
21|4||International Board of Referees|
21|4||Integrating Software Lifecycle Process Standards with Security Engineering|Since the advent and astronomical rise of the Internet and E-business, organizations must secure their computer systems or risk malicious attacks. While there have been several software lifecycle process standards (SLPS) for both military and industrial software development, their activities and deliverables are not yet integrated with security engineering (SE) activities. This lack of integration has created conflicts among the system development stakeholders (e.g. system acquirers and developers) during secure information systems development projects. This paper proposes an integration model that interweaves all the process activities and deliverables of SLPS with SE activities, taking IEEE/EIA 12207 as an example of SLPS. This model provides practical guidelines for the development of secure information systems while informing stakeholders how SLPS is related to the SE activities.
21|4||The Development of Access Control Policies for Information Technology Systems|The identification of the major information technology (IT) access control policies is required to direct “best practice” approaches within the IT security program of an organisation. In demonstrating the need for security access control policies in the IT security program, it highlights the significant shift away from centralised mainframes towards distributed networked computing environments. The study showed that the traditional and proven security control mechanisms used in the mainframe environments were not applicable to distributed systems, and as a result, a number of inherent risks were identified with the new technologies.
21|4||An Efficient and Practical Solution to Remote Authentication: Smart Card|The smart card-based scheme is a very promising and practical solution to remote authentication. Compared with other smart card-based schemes, our solution achieves more functionality and requires much less computational cost. These important merits include: (1) there is no verification table; (2) users can freely choose their passwords; (3) the communication cost and the computational cost is very low; and (4) it provides mutual authentication between the user and the server.
21|4||IFIP technical committee 11|
21|5|http://www.sciencedirect.com/science/journal/01674048/21/5|Taking a stand on hackers|What should our relationship with the hacking community be? You may remember a news item in last month’s ‘Security Views’ that concerned ‘ethical hacking’ and ‘benevolent hacking’. Recall, please, that a young man from India who once defaced a website and then told the owner how to close the site to attacks was later hired as a security consultant by a US Government agency. Then the ‘Dynamic Duo’ hacked the FAA to expose its vulnerability to external attack. Somehow, I cannot get these stories and the issues behind them out of my mind, so I’ll wrestle with them in this month’s editorial. The main issue to be addressed is the type and level of relationship with the hacking community we as information security professionals should maintain.
21|5||Security Views|
21|5||Security crisis management â the basics|Of the more pervasive problems in any kind of security event is how the security event is managed from the inception to the end. There’s a lot written about how to manage a specific incident or how to deal with a point problem such as a firewall log, but little tends to be written about how to deal with the management of a security event as part of corporate crisis management. This article will focus on the basics of security crisis management and of the logical steps required to ensure that a security crisis does not get out of hand.
21|5||Information security policy â what do international information security standards say?|One of the most important information security controls, is the information security policy. This vital direction-giving document is, however, not always easy to develop and the authors thereof battle with questions such as what constitutes a policy. This results in the policy authors turning to existing sources for guidance. One of these sources is the various international information security standards. These standards are a good starting point for determining what the information security policy should consist of, but should not be relied upon exclusively for guidance. Firstly, they are not comprehensive in their coverage and furthermore, tending to rather address the processes needed for successfully implementing the information security policy. It is far more important the information security policy must fit in with the organisation’s culture and must therefore be developed with this in mind.
21|5||Much Ado About Nothing: Win32.Perrun|
21|5||Trusted â¦orâ¦ trustworthy: the search for a new paradigm for computer and network security1|On the occasion of the presentation of the Kristian Beckman Award for 2002 it is appropriate to pause and reflect on the state of computer and associated data network security at the start of the new millennium; appropriately in a country that itself pioneered the use of encryption some thousands of years ago. This paper sets out a number of major questions and challenges which include:
21|5||Enterprise in focus at NetSec 2002|NetSec 2002 took place in San Francisco, amid industry reflection on the balance to be struck between combatting cyber-terrorism and safeguarding civil liberties post-9-11. Brian McKenna reports on the punditry and the pedagogy at the CSI’s event, focusing on security in the enterprise.
21|5||The perils of privacy|
21|5||Digest of recent IT security press coverage|
21|5||In brief|
21|5||Events|
21|5||Guide for Authors|
21|5||International Board of Referees|
21|5||Use of K-Nearest Neighbor classifier for intrusion detection1|A new approach, based on the k-Nearest Neighbor (kNN) classifier, is used to classify program behavior as normal or intrusive. Program behavior, in turn, is represented by frequencies of system calls. Each system call is treated as a word and the collection of system calls over each program execution as a document. These documents are then classified using kNN classifier, a popular method in text categorization. This method seems to offer some computational advantages over those that seek to characterize program behavior with short sequences of system calls and generate individual program profiles. Preliminary experiments with 1998 DARPA BSM audit data show that the kNN classifier can effectively detect intrusive attacks and achieve a low false positive rate.
21|5||Steganographic Method for Secure Communications|Cryptographic methods secure an important message by encrypting it to an unrecognized form of data which may arouse the interest of cryptanalysis for part of the recipients. Steganographic methods hide the encrypted message in cover carriers so that it cannot be seen while it is transmitted on public communication channels such as computer network. Many steganogrphic methods embed a large amount of the secret information in the first k LSBs of the pixels of the cover images. Because of the imperfect sensibility of the human visual system, the existence of the embedded secret information can be imperceptible. Unfortunately, the hidden secret information may be discovered by the common-cover-carrier attack if it has not been appropriately disposed. In this paper, an LSB-based steganographic method is proposed to resolve this problem. By using variable-size insertion and redundant Gaussion noise adding, the stego-images created with the proposed method can survive both the human visual system and the common-cover-carrier attack. Moreover, many cryptographic protocols are involved in the proposed method to resolve the problems of security and key management that may be encountered in other steganogrpahic methods. The proposed method is hence suitable for secure communications.
21|5||The Open Source approach â opportunities and limitations with respect to security and privacy*|Today’s software often does not fulfil basic security or privacy requirements. Some people regard the Open Source paradigm as the solution to this problem. First, we carefully explain the security and privacy aspects of Open Source, which in particular offer the possibility for a dramatic increase in trustworthiness for and autonomy of the user. We show which expectations for an improvement of the software trustworthiness dilemma are realistic. Finally, we describe measures necessary for developing secure and trustworthy Open Source systems.
21|5||IFIP Technical committee 11|
21|6|http://www.sciencedirect.com/science/journal/01674048/21/6|From the editor-in-chief: The Revenger's Tragedy|
21|6||Security Views|
21|6||The Blue Screen of Death and other deadly bugs|
21|6||I-Worm.Lentin (aka Yaha)|
21|6||The USâs National Strategy for Homeland Security|This article briefly reviews the National Strategy which defines the US’s approach to counter future terrorist activity. In the context of computer security and privacy, the chapters on ‘Protecting Critical Infrastructure’, ‘Information Sharing and Systems’ and ‘Law’ are particularly relevant. The Strategy is also important, because all western democracies will be considering the same counter-terrorism imperatives, and are likely to consider similar security enhancing mechanisms as proposed in the Strategy.
21|6||Policy enforcement in the workplace|It is well known, at least among true security professionals, that formal policy is a prerequisite of security. While many organizations have security policy of varying types, having policy and being able to enforce it are totally different things. This writing looks at the importance of formal security policy, and then presents and discusses a new set of tools that provide a ready method for assuring critical policy enforcement in the workplace.
21|6||White collar crime: a handmaiden of international tech terrorism|
21|6||Palladium, fraud, and surviving terrorism â Compsec 2002: Preview of Compsec 2002, 30 Octâ1 Nov, Queen Elizabeth II Conference Centre, Westminster, London, UK|This year’s Compsec addresses security issues brought to the fore by the terrorist attacks and frauds that have rocked the US in the last year. Sarah Hilley, Editor of Computer Fraud & Security, and Network Security previews the event.
21|6||A framework for understanding and predicting insider attacks|In this paper an insider attack is considered to be deliberate misuse by those who are authorized to use computers and networks. Applying this definition in real-life settings to determine whether or not an attack was caused by an insider is often, however, anything but straightforward. We know very little about insider attacks, and misconceptions concerning insider attacks abound. The belief that “most attacks come from inside” is held by many information security professionals, for example, even though empirical statistics and firewall logs indicate otherwise. This paper presents a framework based on previous studies and models of insider behavior as well as first-hand experience in dealing with insider attacks. This framework defines relevant types of insider attack-related behaviors and symptoms—“indicators” that include deliberate markers, meaningful errors, preparatory behaviors, correlated usage patterns, verbal behavior and personality traits. From these sets of indicators, clues can be pieced together to predict and detect an attack. The presence of numerous small clues necessitates the use of quantitative methods; multiple regression equations appear to be a particularly promising approach for quantifying prediction.
21|6||Digest of recent IT security press coverage|
21|6||in brief|
21|6||Events|
21|6||Refereed papers|
21|6||Principles and requirements for a secure e-voting system|Electronic voting (e-voting) is considered a means to further enhance and strengthen the democratic processes in modern information societies. E-voting should first comply with the existing legal and regulatory framework. Moreover, e-voting should be technically implemented in such a way that ensures adequate user requirements. As a result, the aim of this paper is twofold. Firstly, to identify the set of generic constitutional requirements, which should be met when designing an e-voting system for general elections. This set will lead to the specific (design) principles of a legally acceptable e-voting system. Second, to identify, using the Rational Unified Process, the requirements of an adequately secure e-voting system. These requirements stem from the design principles identified previously. The paper concludes that an e-voting capability should, for the time being, be considered only as a complementary means to the traditional election processes. This is mainly due to the digital divide, to the inherent distrust in the e-voting procedure, as well as to the inadequacy of the existing technological means to meet certain requirements.
21|6||University systems security logging: who is doing it and how far can they go?|The importance of providing a secure environment for individual and corporate data, research, and communications has grown to critical proportions as more of the mission and business of colleges and universities is carried out over networked information infrastructures. System administrators must implement new, more extensive processes to protect data, to identify and eliminate vulnerabilities, and to find and manage abuses of the systems they manage. They have responded by increasing the network and major systems logging and monitoring efforts and want to do more. But how far can they go before their logging for the sake of security becomes surveillance and a violation of student record privacy under the Family Educational Rights and Privacy Act (FERPA)? What systems are they logging? How are they managing logs? What training have they had to support their work in the areas of security and data management? What processes are in place to manage log data from unauthorized access?
21|6||Hierarchical access control based on Chinese Remainder Theorem and symmetric algorithm|This paper presents an improvement in a cryptographic key assignment scheme that is proposed by Kuo et al. Considering the problem of dynamic access control in a user hierarchy, such an improvement focuses on the ability to implement easily. Moreover, this improvement is an attempt to greatly reduce the computation time and the storage size required by the key generation and the key derivation phases in Kuo’s scheme.
21|6||IFIP technical committee 11|
21|7|http://www.sciencedirect.com/science/journal/01674048/21/7|The US Governmentâ bigger and better information security?|
21|7||Security Views|
21|7||Note from the Publishers|
21|7||Spam, scams, chains, hoaxes and other junk mail|
21|7||Bugbear|
21|7||Trends in academic research: vulnerabilities analysis and intrusion detection|
21|7||Managed Security Services â new economy relic or wave of the future?|Is IT security ready to go the way of physical security? Should it be done in-house, or should corporates start eating out? Brian McKenna takes some soundings.
21|7||Vulnerabilities categories for intrusion detection systems|
21|7||Security policy update|This article reviews the recent spate of public policy initiatives which will impact on an organization’s approach towards the security of its data.
21|7||Technology and Electronic Communications Act 2000|
21|7||Digest of recent IT security press coverage|
21|7||In brief|
21|7||Events|
21|7||International Board of Referees|
21|7||Guide for authors|
21|7||Cyberterrorism?|The term cyberterrorism is becoming increasingly common in the popular culture, yet a solid definition of the word seems to be hard to come by. While the phrase is loosely defined, there is a large amount of subjectivity in what exactly constitutes cyberterrorism. In the aftermath of the September 11th attacks, this is somewhat disconcerting. In an attempt to define cyberterrorism more logically, a study is made of definitions and attributes of terrorism and terrorist events. From these attributes a list of attributes for traditional terrorism is developed. This attribute list is then examined in detail with the addition of the computer and the Internet considered for each attribute. Using this methodology, the online world and terrorism is synthesized to produce a broader but more useful assessment of the potential impact of computer-savvy terrorists. Most importantly, the concept of ‘traditional’ cyberterrorism, which features the computer as the target or the tool is determined to be only a limited part of the true risk faced. Finally, the authors discuss the impact this new view of cyberterrorism has on the way in which one should build one’s defenses. In particular, the breadth of the issue poses significant questions for those who argue for vertical solutions to what is certainly a horizontal problem. Thus, the validity of special cyberterrorism task forces that are disconnected or loosely connected with other agencies responsible for fighting the general problem of terrorism is questioned, and a broader, more inclusive method suggested.
21|7||Applying digital rights management systems to privacy rights management|Disclaimer
21|7||An enhancement of timestamp-based password authentication scheme|Yang and Shieh proposed a timestamp-based password authentication scheme. Chan and Cheng proved that it is insecure. In this paper, we will give a further cryptanalysis of the scheme, and give an easier attack on it. Finally, we will propose an improved scheme that can withstand both of the attacks. Compared to other authentication schemes, this improved scheme allows the host to authenticate a user only with his login request. The host need not keep any secret or information of the user.
21|7||IFIP technical committee 11|
21|8|http://www.sciencedirect.com/science/journal/01674048/21/8|The gap between cryptography and information security|Few controls available to information security professionals today are more potentially powerful than is encryption. Among the many important benefits of encryption are data confidentiality, integrity of data and system files, protection against repudiation in business and other transactions, assurance of individuals’ identity, protection against cheating in voting and contract signing, and others. Security professionals are required to know at least the basics of cryptography to pass professional certification tests such as the CISSP exam; they must often know much more to be able to be proficient on the job. A number of vendors have produced impressive encryption products for desktop encryption, authentication methods, virtual private networks (VPNs), and digital signatures. Significant advances in cryptography and cryptanalysis research have also occurred over the years.
21|8||Security Views|
21|8||Compsec 2002: the complete security circle|
21|8||KLEZ H|
21|8||Giga Security|The flow of information, within organizations, between networks, and from single users to other individuals and networks, is commonly at rates that only a few years ago were dreams. Earlier methods for the detection and prevention of malicious activities are anywhere from inefficient to unworkable with transmissions at the giga speeds that are prevalent today. This writing sets forth the problems and threats associated with these new high speed transmissions, and presents methodologies and systems for treating them.
21|8||Global Trust, Certification and (ISC)2|With the onset of the global network economy, the conventional wisdom about information security and privacy protection is undergoing serious re-evaluation.
21|8||Policy challenges in building dependability in global infrastructures|[This paper was given at Compsec 2002, in London, on 30 October 2002].
21|8||Digest of recent IT security press coverage: Compiled by Bill McKenna|
21|8||In brief: Compiled by Bill McKenna|
21|8||Events|
21|8||Guide for Authors|
21|8||International Board of Referees|
21|8||Individual Authentication in Multiparty Communications|In this paper we introduce a new authentication scheme to achieve individual authentication in group communications. The scheme is particularly efficient and suitable for applications where users require to transmit stream of data of undefined length through noisy channels. Our scheme is in fact, robust against loss of packets during the transmission. We present the scheme called chained stream authentication (CSA) and then we prove that the scheme is conditionally secure. We then describe two variations of CSA, one interactive to use when multicast is available and a timed version suitable for broadcast communications. We conclude by describing our implementation of the timed version that is integrated and fully compatible with RAT.
21|8||Differentially secure multicasting and its implementation methods|Though the areas of secure multicast group architecture, key distribution and sender authentication are under scrutiny, one topic that has not been explored is how to integrate these with multi-level security. Multi-level security is the ability to distinguish subjects according to classification levels, which determines to what degree they can access confidential objects. In the case of groups, this means that some members can exchange messages at a higher sensitivity level than others. The Bell-La Padula model [BL76] outlines the rules of these multi-level accesses. In multicast groups that employ multi-level security, some of these rules are not desirable so a modified set of rules is developed in this paper and is termed differential security.
21|8||A practical key management scheme for access control in a user hierarchy|In a user hierarchy we say that a security class is subordinate to another security class if the former has a lower security privilege than the latter. To implement such a hierarchical structure, it is often desirable to allow the user of each security class to derive the keys of its subordinating classes. This problem has been extensively studied but the existing solutions have various drawbacks. In this paper, we present a practical solution to this problem, which is an efficient key management scheme that needs only a reasonable amount of extra storage. It is secure because illegal key derivations are prevented, and key replacements do not reveal information about the relationship between the old key and the new key. It is also very flexible in that it supports convenient topological changes and membership updates. Furthermore, it provides a solution to the ex-member problem, that has been ignored in many existing research works.
21|8||IFIP technical committee 11|
22|1|http://www.sciencedirect.com/science/journal/01674048/22/1|From the editor-in-chief|
22|1||Security views|
22|1||Time cost$ money|
22|1||Reseacrh in cryptography and security mechanisms|
22|1||Computer forensics|
22|1||Security in a Flash|A new class of devices has started appearing on the market in large numbers. They are about the size of a typical house key, weigh next to nothing, plug in to USB ports, and provide from 8 megabytes to several gigabytes of removable and easily transportable storage. This article looks at the possible security benefits of such devices.
22|1||A tangled Web of libel lies?|
22|1||UK police promise charter to guard good names|
22|1||LIRVA virus|
22|1||Events|
22|1||Guide for Authors|
22|1||International Board of Referees|
22|1||Efficient anomaly detection by modeling privilege flows using hidden Markov model|Anomaly detection techniques have been devised to address the limitations of misuse detection approaches for intrusion detection with the model of normal behaviors. A hidden Markov model (HMM) is a useful tool to model sequence information, an optimal modeling technique to minimize false-positive error while maximizing detection rate. In spite of high performance, however, it requires large amounts of time to model normal behaviors and determine intrusions, making it difficult to detect intrusions in real-time. This paper proposes an effective HMM-based intrusion detection system that improves the modeling time and performance by only considering the privilege transition flows based on the domain knowledge of attacks. Experimental results show that training with the proposed method is significantly faster than the conventional method trained with all data, without loss of detection performance.
22|1||A quantitative study of Public Key Infrastructures|Public Key Infrastructures have not reached the widespread diffusion expected of them, although they are well understood from a security point of view, because, like many say, the killer application has not been found yet. The lack of a clear understanding of the performance of these systems also contributes significantly to their limited diffusion. Studies have appeared of specific aspects of the operations of PKIs, but no complete studies of the overall system are known.
22|1||A password authentication scheme with secure password updating|Recently, Hwang and Yeh proposed an improvement on the Peyravian-Zunic password scheme. The Hwang-Yeh scheme comprises a password authentication protocol, a password change protocol, and can also provide key distribution. Though the Hwang-Yeh scheme repaired several security problems of the Peyravian-Zunic scheme, it has several security problems: the password change protocol in the Hwang-Yeh scheme is vulnerable to a denial of service attack; and it does not provide the forward secrecy property in session key distribution. Furthermore, we shall fix the Hwang-Yeh scheme to avoid these problems.
22|1||IFIP technical committee 11|
22|2|http://www.sciencedirect.com/science/journal/01674048/22/2|Internet security: whatâs in the future?|Only seven or eight years ago the Internet was still more or less a novelty to many commercial organizations (particularly those outside North America). Many used it sparingly (mostly for email communications); a few others used it for E-commerce and other functions that the Internet was making possible. E-commerce was at the time being sold as a panacea for business; emerging technology such as application firewalls, password filters, and intrusion detection tools seemed to hold great promise for Internet security.
22|2||Security Views|
22|2||The law, cybercrime, risk assessment and cyber protection|
22|2||The circumstances of seizure|Introduction
22|2||Implementing enterprise security: a case study3|Introduction
22|2||Security with unfortunate side effects|
22|2||The economic impact of war with Iraq â asymmetric risks|Concern about war between some Western countries and Iraq has mounted in recent weeks. War with Iraq may have important economic consequences as well as political and security-related consequences. First-order consequences might include increased oil prices and also higher defence expenditure at a time when it appears that tax revenues will be rising much more slowly than government spending. There will also be second order consequences. These will depend on the outcome of war and on whether war achieves its objective of bringing peace and stability to the region or, in fact, makes the region less stable.
22|2||Safeguards for IT managers and staff under the Sarbanes-Oxley Act|
22|2||Events|
22|2||Computers & Security Volume 21 â Author Index|
22|2||Computers & Security Volume 21 â Subject Index|
22|2||International Board of Referees|
22|2||Guide for Authors|
22|2||IP Traceback using header compression|Denial-of-service and other malicious attacks have become increasingly prevalent in recent years. A major issue hindering the ability to trace attacks to their sources is the ease of IP address spoofing which conceals the attackers’ identity. Several techniques, generically named IP traceback, have been proposed to enable tracing of IP packets from destination to source despite IP spoofing. In this paper, we propose a Simple, Novel IP Traceback using Compressed Headers (SNITCH) that is based upon Probabilistic Packet Marking (PPM). This technique employs header compression to increase the number of bits available for insertion of traceback information. Simulations performed on empirical data have shown that 100% of the attack paths can be determined with a maximum of 0.43% false positive paths.
22|2||Dealing with contextual vulnerabilities in code: distinguishing between solutions and pseudosolutions|Vulnerabilities in objects in various operating systems or add-ons continue to surface at a rapid rate, posing a unique security problem, one with which vendors appear to be struggling. Patching a vulnerability discovered in a default system binary, such as the highly publicized sendmail debug vulnerability (this vulnerability has been discussed extensively in the literature and was even exploited in the infamous Internet Worm [1]), is relatively easy. The vendor often simply issues a new version of the binary to replace the vulnerable one. The interface for all applications that invoke this binary remains the same. However, with componentized code, such as in modern object-oriented systems, things do not work quite as smoothly. For example, how should vulnerabilities be patched if an object is vulnerable to attack only if it is used in a certain context, or if only one function out of many is vulnerable? Patching the vulnerability is simple if a function can be replaced. If the vulnerability is contextual and the function has legitimate uses in other areas, however, replacing the function altogether may be inappropriate. What kinds of alternative remedies are appropriate? This paper presents several different approaches to dealing with this difficult problem, and analyzes the strengths and weaknesses of each. Of all the solutions considered, removing code altogether and adding warnings at run time are the least viable. Allowing code to run only if the execution context is correct, permitting only certain callers to execute code, barring certain callers from executing code, and using access control lists to govern access to objects and methods are more reasonable approaches, although each also has limitations.
22|2||A flexible date-attachment scheme on e-cash|The electronic cash payment system is an important technique that has been utilized for electronic commerce. In this paper, we propose a flexible date attaching method to Chaum’s electronic cash scheme, which can also be applied to many other electronic cash schemes. The date attaching method provides an unforgeable and variable scheme to deal with the effective date of e-cash that will allow for many applications. The customer can modify the date slip but it does not become effective until the merchant passes it on to the bank. Also, the usage of this proposed scheme is very flexible, unlike most e-cash schemes that embed an expiration date when e-cash is withdrawn. This scheme still possesses the untraceability property and the computation complexity for customers and merchants is simple when compared to bank operations.
22|2||IFIP technical committee 11|
22|3|http://www.sciencedirect.com/science/journal/01674048/22/3|From the editor-in-chief: Virus and worm trends|I’ll remember the evening of Friday, 24 January for a long time. I had just gotten to sleep when I was awakened by my emergency pager. I jumped out of bed, headed for the nearest phone, and called the number that was displayed on my pager. I reached the person who had initiated the page soon afterwards and learned that a new worm, the ‘MS-SQL Slammer worm’ (which also has other names such as the ‘Sapphire worm’) had struck. Advised that the worm was generating a huge amount of network traffic and that it was targeting port 1434 of Windows systems, I had to make several decisions concerning what to do next. After searching for information about the worm and notifying others with whom I work (as well as CERT/CC, something that unfortunately proved futile) shortly thereafter, I once again went to bed, only to have to deal with the proverbial fallout this worm had created over the next few days.
22|3||Security views|
22|3||Cyber-terrorism in context|Cyber-terrorism has been much discussed in the media, especially at times of heightened international tension. The Institute for Security Technology Studies has tracked previous attacks and, using conflicts such as the Israel-Palestine and India-Pakistan as examples, shows there is a strong correlation between political and military conflicts and the incidence of cyber-terrorism. Research group IDC has recently predicted that in the next year we can expect to experience a cyber-terrorist attack as a result of a war on Iraq, resulting in short term economic disruptions.
22|3||Evidence acquisition|In our last column we discussed the seizure process, stressing the important elements of planning, methodology, interviewing, documentation and thoroughness. The acceptability of evidence gathered will be influenced and possibly accepted or refuted as a result of carrying out the tasks described by these elements in a professional and methodical way. Each step by itself cannot make a successful case, however, any step that is flawed or missed may cause a case to fail.
22|3||International legal aspects of cryptography: Understanding cryptography|Historically, the legal systems in different countries have had to adapt to the advances in technology. For instance, the appearance of automobiles as a popular means of transportation led in the last century to the creation of driving laws and traffic police; in the telecommunications field, telephone networks have made their way into local legislation, with implementations such as government security agencies able to perform surveillance activities through wiretapping of telephone communications. The phenomenal advance of computer technology and network communications, most clearly exemplified by the reach and popularity of the Internet today, is gradually finding its way within legal frameworks throughout the world. However, the constantly evolving environment and complex technical issues involved in this area make it a particularly hard subject to approach legally, requiring from lawmakers both expertise and up-to-date knowledge in order to legislate logically and usefully. In this context, cryptography is only one of numerous computer-based technologies in widespread use [1], but it is a mathematically intricate and often misunderstood area, and thus one of the most difficult to include in legal frameworks. Furthermore, attempts to regulate cryptography, most predominantly when some kind of limit or control is involved, have originated heated debates in recent times, and the delicate balance between national security and individual freedoms has proven to be extremely hard to achieve in this particular field.
22|3||A contest to evaluate IT security services management|This article discusses a project that used a multi-team competition to define, test and validate the added value and costs of a premium level of ‘managed security services’. The services were intended for a limited number of servers used to store and process extremely sensitive information on a large IT infrastructure. They were defined by a specialist third party managed security services (MSS) provider. They included recommended server configuration and intrusion detection software, as well as monitoring services.
22|3||Governments act to improve security|This article reviews the final versions of “The National Strategy to Secure CyberSpace” [1] and “The National Strategy for the Physical Protection of Critical Infrastructures and Key Assets” [2] which together define the US’s approach to maintain continuity of IT infrastructure and systems which could be disrupted by terrorist activity. In this context, the article also reviews the UK’s Treasury Department Green Paper “The Financial System and Major Operational Disruption” [3] as this indicates that the UK Government has been influenced by the strategies and methods adopted by the Bush Administration. The US’s approach is likely to be followed by other advanced economies when deciding how to secure their essential electronic infrastructure.
22|3||Events|
22|3||International Board of Referees|
22|3||Guide for authors|
22|3||Analysis of vulnerabilities in Internet firewalls|Firewalls protect a trusted network from an untrusted network by filtering traffic according to a specified security policy. A diverse set of firewalls is being used today. As it is infeasible to examine and test each firewall for all possible potential problems, a taxonomy is needed to understand firewall vulnerabilities in the context of firewall operations. This paper describes a novel methodology for analyzing vulnerabilities in Internet firewalls. A firewall vulnerability is defined as an error made during firewall design, implementation, or configuration, that can be exploited to attack the trusted network that the firewall is supposed to protect. We examine firewall internals, and cross-reference each firewall operation with causes and effects of weaknesses in that operation, analyzing twenty reported problems with available firewalls. The result of our analysis is a set of matrices that illustrate the distribution of firewall vulnerability causes and effects over firewall operations. These matrices are useful in avoiding and detecting unforeseen problems during both firewall implementation and firewall testing. Two case studies of Firewall-1 and Raptor illustrate our methodology.
22|3||A model for deriving information security control attribute profiles|How does an organization ensure that all information security loopholes are covered? This paper describes a possible solution in terms of an Information Security Control Attribute Profile for an organization. This profile will dictate attributes that should accompany each and every information security control in an organization, thus minimizing the likelihood of malfunctioning controls.
22|3||Generalization of proxy signature-based on discrete logarithms1|In the past few years, many excellent studies on proxy signature schemes have been published. Yet, traditional proxy signature schemes are mainly aimed at dealing with one or two separate proxy conditions each. In this article, the authors shall present a generalized version of the (t1/n1–t2/n2) proxy signature scheme based on the discrete logarithms that can be applied to every proxy situation. The (t1/n1–t2/n2) proxy signature scheme allows the original group of original signers to delegate their signing capability to a designated proxy group. The proxy group of proxy signers can cooperatively generate the proxy signature on behalf of the original group. Any verifier can verify the proxy signature on the message with the knowledge of the identities of the actual original signers and the actual proxy signers. Furthermore, some possible attacks have been considered, and our security analysis shows that none of them can successfully break the proposed scheme.
22|3||Fault trees for security system design and analysis|The academic literature concerning fault tree analysis relates almost entirely to the design and development of safety-critical systems. This paper illustrates how similar techniques can be applied to the design and analysis of security-critical systems. The application of this technique is illustrated in an example inspired by a current public-key cryptosystem.
22|3||IFIP technical committee 11|
22|4|http://www.sciencedirect.com/science/journal/01674048/22/4|Why canât Microsoft stay out of the InfoSec headlines?|Despite all the recent information security-related events — significant privacy infringements, the many new, serious vulnerabilities discovered in various operating systems and applications, recent legal rulings, convictions, and/or acquittals, controversial legislation, and many other significant events, Microsoft seems to always be in the information security headlines. Other vendors such as Oracle also make the headlines, but usually only briefly (such as when Oracle’s CEO pronounced his company’s software “hack proof”). Why does Microsoft seem to constantly be in the spotlight? Consider recent Microsoft-related events:
22|4||Attackers hit Web hosting servers|An unknown attacker broke into a Web hosting server belonging to Bargainhost.co.uk and obtained passwords for 1500 hosted websites. Several of Bargainhost’s customers’ websites were defaced afterwards. Worse yet, the attacker also damaged backups needed to rebuild the sites. Bargainhost reported that it was attempting to manually restore the sites, but could not estimate when it would be able complete this task. Meanwhile, Bargainhost urged its customers to change their passwords. Several of Bargainhost’s customers have complained that the company has been remiss in fixing the problem; many have discontinued using Bargainhost’s Web hosting service as a result of the break-in. After losing nearly seven months of emails, contacts, and forums, snowboarding site Powderroom.net has decided to move its Web hosting business elsewhere despite still having a two-year maintenance contract with Bargainhost. Powderroom’s owner also stated that his company will from now on make its own Web server backups rather than rely on a Web hosting service provider.
22|4||Careless about privacy|At the end of last year the FBI broke up a New York fraud ring which cost US consumers upwards of $2.7 million in fraudulent transactions. It was the largest ever case of identity theft, affecting over 30â000 victims. The perpetrators were able to obtain confidential passwords and codes enabling them to download victims’ credit reports via the Internet and sell the information on at $30 each to criminals, who were able to impersonate the victims and run up fraudulent debts. Passwords are often the weakest link in terms of protection.
22|4||Evidence analysis|In our last column we discussed the evidence acquisition process stressing the chain of evidence and the importance of following a documented method. These two issues are important in order to ensure that the evidence captured will be acceptable in a court of law — provided that it is relevant to the case at hand.
22|4||The role of criminal profiling in the computer forensics process|In today’s increasingly complex world, we find ourselves at a rather unique societal and cultural cross roads. At no other time in history has society been so dependent on technology and its various offshoots and incarnations1. Almost every facet of our day-to-day lives is impacted to some extent by technology (e.g., email, Internet, online banking, digital music, etc.). This reliance and to some extent dependence on technology, has had a ripple effect on other less obvious areas of society (,  and ). One such area is law enforcement and, more specifically, criminal investigations (Kruse and Heiser, 2002). Historically, criminal investigations relied on such concepts as physical evidence, eyewitnesses, and confessions. Today, the criminal investigator must recognize that a vast amount of evidence will be in the electronic or digital form. The crime scene may consist of a computer system or network as opposed to the traditional ‘physical’ scene (Kruse and Heiser, 2002). The eyewitness of today and tomorrow may be a computer generated ‘log file’.
22|4||A taxonomy for information security technologies|The Internet is a public network, which is open and used by all — also for communicating private information. “But private information should be secured!”, I hear you say. Yes. But where should one start looking for help when attempting to secure private information? This paper discusses a taxonomy for information security technologies, which provides information on current state-of-the-art technologies used to secure information at application, host and network level.
22|4||Why we need a new definition of information security|There is an old Peanuts strip where Charlie Brown says, “Working here is like wetting your pants in the pool, wearing a dark bathing suit. You get that warm feeling but nobody notices.” Increasingly, I think computer security professionals in large enterprises are in that metaphorical swimming pool. In fact, many are swimming in the deep end without their water wings. When computer security professionals do an excellent job protecting systems and information, the number of bad outcomes decreases. After a generation of peace, pretty soon people start asking why we need the army. I believe this problem stems in part from a fuzzy fundamental: the definition of information security.
22|4||Events|
22|4||International Board of Referees|
22|4||Guide for authors|
22|4||Methods for preventing unauthorized software distribution|In this paper we present algorithms for protecting software from unauthorized installation. We assume that the user buys software on a disk or downloads it from the Internet — although our methods are not limited to protecting software under these circumstances. We consider two kinds of adversaries. One kind of attacker is a sophisticated hacker who can monitor a line and can read and intercept any information flowing unprotected over the Internet. These attackers are also skillful programmers who can analyze the software, locate any data of interest to them and also write and execute any programs, even the most complicated ones. Another kind of attacker is an average attacker who can copy and use personal or business software.
22|4||Is the mouse click mighty enough to bring society to its knees?|Over the years, we have created an information infrastructure, with most of us connected but nobody totally responsible, which is easily targeted for attacks by adversaries. A series of deadly viruses and denial-of-service attacks are warnings of the fragile state of information security. Information warfare (IW) is a serious concern as it has no border and operates in a different realm. This paper articulates how IW can devastatingly attack the critical information infrastructure. Two aspects of IW are considered, viz., Defensive IW and Offensive IW. Currently, security solutions lag far behind the potential threats. Loosely coupled defenders cannot avert the damage caused by orchestrated coordinated attacks. Society is locked up in a vicious circle of wits and resources. This situation is likely to continue and we need to be proactive and make progress on a rocky path. It is time we got battle space ready. Without the coordinated efforts of government, disparate groups and organisations, society is one click away from grave danger of exploitation and dominance.
22|4||An integral framework for information systems security management|Business use of Internet has exposed security as one of the key-factors for successful online competition. Contemporary management of E-business security involves various approaches in different areas, ranging from technology to organizational issues and legislation. These approaches are often isolated, while management of security requires an integrated approach. This article presents an attempt at management of E-business systems security that is based on integrating existing approaches in a balanced way. To foster practical use of the conceptual model in this paper, brief background knowledge in related areas is given.
22|4||IFIP technical committee 11|
22|5|http://www.sciencedirect.com/science/journal/01674048/22/5|Pandoraâs Box: spyware, adware, autoexecution, and NGSCB|The increased sophistication of today’s computing environment has produced many benefits — better performance, considerably more sophisticated graphics and functionality, greater reliability, and others. At the same time, however, something insidious is occurring. Spyware and adware programs are being deliberately included with other legitimate software packages to deliver functionality that users do not want or need.
22|5||Security Views|A Windows worm known as W32.Sobig.B (but also as the Palyh and the Mankx worm) is arriving in the form of an attachment in messages that appear to be from Microsoft support. Sobig.B creates and then sends messages to addresses it finds in address books of systems it infects. The indicated address of the sender is support@microsoft.com. Subject lines vary, but ‘Screensaver’, ‘Cool Movie’, ‘Re: My application’, ‘Approved (Ref: 38446-263)’, and ‘Your password’ are frequently used. The name of the attachment that contains this worm has a .pif file extension, but the actual name varies. ‘movie28.pif’, ‘screen_temp.pif’, ‘doc_details.pif’, ‘ref-394755.pif’, and ‘password.pif’ are common attachment names. If the recipient of an infected message that Sobig.B sends opens the attachment, the recipient’s system becomes infected. Once the system is infected, Sobig.B creates a Registry entry that causes this worm to be started up whenever the infected system boots.
22|5||Privacy legislation: a comparison of the US and European approaches|We are living in an increasingly connected world, not only in the office, but at home; a world in which more and more personal data is being stored in huge databases. With the increasing demands for personal data, often far in excess of the data required for the transaction in hand, so too does the reluctance to disclose such personal information. There is a perception that our personal information gets translated into unwanted direct marketing; that the most obvious manifestation of the e-connected world is of ever increasing amounts of spam, often from organisations that we have never heard of. This in turn has fuelled an awareness of the need to protect personal information.
22|5||Encountering encryption|In our last column we took a look at analysing evidence. There are lots of other issues to consider in the analysis process that, by the nature and brevity of this column were not addressed. However, no one expects that after reading a few narrowly focused columns on electronic forensics to have become an electronic forensics expert or practitioner. There’s a bit more to it than that.
22|5||The 419 scam: information warfare on the spam front and a proposal for local filtering|The infamous ‘419’ advance fee fraud scam is a major source of spam which takes its victims for hundreds of millions of dollars yearly. Email headers confirm that much of this traffic currently comes out of Lagos and Amsterdam. 419ers also create phony bank websites. Ad hoc information warfare has begun in which anti-scammers hijack these websites as well as scammers’ email accounts. Spam legislation will not stop these particular emails. Filtering outgoing mail, as close as practical to the source, could suppress that portion of the fraud carried out by email, at least in the short term.
22|5||Leading attackers through attack graphs with deceptions|This paper describes a series of experiments in which specific deceptions were created in order to induce red teams attacking computer networks to attack network elements in sequence. It demonstrates the ability to control the path of an attacker through the use of deceptions and allows us to associate metrics with paths and their traversal.
22|5||Next generation security for wireless: elliptic curve cryptography|Scott Vanstone, from Certicom, polemicizes for elliptic curve cryptography. He advances his company's view that ECC is the next generation of public-key cryptography for wireless.
22|5||SSL Virtual Private Networks|Andrew Harding, technical director of clientless VPN provider Neoteris, argues the case for SSL VPNs.
22|5||Events|
22|5||International Board of Referees|
22|5||Utilising fuzzy logic and trend analysis for effective intrusion detection|Computer security, and intrusion detection in particular, has become increasingly important in today’s business environment, to ensure safe and trusted commerce between business partners as well as effective organizational functioning. Various approaches to intrusion detection are currently being utilized, but unfortunately in practice these approaches are relatively ineffective and inefficient. New means and ways that will minimize these shortcomings must, therefore, continuously be researched and defined. This paper will propose a proactive and dynamic approach, based on trend analysis and fuzzy logic that could be utilized to minimize and control intrusion in an organization’s computer system.
22|5||A new taxonomy of Web attacks suitable for efficient encoding|Web attacks, i.e. attacks exclusively using the HTTP/HTTPS protocol, are rapidly becoming one of the fundamental threats for information systems connected to the Internet. When the attacks suffered by Web servers through the years are analyzed, it is observed that most of them are very similar, using a reduced number of attacking techniques. It is generally agreed that classification can help designers and programmers to better understand attacks and build more secure applications. As an effort in this direction, a new taxonomy of Web attacks is proposed in this paper, with the objective of obtaining a useful reference framework for security applications. The use of the taxonomy is illustrated by means of multiplatform real world Web attack examples. Along with this taxonomy, important features of each attack category are discussed. A semantic-dependent Web attack encoding scheme is also defined that, together with the taxonomy, can be used to process the attack information with low time and memory consumption. Applications of the taxonomy and the encoding scheme are described, such as intrusion detection systems and application firewalls.
22|5||A comment on the Chen-Chung scheme for hierarchical access control|Chen and Chung proposed an improved scheme of hierarchical access control based on the Chinese Remainder theorem and symmetric encryption [1]. In this paper, we point out that the part of their scheme for adding a new class is not correct. Furthermore, we discuss how to fix this problem.
22|5||Cryptanalyses and improvements of two cryptographic key assignment schemes for dynamic access control in a user hierarchy|Recently, Wu and Chang and Shen and Chen separately proposed a cryptographic key assignment scheme for solving access control problem in a partially ordered user hierarchy. However, this paper will show the security leaks inherent in both schemes based on polynomial interpolations. That is, the users can have access to the information items held by others without following the predefined partially ordered relation. Finally, we proposed two improvements to eliminate such security flaws.
22|5||IFIP technical committee 11|
22|5||Refereed papers â Guide for Authors|
22|6|http://www.sciencedirect.com/science/journal/01674048/22/6|From the editor-in-chief: Gartner âs prediction concerning intrusion detection systems: sense or nonsense?|
22|6||Security views: Online piracy battle heats up in US|US Senator Orrin Hatch, chairman of the Senate Judiciary Committee, stated at a recent hearing on copyright abuse that he wants to teach cyberpirates a lesson by destroying their computers if they have illegally downloaded copyright-protected material such as music or movies. During the hearing witnesses explained the dangers of using peer-to-peer file-sharing services. They gave examples of peer-to-peer users inadvertently allowing access to their entire hard drives, exposing financial documents and medical data. After a volley of strenuous objections to his proposal, Mr. Hatch backed off from his initial hard-line stance, saying he merely wants to push private industry to come up with solutions to copyright abuse. Curiously, while the furor over his statements raged, Mr. Hatch’s Web site had an unexplained link to a pornographic page.
22|6||Spam: the evolution of a nuisance|Junk e-mail (spam) is twenty five years old — the first junk e-mail was sent on the Arpanet, the fore runner to the Internet, in 1978. Not that there has been any cracking open of the bottles of bubbly to celebrate. It did not receive the spam designation until ten years ago based on the Monty Python sketch in which customers of a restaurant are offered spam with everything. Ironically, companies that offer anti-spam software are being threatened with breach of trademark litigation if they include the word spam in the product title by the company that owns the spam trademark. (for those of you unacquainted with spam, it is a tinned pork luncheon meat that really took off during the Second World War which declined in popularity in the Sixties).
22|6||Presenting the Evidence Report: Introduction|In our last column we took a look at encountering encrypted evidence. There is some additional information that has come to light since the last column. Hard wired keystroke loggers are available that can be installed within the target’s own keyboard. These take about fifteen minutes to install but cannot be detected easily by the person under surveillance. They typically have a capacity to store, using 128 bit encryption, up to two megabytes of keystrokes. This is about 300,000 words or a year’s worth of typing. Viewing the log is, after opening a word processor or WordPad, as simple as typing on the modified keyboard a password that you control. This will execute a menu program stored on the device and keystrokes can then be downloaded for analysis. There are other menu options that allow you to manage the storage associated with the surveillance device as well as changing its controlling password.
22|6||Security engineering and security RoI|IT Security has been practised as a dark art for too long. We should treat it as an engineering discipline and reset our expectations about how security systems should be designed and evaluated. All it would take is a fresh approach, the right metrics and a little competent analysis. This is how it might work.
22|6||Operationalizing IT Risk Management|In a study of four major global organisations conducted during 2002, it was found that all conducted some form of risk assessment to assist in the management of security risks. However, when we analysed the risks that they addressed, three of the four organisations had major gaps in their risk assessment coverage that could result in significant risks being missed. We wondered: why did the gaps exist; are there inhibitors to effective risk assessment; are there blind spots; are approaches to risk assessment deficient in some way; how could we make the process of risk assessment more robust but easier to do? This paper seeks to address some of these questions.
22|6||Security analysis of XML usage and XML parsing|Web-based applications greatly increase the availability of information and the ability of people to access and share information in a collaborative environment. Organisations can only truly make use of this technology to create a competitive advantage if they can trust the technology to distribute and mediate information in a safe and secure manner. The Web was not designed with security in mind and the use of XML as a vehicle for marking up information and mediating information flows does not directly support the imposition of a security architecture to manage the security of collaborative information sharing and dissemination. The adoption of XML as the vehicle for electronic commerce has created an environment where XML is now a core technology to most organisations, yet most organisations are relying on off-the-shelf solutions to parsing and manipulating it. In this paper we will examine how XML and XML parsers can be attacked and used to modify, and enter false or misleading, information relating to an electronic transaction. The attack scenarios will be divided into five categories: DTD, Document Corruption, single-node, multi-node and back-end systems. For each attack type we will explore how the attack is perpetrated and what, if any, countermeasures exist to mitigate the attacks.
22|6||Roadmap to checking data migration|
22|6||RBAC models â concepts and trends|A key function in any information security infrastructure is represented by access control which concerns the ways according to which users can access resources in a computer systems. Access control is one of the most pervasive security mechanisms in use today and is present in almost all systems, from operating systems to database management systems. Access control is usually based on access permits, also called authorizations, specifying which subjects can access which objects for performing which actions. Access control, however, imposes great administrative and architectural challenges and also requires careful design. In particular, a relevant problem, especially when dealing with large systems, is represented by the complexity of access control administration. Access control administration deals with assigning and revoking authorizations.
22|6||Web services set to provoke new sthreats: Preview of Compsec 2003, 30 Oct-1 Nov, Queen Elizabeth II Conference Centre, Westminster, London, UK|This year’s Compsec aims to map out the near future of IT security, offering a practical guide to action on current and upcoming threats. It addresses some of the frameworks of information security — privacy and regulation — and looks at the latest in technologies from intrusion detection through authentication to wireless. Conference programme director Brian McKenna previews the event.
22|6||Events|
22|6||Refereed papers|
22|6||New hierarchical assignment without Public Key cryptography|The access privileges in many distributed systems can be effectively organized as a hierarchical tree. Distributing distinct cryptographic keys to distinct entities, according to their privileges, provides a good solution to the hierarchical access control problem. Many existing key assignment schemes use public key cryptography, which requires lots of costly public key operations and thus leads to a limited degree of deployment. In this paper, we shall propose a new key assignment protocol that employs only a low cost smart card with little memory to perform simple arithmetic operations. Our approach greatly reduces the computational load and the implementation cost. Compared with Lin’s scheme, which is a very efficient scheme without using any public key cryptography, our new scheme further reduces the computational cost by as much as 66% and the quantity of public data by 50%.
22|6||Efficient proxy multisignature schemes based on the elliptic curve cryptosystem|For improving proxy-signature research, Sun [5] attempted to resolve problems related to defective security in the scheme of Yi [3]. However, both Yi and Sun’s schemes involve a significant number of exponential operations to verify the proxy signature. Accordingly, an improvement is proposed here to change the exponential operations into elliptic curve multiplicative ones. As proposed by both Koblitz  and  and Miller [8] in 1985, the elliptic curve is used in developing the cryptosystems. The elliptic curve cryptosystem can achieve a level of security equal to that of RSA or DSA but has a lower computational overhead and a smaller key size than both of these. Therefore, it is used in Sun’s schemes to improve their efficiency.
22|6||Security middleware for enhancing interoperability of Public Key Infrastructure|This paper describes a security middleware for enhancing the interoperability of public key infrastructure (PKI). Security is a key concern in e-commerce and is especially critical in cross-enterprise transactions. Public key cryptography is widely accepted as an important mechanism for addressing the security needs of e-commerce transactions because of its ability to implement non-repudiation. The deployment of public key cryptography is facilitated by the provision of PKI which assures the integrity of cryptographic keys. Nevertheless, industry experiences have shown that the task of implementing PKI-based e-commerce applications is challenging. Prior studies have identified interoperability as a major issue that hinders the adoption of PKI in spite of its effectiveness in implementing strong security mechanisms and protocols. In this paper, we discuss the interoperability issue of PKI applications. This research is part of our effort in designing security infrastructure for e-commerce systems. A middleware architecture was designed to enhance interoperability of PKI applications. The security middleware aims to promote cross-enterprise cross-border e-commerce transactions. The proposed mechanism is proven to be practical in real deployment environment.
22|6||A user friendly remote authentication scheme with smart cards|Based on a one-way function, Sun [6] has proposed an efficient remote authentication scheme using smart cards. The scheme is very elaborate since no password table is required to keep as well as low communication and low computation costs. However, the password of a user has to be computed by the system. This, in general, cannot satisfy user’s requirements. To achieve the aim of user friendliness, we propose a modified version that inherits the advantages of Sun’s scheme while still allowing the users to choose and change their passwords freely.
22|6||IFIP technical committee 11|
22|7|http://www.sciencedirect.com/science/journal/01674048/22/7|Patching Pandemonium|I am not in the best of moods right now because of a problem I am having after installing a patch. While using my mostly reliable Windows 2000 machine, I read an email message that announced that vulnerability scans in the part of the network to which my machine is connected were going to be conducted soon. I quickly brought up the Microsoft Baseline Security Analyzer (MBSA) and checked whether any patches were missing.
22|7||Security Views|
22|7||Nimbyism, dominoes and creaking infrastructure|It was another hot sweltering afternoon in down town New York, when the whirl of air conditioning systems straining on their highest settings suddenly ceased. The power to run them had failed right across New York City, and indeed much of the Midwest and North East America and Eastern Canada. Businesses were brought to a halt, as were subways and surface trains and most airports across the region. Thousands of people were trapped in subways and elevators as officials battled to bring power back online. The New York Stock Exchange (NYSE) and the United Nations were left without power and evacuated. Times Square fell dark along with much of the city.
22|7||Forensic evidence testimony â some thoughts|
22|7||Applying information security governance|Background
22|7||Data dependent rotations, a trustworthy approach for future encryption systems/ciphers: low cost and high performance|This work focuses an alternative direction of cryptography, based on Data Dependent Rotations (DDR). This methodology gives many promises for secure communication networks of the next years. DDR transformations have attracted the interest of researchers, and a great number of new ciphers have been developed. These encryption algorithms are intended to be used in all the applications of present and future. The article summarizes the key issues for this new approach and presents both performance and implementation cost of DDR implementations. The purpose of this work is to provide a state-of-the-art overview on DDR ciphers, which are proved a trustworthy applied methodology for modern cryptography.
22|7||Events|
22|7||International Board of Referees|
22|7||Security enhancement for the timestamp-based password authentication scheme using smart cards|In 1999, Yang and Shieh proposed a timestamp-based password authentication scheme with smart cards. However, Chan and Cheng showed that it was insecure because the scheme was vulnerable to the forged login attack. In this paper, we propose a modified Yang-Shieh scheme to enhance security. Our modification can help withstand the forged login attack and also provide a mutual authentication method to prevent the forged server attack.
22|7||An anomaly intrusion detection method by clustering normal user behavior|For detecting an intrusion based on the anomaly of a user's activities, previous works are concentrated on statistical techniques or frequent episode mining in order to analyze an audit data set. However, since they mainly analyze the average behavior of a user’s activities, some anomalies can be detected inaccurately. This paper proposes an anomaly detection method which utilizes a clustering algorithm for modeling the normal behavior of a user’s activities in a host. Since clustering can identify an arbitrary number of dense ranges in an analysis domain, it can eliminate the inaccuracy caused by statistical analysis. Consequently, it can model the frequent activities of a user more accurately than the statistical analysis does. The common knowledge of activities in the transactions of a user is represented by the occurrence frequency of similar activities by the unit of a transaction as well as the repetitive ratio of similar activities in each transaction. The proposed method also addresses how to maintain identified common knowledge as a concise profile. Furthermore, this paper addresses the selection of good features that can improve the detection rate of anomalous behavior in an on-line transaction.
22|7||Detecting intrusion with rule-based integration of multiple models|As the information technology grows interests in the intrusion detection system (IDS), which detects unauthorized usage, misuse by a local user and modification of important data, has been raised. In the field of anomaly-based IDS several data mining techniques such as hidden Markov model (HMM), artificial neural network, statistical techniques and expert systems are used to model network packets, system call audit data, etc. However, there are undetectable intrusion types for each measure and modeling method because each intrusion type makes anomalies at individual measure. To overcome this drawback of single-measure anomaly detector, this paper proposes a multiple-measure intrusion detection method. We measure normal behavior by systems calls, resource usage and file access events and build up profiles for normal behavior with hidden Markov model, statistical method and rule-base method, which are integrated with a rule-based approach. Experimental results with real data clearly demonstrate the effectiveness of the proposed method that has significantly low false-positive error rate against various types of intrusion.
22|7||The reduced Enigma|This article describes a simplified cryptographic machine, based closely on the World War II Enigma. This ‘reduced Enigma’ exposes some of the design flaws of the original Enigma in a new way. Had the Axis powers built a reduced Enigma, the outcome of the war might have been different.
22|7||Cryptanalysis of an enhanced timestamp-based password authentication scheme|Recently, Fan proposed an enhanced scheme to improve the security of Yang-Shieh’s timestamp-based password authentication scheme. The enhanced scheme can withstand the attacks presented by Chan, Cheng and Fan. In this paper, we show that the enhanced scheme is still insecure. An intruder is able to construct a forged login request by intercepting the legitimate login requests and pass the system authentication with a non-negligible probability.
22|7||IFIP Technical Committee 11|
22|8|http://www.sciencedirect.com/science/journal/01674048/22/8|Information security and the media|The relationship between the media and information security is intriguing. The media is quick to cover security-related incidents such as worm outbreaks and intrusions into systems and networks, serious vulnerabilities and so on, in many respects helping in the job of raising public awareness of security issues. The media's fascination with information security also has negative consequences, however. Information security professionals, especially those who are consultants, often compete for media exposure. Several weeks ago the president, founder, and business administrator of Forensic Tec, a California-based security consultancy, were indicted for breaking into numerous US government and Department of Defense systems. After allegedly breaking into these systems, members of this consultancy openly bragged about how easy it was to breach their security. The press ran stories to the effect that some of the most critical computers within the US were wide open to attack. Interestingly, the indictment accused the individuals of creating a publicity stunt to drum up business for this new, small consultancy.
22|8||Security views|
22|8||Computer security: Mapping the future|
22|8||Setting up an electronic evidence forensics laboratory|In our last column we took a look at being an expert witness and giving forensic evidence testimony. An added word of cautionary advice for private practitioners: CHARGE A LOT for expert testimony! If you are involved in a high-profile case that drags on and on and you must testify as an expert witness repeatedly at the whim and caprice of the various attorneys, it could disrupt your private practice and cause potential loss of current and future earnings. Your current case load could be seriously delayed and your credibility for future work may also be damaged. The previous discussion, by nature, was very general, however each jurisdiction has a formal set of directives and guidelines specifically to assist expert witnesses so you must also refer to these for more details.
22|8||Beyond cryptography: Bruce Schneier's Beyond Fear: thinking sensibly about security in an uncertain world|Bruce Schneier is best known for his classic work, Applied Cryptography, a lengthy, scholarly tome that includes almost 70 pages just for the references. At first glance, it's not at all apparent that Schneier's latest book, Beyond Fear, is even from the same author. Written not for specialised technologists, Beyond Fear is meant to be universally accessible.
22|8||Security and human computer interfaces|Computer users are exposed to technology mainly through user interfaces. Most users' perceptions are based on their experience with these interfaces. HCI (human computer interaction) is concerned with these interfaces and how they can be improved. Considerable research has been conducted and major advances have been made in the area of HCI. Information security is becoming increasingly important and more complex as business is conducted electronically. However, state-of-the-art security-related product development has ignored general aspects of HCI. The objective of this paper is to promote and enable security awareness of end-users in their interaction with computer systems. It thus aims to consolidate and integrate the two fields of information security and HCI. HCI as a research discipline is a well developed field of study, and the authors are of the opinion that the use of security technologies can be significantly enhanced by employing proven HCI concepts in the design of these technologies. In order to achieve this, various criteria for a successful HCI in a security-specific environment will be examined. Part of the Windows XP Internet Connection Firewall will be used as a case study and analysed according to these criteria, and recommendations will be made.
22|8||Improving user security behaviour|Many organisations suspect that their internal security threat is more pressing than their external security threat. The internal threat is predominantly the result of poor user security behaviour. Yet, despite that, security awareness programmes often seem more likely to put users to sleep than to improve their behaviour. This article discusses the influences that affect a user's security behaviour and outlines how a well structured approach focused on improving behaviour could be an excellent way to take security slack out of an organisation and to achieve a high return for a modest, low-risk investment.
22|8||Calendar of forthcoming conferences and events|
22|8||International Board of Referees|
22|8||Understanding users' keystroke patterns for computer access security|User authentication is a major problem in gaining access rights for computer resources. A recent approach to enhance the computer access rights is the use of biometric properties as the keystroke rhythms of users. Therefore user authentication for computers can be more secure using keystroke rhythms as biometric authentication. Methods like minimum distance, statistical, vector based, neural network type and data mining techniques have been applied in analyzing the keystroke patterns. In this paper, a vector based algorithm for a recent approach has been applied in the identification of keystroke patterns. Keystroke Identification system that is a neuro physical characteristic is studied to realize biometric authentication.
22|8||The availability of source code in relation to timely response to security vulnerabilities|Once a vulnerability has been found in an application or service that runs on a computer connected to the Internet, fixing that exploit in a timely fashion is of the utmost importance. There are two parts to fixing vulnerability: a party acting on behalf of the application's vendor gives instructions to fix it or makes a patch available that can be downloaded; then someone using that information fixes the computer or application in question. This paper considers the effects of proprietary software versus non-proprietary software in determining the speed with which a security fix is made available, since this can minimize the amount of time that the computer system remains vulnerable.
22|8||Attacks on the (enhanced) Yang-Shieh authentication|The Yang-Shieh authentication is a time-stamp based password authentication scheme that uses smart cards [1]. In [ and ], various attacks on this scheme are described. However, an enhancement of the scheme is proposed in [3] and enables the scheme to resist these existing attacks. In this paper, we show two new attack that can break the enhanced scheme. We further point out that the fundamental computational assumption of the Yang-Shieh authentication scheme is incorrect.
22|8||Efficient anonymous auction protocols with freewheeling bids|The need for electronic auction services has been increasing in recent years. Taking security into account, anonymity of online bidders becomes more important than it used to be. However, bidders cannot bid of his/her free will in existing anonymous auction protocols. For real-time applications, time delays are the significant factor taken into account. As a result, the proposed paper presents a simple and efficient method to ensure that the bidders can bid arbitrarily and anonymously.
22|8||IFIP technical committee 11|
23|1|http://www.sciencedirect.com/science/journal/01674048/23/1|Security training and awarenessâfitting a square peg in a round hole|
23|1||Security views|
23|1||The future of computer forensics: a needs analysis survey|The current study was a pilot study and attempted to add to the growing body of knowledge regarding inherent issues in computer forensics. The study consisted of an Internet-based survey that asked respondents to identify the top five issues in computer forensics. Sixty respondents answered the survey using a free form text field. The results indicated that education/training and certification were the most reported issue (18%) and lack of funding was the least reported (4%). These findings are consistent with a similar law enforcement community study (Stambaugh, Beaupre, Icove, Cassaday, Williams. State and local law enforcement needs to combat electronic crime. National Institute of Justice Research in Brief (2001)). The findings emphasize the fragmented nature of the computer forensics discipline. Currently there is a lack of a national framework for curricula and training development, and no gold standard for professional certification. The findings further support the criticism that there is a disproportional focus on the applied aspects of computer forensics, at the expense of the development of fundamental theories. Further implications of the findings are discussed and suggestions for future research in the area are presented.
23|1||Applying application security standardsâa case study|Initiating a program to raise awareness of application development security issues within a globally dispersed organisation is a big challenge. This article describes how a combination of both technical and people skills can work towards achieving the goals of technical education, and awareness of corporate goals and policies.
23|1||TBSEâan engineering approach to the design of accurate and reliable security systems|For many years, the IT Security industry has been trying to devise a way to quantify risk and the benefits provided by security countermeasures in a form meaningful to senior business management. Threat-Based Security Engineering (TBSE) is a fresh approach to modelling and forecasting information security risk. TBSE takes a non-deterministic approach to modelling how security threats interact with countermeasures enabling quantitative forecasts of the likelihood and characteristics of security incidents as a direct function of the security measures employed. Preliminary results are encouraging and there appears to be no reason why the TBSE techniques could not be applied to a wide range of threats and countermeasures. Assuming they can, these techniques could become the foundation for a greatly needed disciplined engineering approach to the design of accurate and reliable security systems. Amongst the many other benefits, this would give senior business management the much sought after tools with which to oversee and direct corporate security expenditures. This article describes the TBSE approach and what it can do.
23|1||Events calender|
23|1||Mitigation of network tampering using dynamic dispatch of mobile agents|Detection of malicious activity by insiders, people with legitimate access to resources and services, is particularly difficult in a network environment. In this paper, a novel classification of tampering modes is identified that can be undertaken by insiders against network Intrusion Detection Systems (IDSs). Five categories of tampering modes are defined as spoofing, termination, sidetracking, alteration of internal data, and selective deception. These are further distinguished specifically toward IDS sensor, control, and alarm categories such as spoonfeeding, sugarcoating, and scapegoating.
23|1||An empirical investigation of network attacks on computer systems|In this paper we have analyzed data on reported network incidents at a large number of computer sites over time. We have examined various patterns in the incidents process such as the distribution of the inter-incident times, trends in the incident rates and variations with type of incident. Some suggestions for future data collection are included.
23|1||Restraining and repairing file system damage through file integrity control|Today, many security researches focus on survivable or intrusion-tolerant systems, which are not conceived to be invulnerable, but rather to be able to withstand the impact of an attack and continue providing critical services despite ongoing attacks. We join this fast-growing research community and use this article to present a solution of file integrity control which is able to detect unauthorized file system modifications as fast as possible and repair them in order to keep the system's integrity, availability and confidentiality.
23|1||Holistic security requirement engineering for electronic commerce|With the introduction of electronic commerce, business is becoming dependent on information systems in a new way. Information security is thus becoming more and more important to companies' self-protection. In contrast to previous systems, this is also directly visible to the customer. The changing situation means, however, that the requirements for security cannot be solely filled by new policies and risk analysis. This article proposes an approach called “holistic security requirement engineering” meant to elicit security requirements according to system-theoretic considerations. It will show that security requirements can be defined with the help of investigations in the business environment, workshops with stakeholders and risk analysis. This multidimensional approach will lead to a holistic understanding of the requirements that fit into the system development life cycles.
23|1||Biometric random number generators|Up to now biometric methods have been used in cryptography for authentication purposes. In this paper we propose to use biological data for generating sequences of random bits. We point out that this new approach could be particularly useful to generate seeds for pseudo-random number generators and so-called “key sessions”. Our method is very simple and is based on the observation that, for typical biometric readings, the last binary digits fluctuate “randomly”. We apply our method to two data sets, the first based on animal neurophysiological brain responses and the second on human galvanic skin response. For comparison we also test our approach on numerical samplings of the Ornstein–Uhlenbeck stochastic process. To verify the randomness of the sequences generated, we apply the standard suite of statistical tests (FIPS 140-2) recommended by the National Institute of Standard and Technology for studying the quality of the physical random number generators, especially those implemented in cryptographic modules. Additionally, to confirm the high cryptographic quality of the biometric generators, we also use the often recommended Maurer's universal test and the Lempel–Ziv complexity test, which estimate the entropy of the source. The results of all these verifications show that, after appropriate choice of encoding and experimental parameters, the sequences obtained exhibit excellent statistical properties, which opens the possibility of a new design technology for true random number generators. It remains a challenge to find appropriate biological phenomena characterized by easy accessibility, fast sampling rate, high accuracy of measurement and variability of sampling rate.
23|1||Erratum to âAttacks on the (enhanced) Yang-Shieh authenticationâ [Comput Secur 22(8) (2003) 725â727]|
23|2|http://www.sciencedirect.com/science/journal/01674048/23/2|Incident response teams need to change|
23|2||Security views|
23|2||Non-PKI methods for public key distribution|The X.509 certification authority-based (CA) public key infrastructure (PKI) is a widely accepted PKI standard which defines data formats and procedures related to the distribution of public keys via public key certificates that are digitally signed by CAs. However, X.509 requires a huge and expensive infrastructure with complex operations. This overhead may be tolerable in some cases, but it is highly desirable to find other solutions. The objective of this paper is to present alternative simpler solutions to the X.509 PKI to save storage, bandwidth and to reduce the complexity of the operations. We offer three such solutions. They rely on the existence of passwords that are known to both users and service providers.
23|2||Events calender|
23|2||A secure electronic voting protocol for general elections|Elections and voting behavior are always in our lives. As the activities increase on Internet, some elections and voting behavior would be brought on Internet. It is called “Electronic voting”. Electronic voting can solve the cost problem that occurs in traditional election. But electronic voting still has some problems like completeness, uncoercibility, non-cheating problems. In this paper, most properties of the electronic voting would be discussed and a new proposed electronic voting scheme would try to satisfy these properties and to solve problems mentioned. Furthermore, it also investigates what happens in the real world and how to solve these problems in practice.
23|2||Efficient user identification scheme with key distribution preserving anonymity for distributed computer networks|In 2000, Lee and Chang (Comput Syst Sci Eng 15 (2000) 211) presented a user identification scheme that also can simultaneously achieve key exchange requirement while preserving the user anonymity. Their idea is valuable especially when it is applied to some applications in which the identity of the user should be protected from the public in the distributed computer networks. Unfortunately, our paper shows that their scheme is insecure under two attacks and we further proposed a more efficient identification scheme preserving the same merits. The proposed scheme not only effectively eliminates the security leaks of the Lee–Chang scheme, but also reduces computational complexities and communication costs as compared with their scheme.
23|2||Symmetric RBAC model that takes the separation of duty and role hierarchies into consideration|RBAC is a family of reference models in which permissions are assigned to roles, and users are also assigned to appropriate roles. Studies on the permission–role part of RBAC model are relatively insufficient compared with those on the user–role part, and researches on symmetric RBAC models to overcome this is also in an incipient stage. Therefore there is difficulty in assigning permissions suitable for roles.
23|2||An alternative architectural framework to the OSI security model|In this paper an alternative framework to the OSI security model is presented. An identification of the principles governing security function assignment inside the OSI communication layers is given, followed by an analysis of the advantages of the security reference model. Also the IPsec and Stream Control Transmission Protocol (SCTP) architectures are briefly presented, illustrating their features and usages. The disadvantages and implementation pitfalls of the presented models are then brought forward, in relation to performance and security issues. The Future Core Networks System (FCNS) is presented, which constitutes the proposed reference architecture. The features of the FCNS are given, together with an analysis of the advantages our proposal exhibits with respect to the protocols presented, followed by the software implementation of our model. Results from simulations show that FCNS offers an improvement in throughput of at least 10% in comparison with currently used communication protocol stack architectures. These throughput benefits are achieved even when the full security measures of FCNS are in operation. Finally, we present the FCNS applicability in current network systems and reveal future work.
23|2||An analysis of the tools used for the generation and prevention of spam|This paper examines the problems caused by the spamming of e-mail and newsgroup users. Spamming is now considered to be a serious threat to the Internet and is posing a serious threat to both ISP and users' resources. In particular, this paper examines the motivation of, and the tools used to generate, spam. Methods of protection and prevention are then discussed. The paper includes case studies of some spam generation and prevention tools as well as examines evolving spam-related laws.
23|2||Efficient password authenticated key agreement using smart cards|The smart-card based remote user authentication and key agreement scheme is a very practical solution to create a secure distributed computer environment. In this paper, we propose a novel user authentication and key agreement scheme with much less computational cost and more functionality. The main merits include: (1) the scheme needs no verification table; (2) users can freely choose their own passwords; (3) the communication and computation cost is very low; (4) users and servers can authenticate each other; and (5) it generates a session key agreed by the user and the server. Also, our proposed scheme is a nonce-based scheme which does not have a serious time-synchronization problem.
23|2||An improvement of nonrepudiable threshold proxy signature scheme with known signers|In a (t, n) threshold proxy signature scheme, which is a variant of the proxy signature scheme, the proxy signature key is shared among a group of n proxy signers delegated by the original signer. Any t or more proxy signers can cooperatively sign messages on behalf of the original signer. In 2000, Hwang et al. (Int J Inf 11 (2000) 1) proposed a secure nonrepudiable threshold proxy signature scheme with known signers. In this paper, we point out a cryptanalysis of their scheme. Furthermore, we improve the security of the threshold proxy signature scheme which remedies the weakness of Hwang et al.'s scheme.
23|3|http://www.sciencedirect.com/science/journal/01674048/23/3|Worms and viruses: are we losing control?|
23|3||Security views|
23|3||Towards information security behavioural compliance|Auditing has always played an important role in the business environment. With the introduction of information technology and the resulting security challenges that organizations face daily, it has become essential to ensure the security of the organization's information and other valuable assets. However, one aspect that auditing does not cover effectively is that of the behaviour of the employee, which is so crucial to any organization's security.
23|3||A fair and secure mobile agent environment based on blind signature and proxy host|The advancement in information technology has made it possible to maintain fairness in on-line transactions such as on-line shopping and auctioning. However, traditional techniques have only been developed for authentication instead of for maintaining the fairness principle, which is defined in this paper as the equal treatment of authenticated mobile agents by service hosts. In other words, if the fairness principle were followed, service hosts, such as merchant hosts, auction hosts, and so on, would process the requests from authenticated mobile agents according to their time of arrival rather than according to the service hosts' own benefit.
23|3||The effect of intrusion detection management methods on the return on investment|This paper examines how implementation methods, management methods, and Intrusion Detection System (IDS) policy affect Return on Investment (ROI). The paper will seek to demonstrate the value associated with a well thought out implementation and effective lifecycle management of IDS technology and will culminate in a case study with a number crunching exercise to calculate the ROI for an IDS deployment by a hypothetical financial company named UTVE, Inc. on risk.
23|3||Digital signature of multicast streams secure against adaptive chosen message attack|We design a secure multicast stream signature scheme which can resist adaptive chosen message attack through splitting a multicast stream into a sequence of blocks. Firstly, we propose the definition of one-time block signature scheme and its construction based on one-time message signature scheme and hash tree. Secondly, we propose the definition of multicast stream signature scheme secure against adaptive chosen message attack and its construction based on traditional message signature scheme secure against adaptive chosen message attack and our one-time block signature scheme. Thirdly, we prove the securities of our one-time block signature scheme and multicast stream signature scheme. Finally, we analyze the performance of our multicast stream signature scheme.
23|3||Predicting the intrusion intentions by observing system call sequences|Identifying the intentions or attempts of the monitored agents through observations is very vital in computer network security. In this paper, a plan recognition method for predicting the anomaly events and the intentions of possible intruders to a computer system is developed based on the observation of system call sequences. The probability of the goal state for a system call sequence is defined as the prediction index to determine if the intention is normal. An efficient algorithm based on the dynamic Bayesian network theory with parameter compensation is derived and then applied to update the index recursively. Extensive empirical testing is performed on the data sets published in the literature and those collected in an actual computer system at our lab. The testing results showed that this method can identify the intrusion behaviors from the observed system call sequences with good accuracy.
23|3||Computer security impaired by legitimate users|Computer security has traditionally been assessed from a technical point of view. Another way to assess it is by investigating the role played by legitimate users of systems in impairing the level of protection. In order to address this issue, we wish to adopt a multidisciplinary standpoint and investigate some of the human aspects involved in computer security. From research in psychology, it is known that people make biased decisions. They sometimes overlook rules in order to gain maximum benefits for the cost of a given action. This situation leads to insidious security lapses whereby the level of protection is traded-off against usability. In this paper, we highlight the cognitive processes underlying such security impairments. At the end of the paper, we propose a short usability-centred set of recommendations.
23|3||Events calender|
23|4|http://www.sciencedirect.com/science/journal/01674048/23/4|Intrusion prevention|
23|4||Security views|
23|4||From policies to culture|Management normally sets company vision, rules and regulations through policies. These policies should provide guidance to employees and partners as to how they should act and behave to be in line with management's wishes. These policies need to be structured and organized effectively to cater for business and technological dynamics and advances. Having defined a series of company policies does not ensure that all employees will necessarily obey these policies. Ideally these policies must manifest in some company culture to ensure appropriate behaviour. This can only be achieved through a proper education process. This paper addresses exactly the process of integrating policies, education and culture.
23|4||Events calendar|
23|4||Tele-Lab âIT-Securityâ on CD: portable, reliable and safe IT security training|Besides gaining theoretical knowledge, IT students and professionals need to be prepared to apply security technologies and tools in their daily work. Therefore, today's security training should intend to provide hands-on experience by integrating practical exercises into the learning process. Tele-Lab “IT-Security” is a novel training system that makes interactive security exercises in a real laboratory environment possible that is equipped with rich security tools. Since for security tasks privileged operations have to be allowed, this laboratory environment needs to be carefully prevented from corruption by misuse or failures. To this end, we integrated Tele-Lab “IT-Security” including its operating system into a live system which is completely run on a small-sized CD without hard-disk installation. In this way, its portability, reliability and safety are also improved. Students can very easily access security training on any decent PC by booting Tele-Lab “IT-Security” from CD. Any activity in the training does not affect hardware and software systems; any system failure can be recovered by reboot.
23|4||Logical analysis of AUTHMAC_DH: a new protocol for authentication and key distribution|In the present paper, a new protocol for authentication and key distribution is proposed. The new protocol aims to achieve a comparable performance with the Kerberos protocol and to overcome its drawbacks. For authentication of messages exchanged during authentication and key distribution, the new protocol uses the message authentication codes (MAC) to exchange the Diffie–Hellman components. It has to be noted that the use of MAC will fasten the proposed protocol. On the other hand, the new protocol uses nonces to ensure the freshness of the exchanged messages. Subsequently, there is no need for clock synchronization which will simplify the system requirements. The new protocol is analyzed, using a logical tool, to ensure that it achieves the goals of authentication and key distribution as defined in the BAN logic. The analysis shows that the new protocol achieves the goals of authentication and key distribution without bugs.
23|4||Formal support for certificate management policies|Traditionally, creation and revocation of certificates are governed by policies that are carried manually, off-line, by trusted agents. This approach to certificate management is appropriate for many current applications, where these policies cannot be verified automatically (e.g. require verification of non-digital credentials). But it is expensive, time consuming and error-prone for the growing class of applications where certificate management policies can be formalized and carried out automatically. We argue that, in these cases, creation and revocation of certificates could be viewed as any other on-line service available in a system. Access to these particular service instances could be regulated much in the same manner as file access or resource allocation.
23|4||SAD: web session anomaly detection based on parameter estimation|Web attacks are too numerous in numbers and serious in potential consequences for modern society to tolerate. Unfortunately, current generation signature-based intrusion detection systems (IDS) are inadequate, and security techniques such as firewalls or access control mechanisms do not work well when trying to secure web services. In this paper, we empirically demonstrate that the Bayesian parameter estimation method is effective in analyzing web logs and detecting anomalous sessions. When web attacks were simulated with Whisker software, Snort, a well-known IDS based on misuse detection, caught only slightly more than one third of web attacks. Our technique, session anomaly detection (SAD), on the other hand, detected nearly all such attacks without having to rely on attack signatures at all. SAD works by first developing normal usage profile and comparing the web logs, as they are generated, against the expected frequencies. Our research indicates that SAD has the potential of detecting previously unknown web attacks and that the proposed approach would play a key role in developing an integrated environment to provide secure and reliable web services.
23|4||A scalable and distributed multicast security protocol using a subgroup-key hierarchy|In the present paper, a scalable protocol for securing multicast communication is proposed. The proposed protocol is based on the idea of dividing the whole group into smaller subgroups as in the Iolus protocol. For a member join or leave, the decomposition of the group into smaller subgroups will reduce the computation complexity from O(M), where M is the number of the whole group members, to O(N), where N is the number of the subgroup members. Moreover, each subgroup is organized in a logical key hierarchy as in the LKH protocol. The use of logical key hierarchy will reduce the computation complexity cost from O(N) to O(log(N)) in case of member leave/join. Furthermore, the number of communicating messages containing the changed keys will be reduced. The proposed protocol is compared with the two well-known protocols: Iolus and LKH. The comparison is undertaken according to two criteria: the cost of encryption required for the re-key operation in case of member join or leave and the length of the re-key message. The results show that the proposed protocol outperforms both the Iolus and the LKH protocols. Therefore, the proposed protocol will enhance the group performance in terms of computation and communication.
23|4||The design of a secure anonymous Internet voting system|In this paper, we propose a very practical and secure anonymous Internet voting protocol. Our scheme does not require a special voting channel and communications can occur entirely over the current Internet. This method integrates Internet convenience and cryptology. Issues such as the kinds of “certificate authority” and “public proxy server” are integrated in our scheme to solve the Internet identification and anonymity problems. This protocol combines the RSA blind signature scheme and secret sharing cryptosystem, to provide a fair and practical election.
23|4||Rico: a security proxy for mobile code|Security technology suitable for the burgeoning embedded system market has not been widespread. Untrusted code downloaded from the Internet poses numerous security risks due to the possible presence of viruses or other malicious entities. System administrators typically administer one or more administrative domains making policy management for mobile code a challenge because of the diverse security rules that must be adhered to. In this paper, we introduce Rico, a binary rewriting, security policy and code management system that sits between clients and servers. The system interposes itself between a client that downloads mobile code and the target server to provide the system administrator a means to secure untrusted code by rewriting it. The system supports the following features: (1) A security policy editor that simplifies policy writing with frame wizards, syntax reminders and error checking. (2) Third-party policy incorporation enabling reuse of security policies created by trusted third parties. (3) Policy composition, the capability of combining multiple security policies into one logical policy that can be applied to a mobile program. (4) Efficient security management supported by a graphical user interface and a self-training database.
23|5|http://www.sciencedirect.com/science/journal/01674048/23/5|SarbanesâOxleyâa huge boon to information security in the US|
23|5||Security views|
23|5||On risk: perception and direction|The idea of risk permeates the information security field. We use terms like “risk management”, “risk assessment”, “risk model” and “risk analysis” every day, and those topics are themselves the subject of countless papers and articles in security journals and magazines.
23|5||The 10 deadly sins of information security management|This paper identifies 10 essential aspects, which, if not taken into account in an information security governance plan, will surely cause the plan to fail, or at least, cause serious flaws in the plan. These 10 aspects can be used as a checklist by management to ensure that a comprehensive plan has been defined and introduced.
23|5||Events calender|
23|5||Search engines and privacy|Search engines have become a fundamental tool to access the vast amounts of information available in the World Wide Web in an optimized fashion. As they become ever more powerful, there has been concern on what this could mean for privacy issues, considering the accessibility to personal information in electronic format. This article addresses the nature of these concerns, attempting to clarify the issues at stake in a balanced view considering the position of all parties involved in the problem.
23|5||An English auction scheme in the online transaction environment|Internet technology in the recent years has progressed with great strides, and has transcended physical boundaries to achieve a global community. It has been an efficient tool in the development of modern communication, electronic commerce, and various living applications. Under an Internet environment, a reliable and high-performing English auction scheme is presented in the research involving three parties, namely the Registration Manager, Auction Manager, and Bidder. The Registration Manager identifies and authenticates the bidder. The Auction Manager issues the bidding rights and maintains order during the auction. The proposed scheme has the following features: anonymity, traceability, no framing, unforgeability, non-repudiation, fairness, public verifiability, unlinkability among different auction rounds, linkability in a round of auction, efficiency of bidding, one-time registration, and easy revocation. Given the Internet environment, significant importance is attached to time costs in transmitting bidding data. Hence, the bulletin board method is used to enable both the registration and auction managers to declare the necessary parameters. Furthermore, the elliptic curve cryptosystem, owing to its low computational amount and small key size, is applied to the scheme. Consequently, the auction-manager server load can be effectively reduced, while simultaneously significantly increasing bidding efficiency.
23|5||Internet privacy law: a comparison between the United States and the European Union|The increasing use of personal information in Internet-based applications has created privacy concerns worldwide. This has led to awareness among policy makers in several countries of the desirability of harmonizing privacy laws. The greatest challenge to privacy legislation from an international perspective arises because, while the Internet is virtually borderless, legislative approaches differ from country to country.
23|5||Cumulative notarization for long-term preservation of digital signatures|The long-term preservation of digitally signed documents may be approached and analyzed from various perspectives, i.e. future data readability, signature validity, storage media longevity, etc. The paper focuses on technology and trust issues related to the long-term validation of a digital signature. We exploit the notarization paradigm and propose a mechanism for cumulative data notarization that results in a successive trust transition towards new entities, modern technologies, and refreshed data. A future relying party will have to trust only the information provided by the last notary, in order to verify the validity of the initial signature, thus eliminating any dependency on ceased entities, obsolete data, and weak old technologies. The proposed framework uses recursive XML elements so that a notarization token structure encapsulates an identical data structure containing a previous notarization token.
23|5||Cryptanalysis of a user friendly remote authentication scheme with smart cards|Recently, Sun proposed an efficient remote user authentication scheme by employing one-way hash function. In his scheme, the system does not need to maintain a password table and the communication cost of the scheme is low, but the user cannot freely choose a password. Therefore, Wu and Chieu improved Sun's scheme for enhancing the user's demand that the user can choose and change his password freely. However, in this paper, we point out that Wu and Chieu's improvement is vulnerable to the password guessing and forgery attacks.
23|5||Keystroke dynamics identity verificationâits problems and practical solutions|Password is the most widely used identity verification method in computer security domain. However, because of its simplicity, it is vulnerable to imposter attacks. Use of keystroke dynamics can result in a more secure verification system. Recently, Cho et al. (J Organ Comput Electron Commerce 10 (2000) 295) proposed autoassociative neural network approach, which used only the user's typing patterns, yet reporting a low error rate: 1.0% false rejection rate (FRR) and 0% false acceptance rate (FAR). However, the previous research had some limitations: (1) it took too long to train the model; (2) data were preprocessed subjectively by a human; and (3) a large data set was required. In this article, we propose the corresponding solutions for these limitations with an SVM novelty detector, GA–SVM wrapper feature subset selection, and an ensemble creation based on feature selection, respectively. Experimental results show that the proposed methods are promising, and that the keystroke dynamics is a viable and practical way to add more security to identity verification.
23|6|http://www.sciencedirect.com/science/journal/01674048/23/6|The case for one-time credentials|
23|6||Security views|
23|6||The implications of immunology for secure systems design|The immune system can be a powerful model for understanding and improving computer security. In this article we explore the analogy, starting with a description of early work at discovering “peptides” for computer systems in the form of sequences of system calls, and moving on to the implications of immunology for secure systems design. In particular, we discuss how the immune system tolerates errors, and how we can borrow these ideas to improve the robustness of our computer systems. We observe that a key aspect of the biology is that the immune system and the body have co-evolved so that the body is easier to protect; what we need is a similar co-evolution of computer systems and the methods we use to secure them.
23|6||Events Calendar|
23|6||The use and usability of direction-based filtering in firewalls|The common match fields in firewall rules refer to a packet's source and destination IP addresses, protocol, and source and destination port numbers. However, most firewalls are also capable of filtering based on a packet's direction: which network interface card the packet is crossing, and whether the packet is crossing the interface from the network into the firewall (“inbound”) or vice versa (“outbound”). Taking a packet's direction into account in the firewall's rules is extremely useful: it lets the firewall administrator protect against source address spoofing, write effective egress-filtering rules, and avoid unpleasant side-effects when referring to subnets that span the firewall.
23|6||Further analysis of password authentication schemes based on authentication tests|In this paper, we present further analysis of Yang–Shieh's password authentication schemes. At first, we formally analyze Yang–Shieh's two password authentication schemes on the basis of authentication tests to disclose the insecurity of the two schemes, and then give two kind of examples, one is our attack to the nonce-based scheme and the other is Chan–Cheng's attack (Comput Secur 21 (2002) 74) and Fan–Li–Zhu's attack (Comput Secur 21 (2002) 665) to the timestamp-based scheme. Secondly, we propose an amendment of the timestamp-based scheme to withstand the attacks of Chan–Cheng and Fan–Li–Zhu, and propose our improved nonce-based scheme. Finally, we formally analyze our two improved schemes with the authentication tests, and prove they are secure in password authentication. Our improved schemes preserve the merits of Yang–Shieh's schemes, and the improved timestamp-based scheme can withstand the attacks of Chan–Cheng and Fan–Li–Zhu, and the improved nonce-based scheme is able to prevent malicious replay attacks in the network without synchronized clock or with long transmission delay.
23|6||Peer-assisted carrying authentication (PACA)|In this paper we present a method for password recovery through the employment of multiple Web servers, and which we name Peer-Assisted Carrying Authentication (PACA). The paper starts by highlighting the vulnerabilities of the commonly used techniques for password recovery, namely the question–answer approach. It then proceeds to providing a general coverage of the proposed approach and discusses the details and offered solutions to issues that relate to implementation and security. We present a software application that we developed for proof-of-concept and as a tool for class-based experiments. These were conducted to show the ability of users to hack accounts of other users with whom they have or had some kind of relationship and test the effectiveness of piecewise password recovery. The results indicate that people who are close to others can often guess some of their passwords correctly and therefore, are able to hack their computer accounts. It is shown that PACA makes the hacker's job very difficult through the multiple peer authentication mechanism. In this regard, the findings could be used to set a lower bound on the number of peer sites for authenticating users.
23|6||Vulnerability forecastingâa conceptual model|Vulnerability scanners (VSs) are information security tools able to detect security weaknesses on hosts in a network. VSs secure hosts in a proactive manner. A proactive approach is considered to be better than reactive approaches followed by, for example, intrusion detection systems, because prevention is better than cure. There are many problems and disadvantages of currently available VSs, such as hampering system resources while conducting scans. This paper introduces a conceptual model for vulnerability forecasting. The model uses intelligent techniques to improve on the efficiency of currently available VSs. The model aims to do vulnerability forecasting specifically by predicting the number of known vulnerabilities that will occur in the near future by using intelligent techniques and vulnerability history data. The model is tested by means of a prototype and an evaluation of the model's results is also provided in the paper.
23|6||An operational model and language support for securing XML documents|In this paper we present an operational model for XML document security. Given an XML document X, the operational model defines the process of encrypting data and embedding digital signatures which sign the data in X. The secured XML document Xs includes encrypted and unencrypted data of X, and embedded digital signatures. The operational model also defines the processes of decrypting Xs and verifying the digital signatures embedded in Xs. It offers a security mechanism which integrates element-wise encryption and temporal-based element-wise digital signatures. Our operational model provides element-wise encryption that is more general than previous forms of XML security, by including element encryption, content encryption, and two types of attribute encryption. Moreover, the model of temporal-based element-wise digital signature is novel. Based on the generalized operational model, we define a new language—called document security language (DSL)—to support it. The syntax of the encrypted document and the corresponding transformation language are presented. For automation reasons, the DSL includes a definition for the “standard DSL algorithm downloading and linking protocol” which fulfills automatic algorithm download and linking requirements in the operational model. This makes the DSL based securing tool configurable. Two different implementations further demonstrate its practicability: one uses the Java programming language to implement the securing tool, whilst the other employs the extension mechanism of XSLT 1.0 to implement the encryption and decryption transforms. The two implementations are available free on the Internet. Experimental results obtained when using our securing tool demonstrate the automation, efficiency, and practicability of the proposal operational model. In addition, we have developed a DSL editor with a friendly graphic user interface to make it easier for users to generate DSL documents.
23|7|http://www.sciencedirect.com/science/journal/01674048/23/7|The gap between cryptography and information security: has it narrowed?|
23|7||Security views|
23|7||Improving the ROI of the security management process|This article provides a number of guidelines for improving the ROI of the information security process. Where security initiatives are concerned, an important component of this ROI is realised in the form of risk mitigation and it is important to include this as an explicit factor in the ROI calculation. This calculation need only be sufficiently accurate to support the decision making process. Distinguishing between tactical and strategic initiatives will enable organisations to respond to short-term business drivers without interrupting strategic projects. Organisations will then be able to concentrate on re-engineering current processes in order to better align them with business needs.
23|7||Events Calendar|
23|7||An approach to reliably identifying signs of DDOS flood attacks based on LRD traffic pattern recognition|In the aspect of intrusion detection, reliable detection remains a challenge issue as stated in Kemmrer and Vigna (Suppl IEEE Comput (IEEE Secur Priv) 35(4) (2002) 28). “The challenge is to develop a system that detects close to 100% of attacks with minimal false positives. We are still far from achieving this goal.” Hence, reliable detection of distributed denial-of-service (DDOS) attacks is worth studying. By reliable detection, we mean that signs of attacks can be identified with predetermined detection probability and false alarm probability. This paper focuses on reliable detection of DDOS flood attacks by identifying pattern of traffic with long-range dependence (LRD). In this aspect, there are three fundamental issues in theory and practice:
23|7||Towards Web Service access control|The Internet has revolutionised the capacity to share information and services across organisations. Web Service technology enables organisations to exploit software as a service. Services are accessed by method invocations. Method interfaces are described and published, and may be freely available. Method requests and responses are conveyed in SOAP, which has the ability to pass unhindered through firewalls. Applications that process SOAP requests may be endangered by messages with malicious intent. Protection of methods and resources exposed by SOAP is thus a critical requirement for Web Services to be acceptable to organisations. In Web Service environments, access control is required to cross the borders of security domains, to be implemented between heterogeneous systems. New approaches are required that would address the movement of unknown users across borders so that access to resources can be granted. Specifications have been released to address access control, but are not well established. In this paper, an analysis of current approaches to Web Service access control is made, which leads to five requirements to be addressed by future access control solutions. To address such requirements, a logic-based access control approach is defined for a Web Service endpoint. The paper does not address the access control logic that is required when more than one Web Service is used in an integrated business solution.
23|7||Enhanced three-party encrypted key exchange without server public keys|This investigation proposes a secure and efficient three-party encrypted key exchange (3PEKE) protocol based on the LSSH-3PEKE protocol proposed by Lin et al. [Lin, C.-L., Sun, H.-M., Steiner, M., Hwang, T., 2001. IEEE Commun. Lett. 5 (12), 497–499]. The computational cost of the proposed protocol is equal to that of the LSSH-3PEKE protocol. However, the number of steps in communication is one fewer. A round efficient version of the same protocol is also described.
23|7||Authentication and authorization infrastructures (AAIs): a comparative survey|In this article, we argue that traditional approaches for authorization and access control in computer systems (i.e., discretionary, mandatory, and role-based access controls) are not appropriate to address the requirements of networked or distributed systems, and that proper authorization and access control requires infrastructural support in one way or another. This support can be provided, for example, by an authentication and authorization infrastructure (AAI). Against this background, we overview, analyze, discuss, and put into perspective some technologies that can be used to build and operate AAIs. More specifically, we address Microsoft .NET Passport and some related activities (e.g. the Liberty Alliance Project), Kerberos-based solutions, and AAIs that are based on digital certificates and public key infrastructures (PKIs). We conclude with the observation that there is no single best approach for providing an AAI, that every approach has specific advantages and disadvantages, and that a comprehensive AAI must combine various technologies and approaches.
23|7||Filtering XPath expressions for XML access control|XPath is a standard for specifying parts of XML documents and a suitable language for both query processing and access control of XML. In this paper, we use the XPath expression for representing user queries and access control for XML. And we propose an access-control method for XML, where we control accesses to XML documents by filtering query XPath expressions through access-control XPath expressions. For filtering the access-denied parts out of query XPath expressions, set operations (such as, intersection and difference) between the XPath expressions are essential. However, it is known that the containment problem of two XPath expressions is coNP-hard when the XPath expressions contain predicates (or branch), wildcards and descendant axes. To solve the problem, we directly search XACT (XML Access Control Tree) for a query XPath expression and extract the access-granted parts. The XACT is our proposed structure, where the edges are structural summary of XML elements and the nodes contain access-control information. We show that the query XPath expressions are successfully filtered through the XACT by our proposed method, and also show the performance improvement by comparing the proposed method with the previous work.
23|7||Personalised cryptographic key generation based on FaceHashing|Among the various computer security techniques practice today, cryptography has been identified as one of the most important solutions in the integrated digital security system. Cryptographic techniques such as encryption can provide very long passwords that are not required to be remembered but are in turn protected by simple password, hence defecting their purpose. In this paper, we proposed a novel two-stage technique to generate personalized cryptographic keys from the face biometric, which offers the inextricably link to its owner. At the first stage, integral transform of biometric input is to discretise to produce a set of bit representation with a set of tokenised pseudo random number, coined as FaceHash. In the second stage, FaceHash is then securely reduced to a single cryptographic key via Shamir secret-sharing. Tokenised FaceHashing is rigorously protective of the face data, with security comparable to cryptographic hashing of token and knowledge key-factor. The key is constructed to resist cryptanalysis even against an adversary who captures the user device or the feature descriptor.
23|7||Improvement on Li et al.'s generalization of proxy signature schemes|Recently, Li et al. proposed their generalization of proxy signature schemes. However, all of Li et al.'s schemes have a common security weakness. In Li et al.'s schemes, an adversary first intercepts a valid proxy signature generated by a proxy group on behalf of the proxy group GP. From the intercepted proxy signature, the adversary can forge illegal proxy signatures being likely generated by the proxy group on behalf of an adversary. To overcome this weakness, our improvement is also proposed.
23|8|http://www.sciencedirect.com/science/journal/01674048/23/8|Is the U.S. government really getting serious about information security?|
23|8||Security views|
23|8||From secure wired networks to secure wireless networks â what are the extra risks?|This paper investigates the information security requirements for wireless networks, and then compares that with the information security requirements for wired networks. The extra requirements needed for wireless networks security are then identified and discussed.
23|8||A framework for the governance of information security|This paper highlights the importance of protecting an organization's vital business information assets by investigating several fundamental considerations that should be taken into account in this regard. Based on this, it is illustrated that information security should be a priority of executive management, including the Board and CEO and should therefore commence as a corporate governance responsibility. This paper, therefore, motivates that there is a need to integrate information security into corporate governance through the development of an information security governance (ISG) framework. This paper further proposes such a framework to aid an organization in its ISG efforts.
23|8||Events Calendar|
23|8||XML distributed security policy for clusters|With the increasing use of clusters, efficient and flexible security has now become an essential requirement, though it has not yet been addressed in a coherent fashion for distributed systems.
23|8||Access control in a hierarchy using one-way hash functions|This paper presents a cryptographic key management solution to solve the access control problem in a hierarchy. Based on one-way hash functions, an efficient key assignment and derivation method is proposed. This solution uses limited number of keys and hash functions. Also, the dynamic access control problems, such as adding/deleting nodes, or modifying relationships between nodes in the hierarchy are considered and can be resolved.
23|8||Characterization of defense mechanisms against distributed denial of service attacks|We propose a characterization of distributed denial of service (DDOS) defenses where reaction points are network-based and attack responses are active. The purpose is to provide a framework for comparing the performance and deployment of DDOS defenses. We identify the characteristics in attack detection algorithms and attack responses by reviewing defenses that have appeared in the literature. We expect that this characterization will provide practitioners and academia insights into deploying DDOS defense as network services.
23|8||Embedding biometric identifiers in 2D barcodes for improved security|Two-dimensional (2D) barcode symbology is an emerging technology used for compactly storing and retrieving information. These barcodes can be found on the back of drivers' licenses and are encoded with secure text data. Standard 2D barcode such as PDF417 uses upper and lowercase alphabets, numeric digits and special characters for encoding. Some barcodes also include a compressed photo of the individual. The visual quality of the compressed image is usually poor and occupies a large amount of space which greatly reduces the capacity needed for encoding text. This paper presents a novel approach for embedding uncompressed images in a standard PDF417 2D barcode using a blind digital watermarking technique. The text is encoded in the standard PDF417 format with error correction, while the face and fingerprint images are watermarked in the encoded 2D barcode. Experimental results show that the proposed technique effectively increased the standard capacity of the PDF417 2D barcode without altering the contents of the encoded data. The results also show that the visual quality of the extracted photo image is high. The extracted fingerprint image when compared with the original fingerprint using an AFIS system yielded a high matching score.
23|8||Modelling and solving the intrusion detection problem in computer networks|We introduce a novel anomaly intrusion detection method based on a Within-Class Dissimilarity, WCD. This approach functions by using an appropriate metric WCD to measure the distance between an unknown user and a known user defined respectively by their profile vectors. First of all, each user performs a set of commands (events) on a given system (Unix for example). The events vector of a given user profile is a binary vector, such that an element of this vector is equal to “1” if an event happens, and to “0” otherwise. In addition to this, each user's class k has a typical profile defined by the vector Pk, in order to test if a new user i defined by its profile vector Pi belongs to the same class k or not. The Pk vector is a weighted events vector Ek, such that each weight represents the number of occurrences of an event ek. If the “distance” dki (measured by a dissimilarity parameter) between an unknown profile Pi and a known profile Pk is reasonable according to a given threshold and to some constraints, then there is no intrusion. Else, the user i is suspicious. A simple example illustrates the WCD procedure. A survey of intrusion detection methods is presented.
23|8||New efficient user identification and key distribution scheme providing enhanced security|Apart from user identification and key distribution, it is very useful for the login process to achieve user anonymity. Recently, Wu and Hsu proposed an efficient user identification scheme with key distribution while preserving user anonymity by extending an earlier work of Lee and Chang. We however find out that the Wu and Hsu scheme has a serious weakness, which can be exploited by the service provider to learn the secret token of the user who requests services from the service provider. We further propose a scheme to overcome this limitation while attaining the same set of objectives as the previous works. Performance analyses have shown that efficiency in terms of both computation and communication is not sacrificed in our scheme.
23|8||A hybrid scheme for multicast authentication over lossy networks|For multicast communication, authentication is a challenging problem, since it requires that a large number of recipients must verify the data originator. Many of multicast applications are running over IP networks, in which several packet losses could occur. Therefore, multicast authentication protocols must resist packet loss. Other requirements of multicast authentication protocols are: to perform authentication in real-time and to have low communication and computation overheads. In the present paper, a hybrid scheme for authenticating real-time data applications, in which low delay at the sender is acceptable, is proposed. In order to provide authentication, the proposed scheme uses both public key signature and hash functions. It is based on the idea of dividing the stream into blocks of m packets. Then a chain of hashes is used to link each packet to the one preceding it. In order to resist packet loss, the hash of each packet is appended to another place in the stream. Finally, the first packet is signed. The proposed scheme resists packet loss and is joinable at any point. The proposed scheme is compared to other multicast authentication protocols. The comparison shows that the proposed scheme has the following advantages: first, it has low computation and communication overheads. Second, it has reasonable buffer requirements. Third, the proposed scheme has a low delay at the sender side and no delay at the receiver side, assuming no loss occurs. Finally, its latency equals to zero, assuming no loss occurs.
24|1|http://www.sciencedirect.com/science/journal/01674048/24/1|Security views: Malware update|
24|1||The concept of security and trust in electronic payments|The use of electronic communication channels to conduct businesses without the need for physical conduct or presence has already been established and accepted warmly. But the issue of paying electronically still remains risky and muddy. This article implicates the security and trust issues that are essential for every electronic payment mechanism in order to be accepted and established as a common medium of financial transactions.
24|1||Management of risk in the information age|Linked together, organisations can exchange information and engage in transactions in ways unanticipated before, the emphasis being on information, which became core to most business activities and without which business will fail to operate [Owens S. Information security management :an introduction. London: British Standards Institution; 1998. pp. 1–2]. Consequently, to contribute to ensuring business continuity, the protection of information resources had to be pursued. Risk analysis was traditionally used to analyse risks posing a threat to mostly IT assets [Jung C, Han I, Suh B. Risk analysis for electronic commerce using case-based reasoning. International Journal of Intelligent Systems in Accounting, Finance & Management 1999;8:61–73. John Wiley & Sons, Ltd., p. 62]. Resulting in recommendations for the implementation of appropriate security measures, to reduce those identified high priority risks to an acceptable level. However, Bandyopadhyay et al. [Bandyopadhyay K, Mykytyn PP, Mykytyn K. A framework for integrated risk management in information technology. Management Decision 1999;37(5):437–44. MCB Press, p. 440] state that the evaluation of risk related to IT alone is unrealistic. A holistic view of assessing risks should instead be adopted, moving away from the isolated and partial view of today's “closed world assumption” of searching only within a specific domain to evaluate the risks associated to IT, to consider the entire spectrum related to the IT environment. Thus an alternative approach to risk analysis might have to be developed, to assist in analysing risks to information-specific resources.
24|1||Events Calendar|
24|1||A taxonomy of network and computer attacks|Attacks over the years have become both increasingly numerous and sophisticated. This paper focuses on the provisioning of a method for the analysis and categorisation of both computer and network attacks, thus providing assistance in combating new attacks, improving computer and network security as well as providing consistency in language when describing attacks. Such a taxonomy is designed to be useful to information bodies such as CERTs (Computer Emergency Response Teams) who have to handle and categorise an every increasing number of attacks on a daily basis. Information bodies could use the taxonomy to communicate more effectively as the taxonomy would provide a common classification scheme. The proposed taxonomy consists of four dimensions which provide a holistic taxonomy in order to deal with inherent problems in the computer and network attack field. The first dimension covers the attack vector and the main behaviour of the attack. The second dimension allows for classification of the attack targets. Vulnerabilities are classified in the third dimension and payloads in the fourth. Finally, to demonstrate the usefulness of this taxonomy, a case study applies the taxonomy to a number of well known attacks.
24|1||A randomized RSA-based partially blind signature scheme for electronic cash|Blind signature schemes can yield a signature and message pair whose information does not leak to the signer. However, when blind signatures are used to design e-cash schemes, there are two problems. One is the unlimited growth of the bank's database which keeps all spent e-cashes for preventing double spending. Another problem is that the signer must assure himself that the message contains accurate information such as the face value of the e-cash without seeing it. Partially blind signatures can cope with these problems. In partially blind signatures, the signer can explicitly include some agreed common information such as the expiration date and the face value in the blind signature. Randomized signature schemes can withstand one-more-forgery under the chosen plaintext attack. Based on RSA cryptosystem Fan–Chen–Yeh proposed a randomized blind signature scheme and Chien–Jan–Tseng also proposed a randomized partially blind signature scheme. But, the attacker can remove the randomizing factor from the messages to be signed in these two schemes. The attacker can also change the common information of Chien–Jan–Tseng's partially blind signature. In this paper, we propose a secure randomized RSA-based partially blind signature scheme, and show that the proposed scheme satisfies the blindness and unforgeability properties. We also analyse the computation cost of the proposed scheme.
24|1||An improvement of HwangâLeeâTang's simple remote user authentication scheme|Recently, Hwang–Lee–Tang proposed a simple remote user authentication scheme using smart card, whereby it does not require any password or verification tables in the remote system and any legal users could choose and change their passwords freely. However, their schemes previously generated user's secret hash values are insecure if the secret key of the server is leaked or is stolen, also when the smart card is stolen, unauthorized users can easily change new password of the smart card. Furthermore, their scheme cannot resist the denial of service attack using stolen smart card and does not provide mutual authentication. Accordingly, the current paper demonstrates the vulnerability of Hwang–Lee–Tang's scheme and presents an enhancement to resolve such problems. As a result, the proposed scheme previously generated secret hash values are secure even if the secret key of the system is leaked or is stolen and enables users to update their passwords freely and securely, while also providing mutual authentication and fast detect it when user inputs wrong password. In addition, the computational costs of this scheme are less than those of any previously proposed schemes.
24|1||H2A: Hybrid Hash-chaining scheme for Adaptive multicast source authentication of media-streaming|Many applications, such as broadcasting stock quotes and video-conferencing require data source authentication of the received multicast traffic. Multicast data source authentication must take into consideration the scalability and the efficiency of the underlying cryptographic schemes and mechanisms, because multicast groups can be very large and the exchanged data are likely to be important in volume (streaming). Besides, multicast data source authentication must be robust enough against packet loss because most of multicast multimedia applications do not use reliable packet delivery.
24|1||Information security obedience: a definition|Information is a fundamental asset within any organisation and the protection of this asset, through a process of information security, is of equal importance. This paper examines the relationships that exist between the fields of corporate governance, information security and corporate culture. It highlights the role that senior management should play in cultivating an information security conscious culture in their organisation, for the benefit of the organisation, senior management and the users of information.
24|1||Improvements on the WTLS protocol to avoid denial of service attacks|The current WTLS protocol is closely modeled after the well-studied SSL protocol. However, since some differences exist between these two protocols, even if the SSL protocol is secure, the WTLS protocol may not.
24|1||Multiplexer-based double-exponentiation for normal basis of GF(2m)|In many cryptographic protocols, double-exponentiation is a key arithmetic operation. In this study, we will present a multiplexer-based algorithm for double-exponentiation in GF(2m). The proposed algorithm utilizes the concept of the modified Booth's algorithm. Multiplexers are employed for implementation of the proposed algorithm. The proposed double-exponentiation algorithm only requires m multiplications and saves about 66% time complexity while comparing with the ordinary binary method.
24|2|http://www.sciencedirect.com/science/journal/01674048/24/2|Search engines: a growing contributor to security risk|
24|2||Lycos crosses the line|
24|2||Security views: Malware Update|
24|2||Information Security governance: COBIT or ISO 17799 or both?|This paper investigates the co-existence of and complementary use of COBIT and ISO 17799 as reference frameworks for Information Security governance. The investigation is based on a mapping between COBIT and ISO 17799 which became available in 2004, and provides a level of ‘synchronization’ between these two frameworks.
24|2||The economic approach of information security|This article introduces to the reader the sceptic of the economic evaluation of a security framework. We identify that there must be an economic evaluation of security investment, in order to avoid cost and risks of a security breach. We vindicate why the security economic plan must encompass our choices to provide security solutions. Furthermore, what are the measurements that are employed to provide the confidence of security to an acceptable level.
24|2||Keyjacking: the surprising insecurity of client-side SSL|In theory, PKI can provide a flexible and strong way to authenticate users in distributed information systems. In practice, much is being invested in realizing this vision via client-side SSL and various client keystores. However, whether this works depends on whether what the machines do with the private keys matches what the humans think they do: whether a server operator can conclude from an SSL request authenticated with a user's private key that the user was aware of and approved that request. Exploring this vision, we demonstrate via a series of experiments that this assumption does not hold with standard desktop tools, even if the browser user does all the right things. A fundamental rethinking of the trust, usage, and storage model might result in more effective tools for achieving the PKI vision.
24|2||Analysis of end user security behaviors|Many information security specialists believe that promoting good end user behaviors and constraining bad end user behaviors provide one important method for making information security effective within organizations. Because of the important of end user security-related behaviors, having a systematic viewpoint on the different kinds of behavior that end users enact could provide helpful benefits for managers, auditors, information technologists, and others with an interest in assessing and/or influencing end user behavior. In the present article, we describe our efforts to work with subject matter experts to develop a taxonomy of end user security-related behaviors, test the consistency of that taxonomy, and use behaviors from that taxonomy to conduct a U.S. survey of an important set of end user behaviors. We interviewed 110 individuals who possessed knowledge of end user security-related behaviors, conducted a behavior rating exercise with 49 information technology subject matter experts, and ran a U.S. survey of 1167 end users to obtain self-reports of their password-related behaviors. Results suggested that six categories of end user security-related behaviors appeared to fit well on a two-dimensional map where one dimension captured the level of technical knowledge needed to enact the behavior and another dimension captured the intentionality of the behavior (including malicious, neutral, and benevolent intentions). Our U.S. survey of non-malicious, low technical knowledge behaviors related to password creation and sharing showed that password “hygiene” was generally poor but varied substantially across different organization types (e.g., military organizations versus telecommunications companies). Further, we documented evidence that good password hygiene was related to training, awareness, monitoring, and motivation.
24|2||Cryptanalyses of two key assignment schemes based on polynomial interpolations|Wu and Chang [2001. Cryptographic key assignment scheme for hierarchical access control. International Journal of Computer Systems Science and Engineering 16(1), 25–28] and Shen and Chen [2002. A novel key management scheme based on discrete logarithms and polynomial interpolations. Computers & Security 21(2), 164–171] separately proposed a cryptographic key assignment scheme based on polynomial interpolations to solve the access control problem in a partially ordered user hierarchy. Recently, Hsu and Wu [2003. Cryptanalyses and improvements of two cryptographic key assignment schemes for dynamic access control in a user hierarchy. Computers & Security 22(5), 453–356] cryptanalyzed both schemes by finding the roots of polynomials. Then they proposed some modifications to fix the security flaw. In this paper, we show that both Wu–Chang and Shen–Chen schemes are still insecure even with these modifications. From the public information, the attacker could derive the secret keys of some security classes. Furthermore, the attack could be mounted even without using any polynomial root finding algorithm.
24|2||Dealing with packet loss in the Interactive Chained Stream Authentication protocol|This paper presents an improvement to the Interactive Chained Stream Authentication (I-CSA) protocol which is an efficient protocol useful to authenticate multicast transmissions over a network. The lockstep behavior intrinsic in its structure for data exchange may produce deadlocks, but not security breaches, in case the authentication information of the protocol is destroyed or modified by an intruder. The solution proposed (Enhanced I-CSA) introduces some modifications to the I-CSA protocol. This solution allows the parties involved in the transmission to continue the exchange of data in a secure manner even in case of packet losses, by accepting a reduced efficiency with respect to the I-CSA protocol.
24|2||ISRAM: information security risk analysis method|Continuously changing nature of technological environment has been enforcing to revise the process of information security risk analysis accordingly. A number of quantitative and qualitative risk analysis methods have been proposed by researchers and vendors. The purpose of these methods is to analyze today's information security risks properly. Some of these methods are supported by a software package. In this study, a survey based quantitative approach is proposed to analyze security risks of information technologies by taking current necessities into consideration. The new method is named as Information Security Risk Analysis Method (ISRAM). Case study has shown that ISRAM yields consistent results in a reasonable time period by allowing the participation of the manager and staff of the organization.
24|2||Empirical evaluation of SVM-based masquerade detection using UNIX commands|Masqueraders who impersonate other users pose serious threat to computer security. Unfortunately, firewalls or misuse-based intrusion detection systems are generally ineffective in detecting masqueraders. Although anomaly detection techniques have long been considered as an effective approach to complement misuse detection techniques, they are not widely used in practice due to poor accuracy and relatively high degree of false alarms. In this paper, we performed an empirical study investigating the effectiveness of SVM (support vector machine) in detecting masquerade activities using two different UNIX command sets used in previous studies [R. Maxion, N. Townsend, Proceedings of international conference on dependable systems and networks (DSN-02), p. 219–28, June 2002; R. Maxion, Proceedings of international conference on dependable systems and networks (DSN-03), p. 5–14, June 2003]. Concept of “common commands” was introduced as a feature to more effectively reflect diverse command patterns exhibited by various users. Though still imperfect, we detected masqueraders 80.1% and 94.8% of the time, while the previous studies reported the accuracy of 69.3% and 62.8%, respectively, using the same data set containing only the command names. When command names and arguments were included in the experiment, SVM-based approach detected masqueraders 87.3% of the time while the previous study, using the same data set, reported 82.1% of accuracy. These combined experiments convincingly demonstrate that SVM is an effective approach to masquerade detection.
24|2||An improvement on efficient anonymous auction protocols|In this paper, we propose an improvement on Chang et al.'s efficient anonymous auction protocols to overcome the security weakness in initiation phase of Chang et al.'s scheme. At first, we formally analyze the initiation phase of Chang et al.'s scheme on the basis of authentication tests to disclose the insecurity of the initiation phase. Then we give our attack to the initiation phase, which can make the following auction phase fail to run. Later, we propose our improvement on the initiation phase. Finally, we formally analyze our improved scheme with authentication tests, and prove the security of our improved scheme. Our improved scheme can preserve the merits of Chang et al.'s scheme and overcome the security weakness in initiation phase of Chang et al.'s scheme.
24|3|http://www.sciencedirect.com/science/journal/01674048/24/3|Security dilemmas with Microsoft's Internet Explorer|
24|3||Security views - Malware update|
24|3||Smart card based authentication â any future?|
24|3||Performance of the Java security manager|The Java Security Manager is one major security feature of the Java programming language. However, in many Java applications the Security Manager is not enabled because it slows execution time. This paper explores the performance of the Java Security Manager in depth, identifies the permissions with the worst performance and gives advice on how to use the Security Manager in a more efficient way.
24|3||Secure business application logic for e-commerce systems|The major reason why most people are still sceptical about e-commerce is the perceived security and privacy risks associated with e-transactions, e.g., data, smart cards, credit cards and exchange of business information by means of online transactions. Today, vendors of e-commerce systems have relied solely on secure transaction protocols such as SSL, while ignoring the security of server and client software. This article, Secure Business Application Logic for e-commerce Systems, discusses a key weak link in e-commerce systems: the business application logic. Although the security issues of the front-end and back-end software systems in e-commerce application warrant equal attention, but this research focuses on the Security of Middle Tier of e-commerce server that implements the business application logic and traditionally, e-commerce sites implemented the middle tier of software on the web server using CGI. We also present strategies for secure business application logic: good design and engineering, secure configuration, defensive programming and secure wrappers for server-side software.
24|3||Multiple behavior information fusion based quantitative threat evaluation|How to evaluate network security threat quantitatively is one of key issues in the field of network security, which is vital for administrators to make decision on the security of computer networks. A novel model of security threat evaluation with a series of quantitative indices is proposed on the analysis of prevalent network intrusions. This model is based on multiple behavior information fusion and two indices of privilege validity and service availability that are proposed to evaluate the impact of prevalent network intrusions on system security, so as to provide security evolution over time, i.e., monitor security changes with respect to modification of security factors. The Markov model and the algorithm of D-S evidence reasoning are proposed to measure these two indices, respectively. Compared with other methods, this method mitigates the impact of unsuccessful intrusions on threat evaluation. It evaluates the impact of important intrusions on system security comprehensively and helps administrators to insight into intrusion steps, determine security state and identify dangerous intrusion traces. Testing in a real network environment shows that this method is reasonable and feasible in alleviating the tremendous task of data analysis and facilitating the understanding of the security evolution of the system for its administrators.
24|3||Matching key recovery mechanisms to business requirements|This paper addresses the business needs for key recovery as a countermeasure to the threat of losing potentially valuable information. Several requirements essential for a sound key recovery mechanism are described, and the applicability of two main classes of existing key recovery schemes to a corporate environment is examined. Different requirements are identified for key recovery mechanisms for communicated and archived data, and a further study is made of the applicability of existing mechanisms to these two cases.
24|3||Information systems security policies: a contextual perspective|The protection of information systems is a major problem faced by organisations. The application of a security policy is considered essential for managing the security of information systems. Implementing a successful security policy in an organisation, however, is not a straightforward task and depends on many factors. This paper explores the processes of formulating, implementing and adopting a security policy in two different organisations. A theoretical framework based on the theory of contextualism is proposed and applied in the analysis of these cases. The contextual perspective employed in this paper illuminates the dynamic nature of the application of security policies and brings forth contextual factors that affect their successful adoption.
24|4|http://www.sciencedirect.com/science/journal/01674048/24/4|Personal information compromises: It is time for the U.S. Government to wake up|
24|4||Security views|
24|4||From information security toâ¦business security?|This short opinion paper argues that information security, the discipline responsible for protecting a company's information assets against business risks, has now become such a crucial component of good Corporate Governance, that it should rather be called Business Security instead of Information Security.
24|4||Why users cannot use security|With an increasing range of potential threats, the use of security within end-user systems and applications is becoming ever more important. However, a significant obstacle to achieving this can be the usability of the security features that are offered, and although related functionality is now provided in a wide range of end-user applications, the users themselves will fail to benefit if they cannot make it work for them. This paper highlights the importance of enabling users to protect themselves, and identifies that they may currently encounter problems in terms of finding, understanding, and ultimately using the security features that are meant to be at their disposal. The security options within Microsoft Word are used to provide illustrative examples of typical problems, with consequent suggestions to improve both the presentation and guidance available to users within such applications.
24|4||Infection dynamics on the Internet|In previous works, the connectivity of nodes in social networks such as the Internet has been shown to follow a scale-free distribution in which there is a larger probability of nodes with lower connectivity and a smaller probability of nodes with higher connectivity. This network structure facilitates communication but also aids in the propagation of viruses. In this work, solutions have been obtained for a dynamical mean-field equation that characterizes virus infections and growth in scale-free networks. In contrast to previous findings, a threshold condition has been found for the persistence of computer infections. The effect of connectivity-dependent growth and recovery rates is also reported. It has been found that it is possible to reduce the deleterious effects of viruses by preferentially discouraging growth and enhancing recovery in high-connectivity nodes. Significantly, a security “figure-of-merit” has been derived that will allow network administrators to sample their environment in real time and measure the risk relative to E-mail-borne threats.
24|4||Real-time intrusion detection for high-speed networks|Network-based intrusion detection systems (NIDSs) frequently have problems with handling heavy traffic loads in real-time, which result in packet loss and false negatives. This paper presents a high-performance network intrusion detection system, called HPMonitor, which combines a high-efficiency detection engine and a load-balancing device to address these problems. The paper describes HPMonitor's system architecture, discusses a flow-based dynamic load-balancing algorithm called dynamic least load first (DLLF) algorithm, and introduces a new multi-pattern string matching algorithm called shift max algorithm (SMA). The test results reveal that the DLLF algorithm is an effective balancing algorithm for NIDS. Meanwhile, the experimental results show that the SMA algorithm is faster in searching large sets of patterns when compared with other algorithms, and its performance is affected little when the patterns set number increases.
24|4||Feature deduction and ensemble design of intrusion detection systems|Current intrusion detection systems (IDS) examine all data features to detect intrusion or misuse patterns. Some of the features may be redundant or contribute little (if anything) to the detection process. The purpose of this study is to identify important input features in building an IDS that is computationally efficient and effective. We investigated the performance of two feature selection algorithms involving Bayesian networks (BN) and Classification and Regression Trees (CART) and an ensemble of BN and CART. Empirical results indicate that significant input feature selection is important to design an IDS that is lightweight, efficient and effective for real world detection systems. Finally, we propose an hybrid architecture for combining different feature selection algorithms for real world intrusion detection.
24|4||Secure information systems development â a survey and comparison|Nowadays, security solutions are mainly focused on providing security defences (such as firewalls, routers, configuration server, password and encryption) instead of solving one of the main reasons of security problems that refers to an appropriate information systems design. Fortunately, there have been developed new methodologies incorporating security into their development processes. This paper makes a comparison of eleven secure systems design methodologies. The analysed methodologies fulfil criteria partially and in this paper, we make it clear that security aspects cannot be completely specified by these methodologies since they have a series of limitations that we have to take into account. At the same time, each one of these methodologies comprises very important aspects concerning security that can be used as a basis for new methodologies or extensions that may be developed.
24|4||Information Assurance for security protocols|Security protocols are used pervasively to protect distributed communications in the third Millennium. This motivates the need for a definition of Information Assurance for security protocols, which, to the best of our knowledge, is still missing. Such a definition is advanced in terms of the requirements that security protocols be analysed at the same time realistically, accurately and formally, notions that the existing literature only favours in separate contexts. The precise meanings of these terms are described by means of general considerations and concrete examples. The main goal of this paper is to draw attention to and raise concern on this novel but significant niche of computer security.
24|4||A survey and trends on Internet worms|With the explosive growth and increasing complexity of network applications, the threats of Internet worms against network security are more and more serious. This paper presents the concepts and research situations of Internet worms, their function component, and their execution mechanism. It also addresses the scanning strategies, propagation models, and the critical techniques of Internet worm prevention. Finally, the remaining problems and emerging trends in this area are also outlined.
24|5|http://www.sciencedirect.com/science/journal/01674048/24/5|Non-infosec professionals in infosec?|
24|5||Security views|
24|5||Technology evolution drives need for greater information technology security|Securing the enterprise perimeter is not enough; we must secure the data itself. This simple idea has taken decades to become a practical technology, and is built upon a foundation of policies and processes.
24|5||The five Ps of patch management: Is there a simple way for businesses to develop and deploy an advanced security patch management strategy?|
24|5||Recent attacks on alleged SecurID and their practical implications|SecurID tokens are developed by SDTI/RSA Security to authenticate users to a corporate computer infrastructure. In this paper we show the results of our analysis of the function contained in these tokens. The block cipher at the heart of the function can be broken in milliseconds. We present two attack scenarios on the full function: if one can observe the output of the device during some time period, one can predict with high probability future output values and one can recover the secret key significantly faster than by exhaustive search.
24|5||A preliminary model of end user sophistication for insider threat prediction in IT systems|The dangers that originate from acts of IT system misuse by legitimate users constitute a separate category of threats with well documented consequences for the integrity, privacy and availability of computer systems and networks. Amongst the various properties of malicious legitimate users one of the most notable ones is the level of his/her sophistication. Various studies indicate that user sophistication and the potential to misuse IT systems are properties that are strongly related. This paper presents a methodology that automates the process of gauging end user sophistication. The establishment of suitable metrics to characterize end user sophistication is discussed followed by an experimental verification of the metrics on a sample of 60 legitimate users, using the UNIX Operating System. The results indicate that a combination of application execution audits and computational resource utilization metrics could be used to characterize the level of IT sophistication of an end user. Although additional testing in a greater variety of computational environments is required in order to validate the derived preliminary scheme, it is considered that the derived methodology could serve as a component of experimental insider threat prediction processes, or any other model that requires a procedure to measure the level of IT knowledge of a legitimate user base.
24|5||Secure authentication scheme for session initiation protocol|The Session Initiation Protocol provides an expandable and easy solution to the IP-based telephony environment. When users ask to use an SIP service, they need to be authenticated in order to get service from the server. Therefore, some SIP authentication procedure schemes were proposed to meet the above demand. However, there are security problems that need to be solved, such as off-line password guessing attacks and server spoofing. In this article, we shall propose a new scheme for a secure authentication procedure for the Session Initiation Protocol to enhance the security of the original scheme.
24|5||CIDS: An agent-based intrusion detection system|The paper describes security agent architecture, called CIDS, which is useful as an administrative tool for intrusion detection. Specifically, it is an agent-based monitoring and detection system, which is developed to detect malfunctions, faults, abnormalities, misuse, deviations, intrusions, and provide recommendations (in the form of common intrusion detection language). The CIDS can simultaneously monitor networked-computer activities at multiple levels (user to packet level) in order to find correlation among the deviated values (from the normal or defined policy) to determine specific security violations. The current version of CIDS (CIDS 1.4) is tested with different simulated attacks in an isolated network, and some of those results are reported here.
24|5||Two-level controllers hierarchy for a scalable and distributed multicast security protocol|LKH protocols are considered one of the best solutions proposed for solving the scalability of multicast security protocols. It has been proved that binary trees have the best computation and communication overheads. For a binary tree, the computation and communication overheads – in case of a member join or leave – of LKH protocols is about 2h encryption operations, where h represents the tree height. Many enhancements to LKH protocols are proposed in order to lower the computation and communication overheads. One of these solutions, a protocol that we named CEKPS protocol achieves a lower computation overhead, which is about h encryption operations and h one-way functions. In addition, its communication overhead is h messages. The abovementioned protocols rely on one manager, which could represent a bottleneck in case of a group with large number of members and where many join/leave operations occur. In the present paper, we propose a protocol for a scalable and distributed multicast security protocol. The aim of the proposed protocol is to achieve a lower computation overhead compared to CEKPS protocol. In order to achieve its goal, the proposed protocol relies on two levels of managers in order to distribute the computation required in case of a member join or leave. The proposed protocol is based on the idea of LKH protocols. To fasten the operations required in case of a member leave or join, the proposed protocol uses one-way functions as in CEKPS protocol. The proposed protocol is compared with LKH and CEKPS protocols. The comparison is undertaken according to two criteria: the cost of encryption required for the re-key operation in case of member join or leave and the length of the re-key message. The results show that the proposed protocol outperforms both LKH and CEKPS protocols.
24|5||Capital market reaction to defective IT products: The case of computer viruses|Studies in various industries indicate that market reaction to recall announcements is used as a catalyst to control the creation of substandard products. In the IT industry, flawed software is being blamed for the increasing numbers of computer viruses that plague information systems and the escalating costs to repair these viruses. This paper examines whether the market penalizes firms that produce substandard IT products. We use the event study methodology to assess the impact of public virus announcements on the stock prices of responsible IT vendors between 1988 and 2002. The results show that the market reacts negatively to the production of flawed Information Technology in approximately 50% of the cases. However, this negative market reaction is not statistically significant over extended periods and is limited to announcements involving certain types of defects (i.e., IT products that contain computer viruses). There was no statistically significant negative market reaction for announcements involving IT products that are susceptible to computer viruses. Our analysis implies that unlike in other industries, market forces alone cannot be used as an effective control mechanism for the production of substandard IT products. The study concludes that under these present conditions, IT vendors have little economic incentives to invest in defect-free computing.
24|6|http://www.sciencedirect.com/science/journal/01674048/24/6|The human factor in security|
24|6||Security views|
24|6||Telecom fraud: The cost of doing nothing just went up|Craig Pollard, head of project management at Siemens Communications examines the increasing threat of telephony fraud and highlights the vulnerabilities to IT managers.
24|6||Host intrusion prevention: Part of the operating system or on top of the operating system?|Instrusion prevention systems (IPS) are becoming essential for securing information technology (IT). However, IPS will never become a fully integral part of the operational system, because of the complexity of the problem, the changing nature of threats, and the dependence of IPS on the particulars of the applications being protected. But certain IPS functionality is likely to move into the operating system, namely innate defenses that offer simple protection against vulnerability classes, and IPS plumbing, i.e. the IPS components that are currently in kernel modules. The integration of these aspects of IPS with the operating system will result in an OS that is more adaptable and easier to develop new innovative IPS technologies upon.
24|6||Information Security Governance â Compliance management vs operational management|This paper discusses the difference that should exist between Information Security Operational Management and Information Security Compliance Management.
24|6||Information security policy's impact on reporting security incidents|The New Health Privacy Rule, effective from April 14, 2003, has made it illegal for healthcare providers and insurers to release a patient's medical records without the individual's consent [Cropper, Carol Marie. How to keep prying eyes off your medical records. Business Week November 19, 2001;130–2]. Rule provisions dictate that healthcare providers and insurers must have a written information security policy and present it to patients [Cropper, Carol Marie. How to keep prying eyes off your medical records. Business Week November 19, 2001;130–2].
24|6||A novel digital image watermarking scheme based on the vector quantization technique|In this paper, a novel VQ-based digital image watermarking scheme is proposed. During the encoding process of the VQ compression technique, the proposed scheme embeds a representative digital watermark in the protected image so that the watermark can be retrieved from the image to effectively prove which party is in legal possession of the copyright in case an ownership dispute arises. In our method, the codewords in the VQ codebook are classified into different groups according to different characteristics and then each binary watermark bit is embedded into the selected VQ encoded block. The main feature of the proposed scheme is that the watermark exists both in the VQ compressed image and in the reconstructed image after VQ decoding. Because the watermark is hidden inside the compressed image, which is much smaller in size, much transmission time and storage space can be saved when the compressed data, instead of the original form, are transmitted over the Internet. Furthermore, the reconstructed image has robustness against aggressive image processing. The embedded watermark can even survive JPEG lossy compression.
24|6||The insider threat to information systems and the effectiveness of ISO17799|Insider threat is widely recognised as an issue of utmost importance for IS security management. In this paper, we investigate the approach followed by ISO17799, the dominant standard in IS security management, in addressing this type of threat. We unfold the criminology theory that has designated the measures against insider misuse suggested by the standard, i.e. the General Deterrence Theory, and explore the possible enhancements to the standard that could result from the study of more recent criminology theories. The paper concludes with supporting the argument for a multiparadigm and multidisciplinary approach towards IS security management and insider threat mitigation.
24|6||Can critical infrastructures rely on the Internet?|The functionality of critical infrastructures (CI) depends more and more on an operative Internet, which has to secure data packet transport even during Internet Exchange Point (IX) failures and “worm” attacks. The paper focuses on the questions:
24|6||A distributed systems approach to secure Internet mail|One of the obstacles to improve security of the Internet is ad hoc development of technologies with different design goals and different security goals. This paper proposes reconceptualizing the Internet as a secure distributed system, focusing specifically on the application layer. The notion is to redesign specific functionality, based on principles discovered in research on distributed systems in the decades since the initial development of the Internet. Because of the problems in retrofitting new technology across millions of clients and servers, any options with prospects of success must support backward compatibility. This paper outlines a possible new architecture for internet-based mail which would replace existing protocols by a more secure framework. To maintain backward compatibility, initial implementation could offer a web browser-based front end, but the longer-term approach would be to implement the system using appropriate models of replication.
24|6||Improvement on the flexible tree-based key management framework|Matsuzaki et al. had proposed a flexible tree-based key management framework for a terminal to connect with multiple content distribution systems (CDSs) using a public bulletin board. In their scheme, the key management center constructs the public bulletin board by utilizing symmetric cryptosystem to protect terminal node keys. On the other hand, the terminals can obtain its node keys by decrypting the cipher which is posted on the public bulletin board of CDS. However, this method is not efficient for a large group which has a number of terminals. When a terminal changes its membership, the key managerial center needs a large amount of computation to structure the public bulletin, and the terminal cannot efficiently compute its node keys from the large pubic bulletin board. In this paper, we propose an improved scheme to structure a public bulletin board efficiently by the key management center of CDS. The improved scheme is capable of a large group of terminals and ensures that low powered equipment can efficiently obtain their node key.
24|7|http://www.sciencedirect.com/science/journal/01674048/24/7|Aligning disaster recovery and security incident response|
24|7||Security views - Malware update|
24|7||Beyond SarbanesâOxley compliance|The information security issues of the Sarbanes–Oxley Act require companies to be able to document which users accessed and attempted to modify data. Biometric systems are the only way to uniquely identify the user and to prove in a log file, which person did access and change data or was denied trying to access restricted data. Passwords alone clearly offer no protection and no identity management at all.
24|7||Authentication of users on mobile telephones â A survey of attitudes and practices|With the ever-increasing functionality and services accessible via mobile telephones, there is a strong argument that the level of user authentication implemented on the devices should be extended beyond the Personal Identification Number (PIN) that has traditionally been used. This paper presents the results of a survey of 297 mobile subscribers, which attempted to assess their use of mobile devices, their use of current authentication methods, and their attitudes towards future security options. The findings revealed that the majority of the respondents make significant use of their devices, with clear demands for protection against unauthorised use. However, the use of current PIN-based authentication is problematic, with a third of the respondents indicating that they do not use it at all, and other problems being reported amongst those that do. In view of this, the respondents' opinions in relation to future security options are interesting, with 83% being willing to accept some form of biometric authentication on their device. The discussion considers these findings, and the potential applicability of the preferred techniques to mobile devices.
24|7||Events calendar, please update, see marked copy|
24|7||PAID: A Probabilistic Agent-Based Intrusion Detection system|In this paper we describe architecture and implementation of a Probabilistic Agent-Based Intrusion Detection (PAID) system. The PAID system has a cooperative agent architecture. Autonomous agents can perform specific intrusion detection tasks (e.g., identify IP-spoofing attacks) and also collaborate with other agents. The main contributions of our work are the following: our model allows agents to share their beliefs, i.e., the probability distribution of an event occurrence. Agents are capable to perform soft-evidential update, thus providing a continuous scale for intrusion detection. We propose methods for modelling errors and resolving conflicts among beliefs. Finally, we have implemented a proof-of-concept prototype of PAID.
24|7||Query-directed passwords|A classical tradeoff in the field of user authentication is between user convenience and system security. Should users authenticate themselves with their mother's maiden name, which is easily recalled but not very secure; or should they memorize a long, random password that is secure but unmemorable? In recent years, tokens and biometrics have been offered as the answer to this convenience-versus-security conflict; however, these require infrastructure modifications.
24|7||Sinkhole intrusion in mobile ad hoc networks: The problem and some detection indicators|We analyze the “sinkhole” problem in the context of the Dynamic Source Routing (DSR) protocol for wireless mobile ad hoc networks (MANETs). The sinkhole effect is caused by attempts to draw all network traffic to malicious nodes that broadcast fake shortest path routing information. Two reliable indicators of sinkhole intrusion are proposed and analyzed. We will study how these sinkholes may be detected and term such as sinkhole intrusion detection. One indicator is based on the sequence number in the routing message and the other one is related to the proportion of the routes that travel to a suspected node. Threshold values that imply possible sinkhole intrusion are derived for these two indicators. The simulation results show that the indicators are consistent and reliable for detecting sinkhole intrusion.
24|7||Defending against spoofed DDoS attacks with path fingerprint|In this paper, we propose a new scheme, called ANTID, for detecting and filtering DDoS attacks which use spoofed packets to circumvent the conventional intrusion detection schemes. The proposed anti-DDoS scheme intends to complement, rather than replace conventional schemes. By embedding in each IP packet a unique path fingerprint that represents the route an IP packet has traversed, ANTID is able to distinguish IP packets that traverse different Internet paths. In ANTID, a server maintains for each of its communicating clients the mapping from the client's IP address to the corresponding path fingerprint. The construction and renewal of these mappings is performed in an on-demand fashion that helps to reduce the cost of maintenance. With presence of the mapping table, the onset of a spoofed DDoS attack can be detected by observing a surge of spoofed packets. Consequently, spoofed attack packets are filtered so as to sustain the quality of protected Internet services. ANTID is lightweight, robust, and incrementally deployable. Our experiment results showed that the proposed scheme can detect 99.95% spoofed IP packets and can discard them with little collateral damage to legitimate clients. It also showed that the higher the aggregated attack rate is, the sooner the attack can be detected.
24|8|http://www.sciencedirect.com/science/journal/01674048/24/8|Infosec certification: Which way do we turn from here?|
24|8||Security views|
24|8||Timing is everything|Social engineering attacks are well-known to prey on human weaknesses. Besides these weaknesses, humans insist on eating, sleeping, and partaking in non-work activities. On a global scale, work schedules combined with IT policies leave large windows of vulnerability – but how large? We examine calendar data through the year 2010 and locate the longest vulnerability windows which could be exploited by well-timed attacks by malicious software. The same data can be analyzed to solve a related problem: determining the best times to release software patches.
24|8||Real-time information integrity = system integrity + data integrity + continuous assurances|A majority of companies today are totally dependent on their information assets, in most cases stored, processed and communicated within information systems in digital format. These information systems are enabled by modern information and communication technologies. These technologies are exposed to a continuously increasing set of risks. Yet, management and stakeholders continuously make important business decisions on information produced in real-time from these information systems. This information is unaccompanied by objective assurances as the current auditing procedures provide assurances months later. Therefore, risk management, including a system of internal controls, has become paramount to ensure the information's integrity. A system of internal controls, including IT controls at its core, help limit uncertainty and mitigate the risks to an acceptable level. Auditors play an increasingly important role in providing independent assurances that the information system's infrastructure and data maintain their integrities. These assurances include proposed new methods such as continuous auditing for assurance on demand.
24|8||Safety and Security in Multiagent Systems: Report on the 2nd SASEMAS workshop (SASEMAS'05)|As intelligent autonomous agents and multiagent systems' applications become more pervasive, it becomes increasingly more important to understand the risks associated with using these systems. Incorrect or inappropriate agent behaviour can have harmful effects including financial cost, loss of data, and injury to humans or systems. Thus, security and safety are two central issues when developing and deploying such systems.
24|8||Calendar of Events|
24|8||Robust remote authentication scheme with smart cards|Due to low-computation cost and convenient portability, smart cards are usually adopted to store the personal secret information of users for remote authentication. Although many remote authentication schemes using smart cards have been introduced in the literatures, they still suffer from some possible attacks or cannot guarantee the quality of performance for smart cards. In this paper, we classify the security criteria of remote authentication and propose a new remote login scheme using smart cards to satisfy all of these criteria. Not only does the proposed scheme achieve the low-computation requirement for smart cards, but also it can withstand the replay and the offline dictionary attacks as well. Moreover, our scheme requires neither any password table for verification nor clock synchronization between each user and the server while providing both mutual authentication and the uniqueness of valid cards.
24|8||A novel mix-based location privacy mechanism in Mobile IPv6|Mobile IP (MIP), a link-layer-independent protocol, is suitable for Internet Protocol (IP) based mobility across homogeneous media as well as heterogeneous networks. Mobile IPv6 (MIPv6) not only possesses the major characteristics of Mobile IPv4 (MIPv4), but also has more advantages such as the expansion of address space and elimination of the “triangle routing”, which make MIPv6 the most suitable candidate for future heterogeneous environment. Location privacy is very important for mobile node (MN) in mobile communications because exposure of the relationship between MN's real physical location and its identity will lead to serious violation of the MN's privacy. And the attackers can easily launch the traffic analysis attack according to such revealed relationship. However, the location privacy of MN to avoid attackers tracing in MIP is not paid more attention up to the present. As the most widely used anonymous communication technology, mix-network can be used to provide the location privacy in MIP. In this paper, we employ the practical mix-network to provide location privacy on signaling control information in MIPv6. By utilizing the practical mix-network, a novel MIPv6 network model is proposed. Based on the network model, a new location privacy extension to MN's home binding and correspondent registration in MIPv6 is proposed and it can be integrated into MIPv6 easily. As a result, our location privacy proposal possesses the benefits succeeded from the adopted practical mix-network, e.g. reducing the trust requirements among the mix servers and increasing the robustness compared with other mix-based MIP location privacy schemes. In addition, the computation load in MN does not increase significantly during the binding procedures according to the analysis, thus it is more suitable for the asymmetric wireless environment.
24|8||SEAS, a secure e-voting protocol: Design and implementation|This paper presents SEAS, the Secure E-voting Applet System, a protocol for implementing a secure system for polling over computer networks, usable in distributed organizations whose members may range up to dozens of thousands. We consider an architecture requiring the minimum number of servers involved in the validation and voting phases. Sensus, [Cranor L, Cytron RK. Sensor: a security-conscious electronic polling system for the internet. In: Proceedings of HICSS'97. IEEE; 1997. p. 561–70], a well-known e-voting protocol, requires only two servers, namely a validator and a tallier. Even if satisfying most of the security requirements of an e-voting system, Sensus suffers from a vulnerability that allows one of the entities involved in the election process to cast its own votes in place of those that abstain from the vote. SEAS is a portable and flexible system that preserves the limited number of servers of Sensus, but it avoids the mentioned vulnerability. We propose a prototype implementation of SEAS based on Java applet and XML technology.
24|8||Hash channels|It is believed that in authentication protocols where no random numbers are involved, it is hard to introduce subliminal channels. In this paper we show that subliminal channels exist in digital signature schemes where no random numbers are used. The source where the subliminal channels exist is widely acceptable hash functions with their outputs being of some randomness. Because the subliminal channels are established by using hash functions, the subliminal channels are called hash channels in this paper. This indicates that there exist subliminal channels in any authentication protocols that use hash functions.
24|8||A multinomial logistic regression modeling approach for anomaly intrusion detection|Although researchers have long studied using statistical modeling techniques to detect anomaly intrusion and profile user behavior, the feasibility of applying multinomial logistic regression modeling to predict multi-attack types has not been addressed, and the risk factors associated with individual major attacks remain unclear. To address the gaps, this study used the KDD-cup 1999 data and bootstrap simulation method to fit 3000 multinomial logistic regression models with the most frequent attack types (probe, DoS, U2R, and R2L) as an unordered independent variable, and identified 13 risk factors that are statistically significantly associated with these attacks. These risk factors were then used to construct a final multinomial model that had an ROC area of 0.99 for detecting abnormal events. Compared with the top KDD-cup 1999 winning results that were based on a rule-based decision tree algorithm, the multinomial logistic model-based classification results had similar sensitivity values in detecting normal (98.3% vs. 99.5%), probe (85.6% vs. 83.3%), and DoS (97.2% vs. 97.1%); remarkably high sensitivity in U2R (25.9% vs. 13.2%) and R2L (11.2% vs. 8.4%); and a significantly lower overall misclassification rate (18.9% vs. 35.7%). The study emphasizes that the multinomial logistic regression modeling technique with the 13 risk factors provides a robust approach to detect anomaly intrusion.
25|1|http://www.sciencedirect.com/science/journal/01674048/25/1|Dilemmas and boundaries of digital rights management|
25|1||Security views - Malware update|
25|1||Re-engineering enterprise security|Security concerns plague businesses of all sizes, but for large, international organisations, security management can be very complex. Many companies are therefore taking a fresh look at security to see how it can be re-engineered on an enterprise level to deliver seamless 24×7 management support globally. This article looks at the steps they must take and the issues they may face, as well as showing how the success of a programme can be measured using KPIs.
25|1||Radio frequency identification (RFID)|First conceived in 1948, Radio Frequency Identification (RFID) has taken many years for the technology to mature to the point where it is sufficiently affordable and reliable for widespread use. From Electronic Article Surveillance (EAS) for article (mainly clothing) security to more sophisticated uses, RFID is seen by some as the inevitable replacement for bar codes. With increasing use comes increasing concern on privacy and security. Clearly there is considerable work to be undertaken before RFID becomes as pervasive as bar codes although the tempo of change is increasing rapidly.
25|1||The challenges of understanding and using security: A survey of end-users|Many applications contain security features that are available for end-users to select and configure, as well as the potential to place users in situations where they must take security-related decisions. However, the manner in which these aspects are implemented and presented can often serve to complicate the process, such that users cannot actually use the security that they desire, or which may be expected of them. This paper presents the results of a survey of over 340 end-users in order to determine their understanding of the security features within Windows XP and three popular applications (Internet Explorer, Outlook Express, and Word). The study reveals some significant areas of difficulty, with many standard security features presenting apparent usability challenges for large proportions of the respondents. The results highlight the need for a more considered approach towards the presentation of security functionality if users are to have a realistic chance of protecting themselves.
25|1||Events|
25|1||Towards a location-based mandatory access control model|With the growing use of wireless networks and mobile devices, we are moving towards an era where location information will be necessary for access control. The use of location information can be used for enhancing the security of an application, and it can also be exploited to launch attacks. For critical applications, such as the military, a formal model for location-based access control is needed that increases the security of the application and ensures that the location information cannot be exploited to cause harm. In this paper, we show how the mandatory access control (MAC) model can be extended to incorporate the notion of location. We also show how the different components in the MAC model are related with location and how this location information can be used to determine whether a subject has access to a given object. This model is suitable for military applications consisting of static and dynamic objects, where location of a subject and object must be considered before granting access.
25|1||An identity management protocol for Internet applications over 3G mobile networks|This paper, proposes a protocol (IDM3G) for implementing identity management for Internet applications over 3G mobile networks. IDM3G combines the identity management principles of the Liberty Alliance specifications, elements of the OASIS's SAML and the 3GPP UMTS security specifications, targeting to a more effective and lightweight identity management solution than the existing ones. IDM3G instead of establishing new authentication and authorization mechanisms, utilizes the latest security features of 3G mobile networks in order to implement trust relationships, focusing on mutual authentication and authorization, avoiding at the same time the submission of the user identity itself.
25|1||Cryptanalysis of two password-based authentication schemes using smart cards|Recently, Juang [2004. Comput Secur (23)] and Yoon et al. [2005. Comput Secur (24)] proposed password-based authentication schemes using smart cards. Juang's scheme further allows for key agreement. In this paper, we present attacks on both schemes.
25|1||Aligning the information security policy with the strategic information systems plan|Two of the most important documents for ensuring the effective deployment of information systems and technologies within the modern business enterprise are the strategic information systems plan (SISP) and the information security policy. The strategic information systems plan ensures that new systems and technologies are deployed in a way that will support an organisation's strategic goals whilst the information security policy provides a framework to ensure that systems are developed and operated in a secure manner. To date, the literature with regard to the formulation of the information security policy has tended to ignore its important relationship with the strategic information systems plan, and vice versa. In this paper we argue that these two important policy documents should be explicitly and carefully aligned to ensure that the outcomes of strategically important information system initiatives are not compromised by problems with their security.
25|1||Steganography in games: A general methodology and its application to the game of Go|Techniques to hide valuable information within seemingly harmless messages have been widely used for centuries. Typically, their use is appropriate when encryption is not available or not adequate (e.g. when available cryptography is too weak), or simply when it is convenient that no external observer can infer that some information is being exchanged. In the digital era, new cover mediums for hiding data in communication are constantly being proposed, from the classical image files (such as bmp, gif, and jpg formats) to audio files (i.e. wav and mp3), text and html documents, emails disguised as spam, TCP/IP packets, executables programs, DNA strands, etc. In this work, we present and analyze a novel methodology that illustrates how games (such as Chess, Backgammon, Go, etc.) can be used to hide digital contents. We also look at some of its possible advantages and limitations when compared with other techniques, discussing some improvements and extensions. Finally, we present the results of a first implementation of an open-source prototype, called StegoGo, for hiding digital contents in Go games.
25|1||Efficient remote mutual authentication and key agreement|A smart card based scheme is very practical to authenticate remote users. In 2004, Juang [Juang WS. Efficient password authenticated key agreement using smart cards. Computers and Security 2004;23:167–73] proposed a mutual authentication scheme using smart cards. The advantages in the scheme include freely chosen passwords, no verification tables, low communication and computation cost, and session key agreement. In addition, synchronized clocks are not required in the scheme due to its nonce based approach. In this paper, however, we shall discuss the weakness of Juang's [Juang WS. Efficient password authenticated key agreement using smart cards. Computers and Security 2004;23:167–73] scheme and propose another similar scheme to improve the weakness. Our scheme not only preserves all the advantages of Juang's scheme but also improves its efficiency.
25|2|http://www.sciencedirect.com/science/journal/01674048/25/2|About âUnofficial patchesâ|
25|2||Security Views - Malware Update|
25|2||The metamorphosis of malware writers|The reasons for writing malware are changing – and so is the malware itself. Danny Bradbury reports on the development of a seedy commercial market.
25|2||Computer forensics and electronic discovery: The new management challenge|Recent American court decisions and legislation have shown that the failure of an organization to retain electronic documents and to be able to locate the information when needed can cost the organization millions of dollars as well as its reputation. In spite of understanding the need for compliance, very few organizations actually have a good understanding of how to implement a system that will satisfy the requirements for electronic document retention and retrieval for litigation purposes.
25|2||Calendar of Events|
25|2||Uncovering identities: A study into VPN tunnel fingerprinting|Operating System fingerprinting is a reconnaissance method which can be used by attackers or forensic investigators. It identifies a system's identity by observing its responses to targeted probes, or by listening on a network and passively observing its network ‘etiquette’. The increased deployment of encrypted tunnels and Virtual Private Networks (VPNs) calls for the formulation of new fingerprinting techniques, and poses the question: “How much information can be gleaned from encrypted tunnels?” This paper investigates IPSec VPN tunnel-establishment and tear-down on three IPSec implementations: Microsoft Windows 2003, Sun Solaris 9 x86, and racoon on Linux 2.6 kernel. By analysing each platform's Internet Key Exchange (IKE) messages, which negotiate the IPSec tunnel, we identify a number of discriminants, and show that each of these platforms can be uniquely identified by them. We also show that the nature of some encrypted traffic can be determined, thus giving the observer an idea of the type of communication that is taking place between the IPSec endpoints.
25|2||Provably secure authenticated key exchange protocols for low power computing clients|Low power computing devices such as cellular phones, PDAs, and smart cards are very popular and widely used by people nowadays. To secure communications between a client and a server through a low power computing device, several AKE–LPC (Authenticated Key Exchange – for Low Power Computing clients) protocols have been proposed recently. This paper proposes a new efficient AKE–LPC protocol, which requires the client to perform only one hash operation during the execution phase. An augmented protocol which can additionally provide explicit mutual authentication is also presented. Furthermore, the security of the proposed protocols is formally proven in the random oracle model.
25|2||A new protocol to counter online dictionary attacks|The most popular method of authenticating users is through passwords. Though passwords are the most convenient means of authentication, they bring along themselves the threat of dictionary attacks. While offline dictionary attacks are possible only if the adversary is able to collect data for a successful protocol execution by eavesdropping on the communication channel and can be successfully countered by using public key cryptography, online dictionary attacks can be performed by anyone and there is no satisfactory solution to counter them. In this paper, we propose an authentication protocol which is easy to implement without any infrastructural changes and yet prevents online dictionary attacks. Our protocol uses only one way hash functions and eliminates online dictionary attacks by implementing a challenge–response system. This challenge–response system is designed in a fashion that it hardly poses any difficulty to a genuine user but is extremely burdensome, time consuming and computationally intensive for an adversary trying to launch as many as hundreds of thousands of authentication requests as in case of an online dictionary attack. The protocol is perfectly stateless and thus less vulnerable to denial of service (DoS) attacks.
25|2||Layered security design for mobile ad hoc networks|When security of a given network architecture is not properly designed from the beginning, it is difficult to preserve confidentiality, authenticity, integrity and non-repudiation in practical networks. Unlike traditional mobile wireless networks, ad hoc networks rely on individual nodes to keep all the necessary interconnections alive. In this article we investigate the principal security issues for protecting mobile ad hoc networks at the data link and network layers. The security requirements for these two layers are identified and the design criteria for creating secure ad hoc networks using multiple lines of defence against malicious attacks are discussed.
25|2||Security considerations for incremental hash functions based on pair block chaining|Incremental hash functions have gained much attention due to their incremental property, i.e. hashes of updated messages can be speedily computed from previous hashes without having to re-hash the message as was the case in conventional hash functions. In this paper, we first show how collisions can be obtained in such incremental hash functions that are based on pair block chaining, highlighting that more caution should be taken into its design process. We then identify some design and implementation criteria for such incremental hash functions.
25|2||A framework and taxonomy for comparison of electronic voting schemes|Electronic voting is an emerging social application of cryptographic protocols. A vast amount of literature on electronic voting has been developed over the last two decades. In this paper, we provide a framework that classifies these approaches and defines a set of metrics under which their properties can be compared. Such a methodology reveals important differences in security properties between the classes and allows for selection and future design of voting schemes, based on application requirements. We illustrate the use of our framework by analyzing some of the existing electronic voting schemes.
25|3|http://www.sciencedirect.com/science/journal/01674048/25/3|Special systems: Overlooked sources of security risk?|
25|3||Security Views - Malware Update|
25|3||Modeling network security|
25|3||Information Security â The Fourth Wave|In a previous article [von Solms, 2000], the development of Information Security up to the year 2000 was characterized as consisting of three waves:
25|3||Calendar of events - update|
25|3||Real-time analysis of intrusion detection alerts via correlation|With the growing deployment of networks and the Internet, the importance of network security has increased. Recently, however, systems that detect intrusions, which are important in security countermeasures, have been unable to provide proper analysis or an effective defense mechanism. Instead, they have overwhelmed human operators with a large volume of intrusion detection alerts. This paper presents a fast and efficient system for analyzing alerts. Our system basically depends on the probabilistic correlation. However, we enhance the probabilistic correlation by applying more systematically defined similarity functions and also present a new correlation component that is absent in other correlation models. The system can produce meaningful information by aggregating and correlating the large volume of alerts and can detect large-scale attacks such as distributed denial of service (DDoS) in early stage. We measured the processing rate of each elementary component and carried out a scenario-based test in order to analyze the efficiency of our system. Although the system is still imperfect, we were able to reduce the numerous redundant alerts 5.5% of the original volume without distorting the meaning through two-phase reduction. This ability reduces the management overhead drastically and makes the analysis and correlation easy. Moreover, we were able to construct attack scenarios for multistep attacks and detect large-scale attacks in real time.
25|3||A novel remote user authentication scheme using bilinear pairings|The paper presents a remote user authentication scheme using the properties of bilinear pairings. In the scheme, the remote system receives user login request and allows login to the remote system if the login request is valid. The scheme prohibits the scenario of many logged in users with the same login-ID, and provides a flexible password change option to the registered users without any assistance from the remote system.
25|3||A novel approach for computer security education using Minix instructional operating system|To address national needs for computer security education, many universities have incorporated computer and security courses into their undergraduate and graduate curricula. In these courses, students learn how to design, implement, analyze, test, and operate a system or a network to achieve security. Pedagogical research has shown that effective laboratory exercises are critically important to the success of these types of courses. However, such effective laboratories do not exist in computer security education.
25|3||A traceable threshold signature scheme with multiple signing policies|In recent years, a great deal of work has been done on threshold signature schemes and many excellent schemes have been proposed. In Eurocrypt'94, Li et al. [Threshold-multisignature schemes where suspected forgery implies traceability of adversarial shareholders. In: Advances in Cryptology—Proceedings of EUROCRYPT 94; 1994. p. 413–9] proposed a threshold signature scheme with traceability, which allows us to trace back to find the signer without revealing the secret keys. And in 2001, Lee [Threshold signature scheme with multiple signing policies. IEE Proc Comput Digit Tech 2001;148(2):95–9] proposed a threshold signature scheme with multiple signing policies, which allows multiple secret keys to be shared among a group of users, and each secret key has its specific threshold value. In this paper, based on these schemes, we present a traceable threshold signature scheme with multiple signing policies, which not only inherits their properties, but also fixes their weaknesses.
25|3||Security implications in RFID and authentication processing framework|The objective of this paper is to propose an idea called APF (Authentication Processing Framework) as one of the ways to deter the growing concerns of unauthorized readers from accessing the tag (transponder) which could result into the violations of information stored in the tag. On one hand, we will discuss the importance of RFID systems and on the other hand, we will discuss about the security implications that the RFID systems have over consumers' privacy and security. In this paper, we are trying to weigh the two issues, importance of RFID system and the RFID security implications. Having done that, we are recommending our idea called APF (Authentication Processing Framework) as a good method to overcome the above mentioned problem.
25|3||Change trend of averaged Hurst parameter of traffic under DDOS flood attacks|Distributed denial-of-service (DDOS) flood attacks remain great threats to the Internet though various approaches and systems have been proposed. Because arrival traffic pattern under DDOS flood attacks varies significantly away from the pattern of normal traffic (i.e., attack free traffic) at the protected site, anomaly detection plays a role in the detection of DDOS flood attacks. Hence, quantitatively studying statistics of traffic under DDOS flood attacks (abnormal traffic for short) are essential to anomaly detections of DDOS flood attacks.
25|3||An empirical examination of the reverse engineering process for binary files|Reverse engineering of binary code file has become increasingly easier to perform. The binary reverse engineering and subsequent software exploitation activities represent a significant threat to the intellectual property content of commercially supplied software products. Protection technologies integrated within the software products offer a viable solution towards deterring the software exploitation threat. However, the absence of metrics, measures, and models to characterize the software exploitation process prevents execution of quantitative assessments to define the extent of protection technology suitable for application to a particular software product. This paper examines a framework for collecting reverse engineering measurements, the execution of a reverse engineering experiment, and the analysis of the findings to determine the primary factors that affect the software exploitation process. The results of this research form a foundation for the specification of metrics, gathering of additional measurements, and development of predictive models to characterize the software exploitation process.
25|3||A simple, configurable SMTP anti-spam filter: Greylists|This paper addresses methods for combating spam, focusing especially on those based on the economic motivations of unsolicited commercial e-mail. Considering the fact that to date no machine has passed the Turing test, well-known blacklist and whitelist solutions can be generalized by greylists. An outline of a simple SMTP anti-spam application following these ideas and running on a UNIX machine is offered. Some problems regarding the application are discussed, together with some of the results obtained after a two-month test period.
25|4|http://www.sciencedirect.com/science/journal/01674048/25/4|Representing information security fairly and accurately|
25|4||Malware Update|
25|4||Reflecting on 20 SEC conferences|The ever-increasing use of information technology in business and everyday life led to a raised awareness of security issues. The last three decades spawned a large amount of research literature on security.
25|4||Calendar of events|
25|4||Predation and the cost of replication: New approaches to malware prevention?|Computer viruses and worms are often compared to their biological counterparts. Researchers in the field speak of “infection”, “innate immunity” and “epidemics” – all expressions with distinctly biological connotations. However, despite the similarity of language, there are marked and important differences between computer viruses and their biological namesake. In this paper, some of the most critical differences are examined, and an illustration of how they may limit the application of biologically inspired defenses to computer virus spread is given. Furthermore, due to our historical lack of success in containing computer virus outbreaks, we apply a different biological metaphor to the problem: that of predator and prey. In particular, we focus on the issue of the cost of predation, and note that the essentially “free” predation computer viruses enjoy limits the applicability of biological analogies of protection in the global computing infrastructure.
25|4||Scalable balanced batch rekeying for secure group communication|Secure group communication is important for applications such as pay-per-view. Other authors have proposed the key tree approach to distribute a shared group key in a way such that the rekeying cost scales linearly with the logarithm of the group size for a join or depart request. The efficiency of the key tree approach depends critically on whether the key tree remains balanced. Periodic rebalancing can be used to balance the key tree whenever it becomes unbalanced but this adds extra costs to the network. In this paper, we present two Merging Algorithms suitable for batch join events. As the multicast session consists of other events as well, we then show how we can extend our algorithms into existing work to minimise the maximum difference in height without adding extra network costs. Simulation results show our Merging Algorithms not only balance the key tree but their rekeying costs are lower compared to existing algorithms.
25|4||A hybrid honeypot framework for improving intrusion detection systems in protecting organizational networks|This paper proposes a hybrid and adaptable honeypot-based approach that improves the currently deployed IDSs for protecting networks from intruders. The main idea is to deploy low-interaction honeypots that act as emulators of services and operating systems and have them direct malicious traffic to high-interaction honeypots, where hackers engage with real services. The setup permits for recording and analyzing the intruder's activities and using the results to take administrative actions toward protecting the network. The paper describes the basic components, design, operation, implementation and deployment of the proposed approach, and presents several performance and load testing scenarios. Implementation and performance plus load testing show the adaptability of the proposed approach and its effectiveness in reducing the probability of attacks on production computers.
25|4||A prototype for assessing information security awareness|Due to the intensified need for improved information security, many organisations have established information security awareness programs to ensure that their employees are informed and aware of security risks, thereby protecting themselves and their profitability. In order for a security awareness program to add value to an organisation and at the same time make a contribution to the field of information security, it is necessary to have a set of methods to study and measure its effect. The objective of this paper is to report on the development of a prototype model for measuring information security awareness in an international mining company. Following a description of the model, a brief discussion of the application results is presented.
25|4||Design of an enhancement for SSL/TLS protocols|When studying the Transport Layer Security (TLS) Protocol, it is noticed that the most time-consuming phase is the handshaking process between the client and the server, since many messages should be sent until successful negotiation is done and a secure session is created. The goal of this work is to design a security management system (SMS) to improve the handshaking process by making use of TLS client-side session caching, and allowing trusted users to share sessions with others, as well as giving the client an option to create his own private session with the server even when there is no trusted digital certificate from a certificate authority (CA) to link them. According to our experimental setup, the use of the proposed design has improved the performance by 3.5 times relative to the handshaking of traditional TLS.
25|4||An anonymous voting mechanism based on the key exchange protocol|In democratic society, elections and voting are always the most important hallmarks. However, there are plenty of problems in the traditional election such as inconvenience, unfairness, non-mobility, non-anonymity, and so forth. What is more, the cost of the traditional election often places a heavy burden on the nation. To solve the problems of the traditional election, the concept of “electronic voting” is introduced, where people are allowed to vote over the Internet and the government can save lots of money. The properties of mobility and convenience are the most significant reasons why people may adopt the electronic voting mechanism in the future. Although technologies for electronic voting have been developed for about 20 years, some problems, such as uncoercibility and completeness, still cannot be overcome. In this article, we are going to present an efficient and secure voting mechanism by employing Chaum's blind signature scheme and Diffie–Hellman key exchange protocol. Our proposed electronic voting mechanism not only achieves lots of essential requirements of general electronic voting schemes but also possesses better efficiency such that it can be the practical one.
25|5|http://www.sciencedirect.com/science/journal/01674048/25/5|The changing winds of information security|
25|5||Security Views - Malware Update|
25|5||Continuous auditing technologies and models: A discussion|In the age of real-time accounting and real-time communication current audit practices, while effective, often provide audit results long after fraud and/or errors have occurred. Real-time assurances can assist in preventing intentional or unintentional errors. This can best be achieved through continuous auditing which relies heavily on technology. These technologies are embedded within and are crucial to continuous auditing models.
25|5||Calender of Events|
25|5||PING attack â How bad is it?|PING-based Distributed Denial of Service (DDoS) attacks are infamous as they are known to have brought down high profile web sites such as Ebay, ETrade and Yahoo. They have also been used in an attempt to bring down the entire Internet by attacking its DNS root servers. In this paper, we investigate the impact of PING-flooding on computer systems. We create real PING-attack traffic in a controlled lab environment at UTPA to understand the intensity of the attack and its impact on processing power of a Windows-XP computer deploying Pentium-4, 2.66 GHz processor. In this experiment, we set out to measure the rate of resource exhaustion of the computer as its bandwidth is increasingly consumed by the PING-attack traffic. It is observed that PING attack causes resource starvation for the computer when the PING-attack traffic increasingly consumes the bandwidth of a Fast Ethernet Link.
25|5||Comparing Java and .NET security: Lessons learned and missed|Many systems execute untrusted programs in virtual machines (VMs) to mediate their access to system resources. Sun introduced the Java VM in 1995, primarily intended as a lightweight platform for executing untrusted code inside web pages. More recently, Microsoft developed the .NET platform with similar goals. Both platforms share many design and implementation properties, but there are key differences between Java and .NET that have an impact on their security. This paper examines how .NET's design avoids vulnerabilities and limitations discovered in Java and discusses lessons learned (and missed) from experience with Java security.
25|5||On Incident Handling and Response: A state-of-the-art approach|Incident Response has always been an important aspect of Information Security but it is often overlooked by security administrators. Responding to an incident is not solely a technical issue but has many management, legal, technical and social aspects that are presented in this paper. We propose a detailed management framework along with a complete structured methodology that contains best practices and recommendations for appropriately handling a security incident. We also present the state-of-the art technology in computer, network and software forensics as well as automated trace-back artifacts, schemas and protocols. Finally, we propose a generic Incident Response process within a corporate environment.
25|5||Authentication delegation for subscription-based remote network services|There is growing interest in collaboration and resource sharing among institutions and organizations. In this paper, we investigate the problems of identity management inherent in distributed subscription-based resource sharing. The paper describes the design, implementation and performance of a system that provides controlled access to subscription-based remote network services through a browser. A third-party authentication protocol is designed and employed to exchange security assertions among involved parties. The web servers use plug-ins to provide an authentication-delegation service and a policy-based authorization service. Users can use a single userID and password to access multiple subscribed resource sites.
25|5||A qualitative analysis of software security patterns|Software security, which has attracted the interest of the industrial and research community during the last years, aims at preventing security problems by building software without the so-called security holes. One way to achieve this goal is to apply specific patterns in software architecture. In the same way that the well-known design patterns for building well-structured software have been defined, a new kind of patterns called security patterns have emerged. These patterns enable us to incorporate a level of security already at the design phase of a software system. There exists no strict set of rules that can be followed in order to develop secure software. However, a number of guidelines have already appeared in the literature. Furthermore, the key problems in building secure software and major threat categories for a software system have been identified. An attempt to evaluate known security patterns based on how well they follow each principle, how well they encounter with possible problems in building secure software and for which of the threat categories they do take care of, is performed in this paper. Thirteen security patterns were evaluated based on these three sets of criteria. The ability of some of these patterns to enhance the security of the design of a software system is also examined by an illustrative example of fortifying a published design.
25|6|http://www.sciencedirect.com/science/journal/01674048/25/6|(iii) Contents|
25|6||Microsoft is back in the hot seat|
25|6||Security Views - Malware Update|
25|6||Microsoft's new window on security|Microsoft may be trying to escape from its past by building new security features into its next operating system, but it has been forced into some compromises.
25|6||Information Security Governance: A model based on the DirectâControl Cycle|It is generally accepted that Information Security Governance is an integral part of Corporate Governance. It is therefore essential for any company to have a proper Information Security Governance program which reflects this integration with Corporate Governance. One of the core principles of Governance, and specifically Corporate Governance, is the Direct–Control Cycle which, in its simplest form, ‘prescribes’ and ‘checks’. This paper presents an Information Security Governance model based on this cycle.
25|6||A quantitative method for ISO 17799 gap analysis|ISO/IEC 17799:2005 is one of the leading standards of information security. It is the code of practice including 133 controls in 11 different domains. There are a number of tools and software that are used by organizations to check whether they comply with this standard. The task of checking compliance helps organizations to determine their conformity to the controls listed in the standard and deliver useful outputs to the certification process. In this paper, a quantitative survey method is proposed for evaluating ISO 17799 compliance. Our case study has shown that the survey method gives accurate compliance results in a short time with minimized cost.
25|6||A Secure Identification and Key agreement protocol with user Anonymity (SIKA)|Anonymity is a desirable security feature in addition to providing user identification and key agreement during a user's login process. Recently, Yang et al., proposed an efficient user identification and key distribution protocol while preserving user anonymity. Their protocol addresses a weakness in the protocol proposed by Wu and Hsu. Unfortunately, Yang's protocol poses a vulnerability that can be exploited to launch a Denial-of-Service (DoS) attack. In this paper, we cryptanalyze Yang's protocol and present the DoS attack. We further secure their protocol by proposing a Secure Identification and Key agreement protocol with user Anonymity (SIKA) that overcomes the above limitation while achieving security features like identification, authentication, key agreement and user anonymity.
25|6||FTKM: A fault-tolerant key management protocol for multicast communications|Several protocols have been proposed to deal with the group key management problem. Unfortunately, most of these protocols lack fault-tolerance. This is a serious drawback in real network conditions, especially, when considering asynchronous wide-area networks, such as the Internet, where messages may be delayed indefinitely and nodes may fail. To cope with this limitation, failure detection/correction mechanisms are necessary. This kind of solutions introduces a new challenge, which is the requirement of periodically exchanging failure information between group members. This is a heavy communication overhead even with small groups. To avoid the periodic broadcast of failure information, we propose in this paper a solution that structures group members into a logical ring where only pair-wise communications are necessary to detect failures. Compared to other schemes, our solution is efficient and fault-tolerant.
25|6||A secure extension of the KwakâMoon group signcryption scheme|This paper presents the secure extension of the Kwak–Moon group signcryption scheme [Kwak D, Moon S. Efficient distributed signcryption scheme as group signcryption. In: First applied cryptography and network security – ACNS'03. Lecturer notes in computer science, vol. 2846. Springer-Verlag; 2003. p. 403–17] as a countermeasure against the cryptanalysis in [Wang G, Deng RH, Kwak D, Moon S. Security analysis of two signcryption scheme. In: Information security conference – ISC 2004. Lecturer notes in computer science, vol. 3225. Springer-Verlag; 2004. p. 123–33]. The cryptanalysis revealed that the Kwak–Moon scheme cannot satisfy the properties of unforgeability, coalition-resistance, and traceability. Therefore, to avoid these weaknesses, while providing the same functions, we add confidentiality to the original group signature by distributing a shared secret among group members through an efficient group key agreement. However, in case of just combining a group signature and a group key agreement, if an attacker who does not belong to the group acquires a valid group signature, it is still possible for him to impersonate a valid group member and delegate the group. Thus, to avoid this possibility, the proposed scheme confirms whether or not the sender is equal to the signer by including a session key encryption in the signed message. In addition, we analyze the security of the proposed scheme and apply it to an anonymous statistical survey of attributes.
25|6||NetHost-Sensor: Investigating the capture of end-to-end encrypted intrusive data|Intrusion Detection Systems (IDSs) are systems that protect against violation of data integrity, confidentiality and availability of resources. In the past 20 years, these systems have evolved with the technology and have become more sophisticated. Despite these advances, IDS is still an immature field, and the benefits obtained from detecting end-to-end encrypted attacks justify the need for more research.
25|6||Application of temporal and spatial role based access control in 802.11 wireless networks|In this study, we have investigated the security aspects of wireless local area networks and discussed the weaknesses associated with various conventional 802.11 security protocols such as WEP and 802.1X. We propose an architecture to control access to 802.11 wireless networks, based on roles, location and time information, using the tested wired network components such as VPNs and Firewalls. The presented architecture, in which temporal and spatial RBAC is implemented, reduces the security risks in enterprise level deployment of wireless LANs.
25|6||RT-UNNID: A practical solution to real-time network-based intrusion detection using unsupervised neural networks|With the growing rate of network attacks, intelligent methods for detecting new attacks have attracted increasing interest. The RT-UNNID system, introduced in this paper, is one such system, capable of intelligent real-time intrusion detection using unsupervised neural networks. Unsupervised neural nets can improve their analysis of new data over time without retraining. In previous work, we evaluated Adaptive Resonance Theory (ART) and Self-Organizing Map (SOM) neural networks using offline data. In this paper, we present a real-time solution using unsupervised neural nets to detect known and new attacks in network traffic. We evaluated our approach using 27 types of attack, and observed 97% precision using ART nets, and 95% precision using SOM nets.
25|6||Infection, imitation and a hierarchy of computer viruses|Infection is an essential character of computer viruses. In addition, computer viruses can also imitate the behavior of infected programs in some ways in order to hide themselves. In this paper we define infection and imitation mathematically, and classify computer viruses into 3 types according to their different imitation behaviors. Furthermore, we give some results about the degree of unsolvability of each type of computer viruses. We show that the set of type 0 and type 1 computer viruses is Î 2-complete, while the set of type 2 computer viruses is Î 3-complete.
25|6||Guide for Authors.|
25|6||IBC - Calendar of Events|
25|7|http://www.sciencedirect.com/science/journal/01674048/25/7|(iii) Contents|
25|7||Issues concerning the distribution of vulnerability information|
25|7||Security Views - Malware Update|
25|7||Risk and restitution: Assessing how users establish online trust|The belief that users must be assured of security prior to engaging with an online service is challenged through the examination of attitudes from participants of a number of focus groups within the UK. What is apparent from our evidence is that rather than accepting simple assurances of protection, the average user is far more informed than service providers often credit, and will carry out a personal risk assessment prior to engaging with a service. Rather than guarantees of security, clearly defined indications of mitigation and restitution in the event of failure or problems are what users consider important. These findings have far reaching implications for service providers and a number of consequent recommendations are defined.
25|7||Information security governance: Due care|Most modern corporate governance guidelines, and also some country laws, make the Board and specifically the CEO responsible for the well-being of the organization. These parties must ensure that critical company assets are identified and that these assets are protected against possible risks that may negatively influence the organization. Information can certainly be regarded as a critical business asset in most organizations today. Therefore, due care needs to be applied in the protection of information resources. Failure to do so can lead to a legal charge of negligence. As best practices can be argued as a very effective approach to apply due care, this paper proposes a self-evaluation exercise (based on best practices) for boards of companies to be used to determine whether due care has indeed been applied.
25|7||Security issues in SCADA networks|The increasing interconnectivity of SCADA (Supervisory Control and Data Acquisition) networks has exposed them to a wide range of network security problems. This paper provides an overview of all the crucial research issues that are involved in strengthening the cyber security of SCADA networks. The paper describes the general architecture of SCADA networks and the properties of some of the commonly used SCADA communication protocols. The general security threats and vulnerabilities in these networks are discussed followed by a survey of the research challenges facing SCADA networks. The paper discusses the ongoing work in several SCADA security areas such as improving access control, firewalls and intrusion detection systems, SCADA protocol analyses, cryptography and key management, device and operating system security. Many trade and research organizations are involved in trying to standardize SCADA security technologies. The paper concludes with an overview of these standardization efforts.
25|7||A dynamic context-aware access control architecture for e-services|The universal adoption of the Internet and the emerging web services technologies constitutes the infrastructure that enables the provision of a new generation of e-services and applications. However, the provision of e-services through the Internet imposes increased risks, since it exposes data and sensitive information outside the client premises. Thus, an advanced security mechanism has to be incorporated, in order to protect this information against unauthorized access. In this paper, we present a context-aware access control architecture, in order to support fine-grained authorizations for the provision of e-services, based on an end-to-end web services infrastructure. Access permissions to distributed web services are controlled through an intermediary server, in a completely transparent way to both clients and protected resources. The access control mechanism is based on a Role-Based Access Control (RBAC) model, which incorporates dynamic context information, in the form of context constraints. Context is dynamically updated and provides a high level of abstraction of the physical environment by using the concepts of simple and composite context conditions. Also, the paper deals with implementation issues and presents a system that incorporates the proposed access control mechanism in a web services infrastructure that conform to the OPC XML-DA specification.
25|7||A taxonomy and comparison of computer security incidents from the commercial and government sectors|Cyber incidents are growing in intensity and severity. Several industry groups are therefore taking steps to better coordinate and improve information security across sectors. Also, various different types of public–private partnerships are developing, where cyber incident information is shared across institutions. This cooperation may improve the understanding of various types of cyber incidents, their severity, and impact on various types of targets. Research has shown that different types of attackers may be distinguished in terms of sophistication, skill level, attacking style, and objective of attack. It may further be proposed that different sectors experience different types of attacks. Attack characteristics and information about the modus operandi of criminal offenders have been used to learn more about the attacker and the motive of an attack. This information may also be used to distinguish between cyber attacks towards different types of targets. The current study focuses on reported cyber intrusions by the commercial and government sectors. The reported data come from CERT®Coordination Center (CERT/CC), which has categorized the aspects of cyber intrusions in the current study. The aspects analyzed are: ‘Method of Operation (MO)’ which refers to the methods used by perpetrator to carry out an attack; ‘Impact’ which refers to the effect of the attack; ‘Source’ which refers to the source of the attack, and ‘Target’ which refers to the victim of the attack. The current study uses 839 cases of cyber attacks towards the commercial sector and 558 cases towards the government sector. The 23 variables from the four different cyber intrusion aspects; MO, impact, source sector and target sector, were analyzed using multidimensional scaling (MDS), which is a technique that has often been used when profiling traditional types of crimes. The analysis gave a Guttman–Lingoes' coefficient of alienation of 0.19 with 42 iterations in a 3-dimensional solution. It was shown that the commercial and government sectors experience different types of attacks, with different types of impact, stemming from different sources. The findings and implications are discussed in relation to the benefits of standardization, reporting, and sharing of cyber incident information.
25|7||Profiling program behavior for anomaly intrusion detection based on the transition and frequency property of computer audit data|Intrusion detection is an important technique in the defense-in-depth network security framework. In recent years, it has been a widely studied topic in computer network security. In this paper, we present two methods, namely, the Hidden Markov Models (HMM) method and the Self Organizing Maps (SOM) method, to profile normal program behavior for anomaly intrusion detection based on computer audit data. The HMM method utilizes the transition property of events while SOM method relies on the frequency property of events. Two data sets, CERT synthetic Sendmail system call data collected in the University of New Mexico (UNM) and Live FTP system call data collected in the CNSIS lab of Xi'an Jiaotong University, were used to assess the two methods. Testing results show that the HMM method using the transition property of events produces good detection performance while high computational expense is required both for training and detection. The HMM method is better than other two methods reported previously in terms of detection accuracy for the same data set. The SOM method considering the frequency property of events, on the other hand, is suitable for real-time intrusion detection because of its capability of processing a large amount of data with low computational overhead.
25|7||Call for papersâIFIPSEC 2007|
25|7||Guide for Authors.|
25|7||IBC - Calendar of Events|
25|8|http://www.sciencedirect.com/science/journal/01674048/25/8|(iii) Contents|
25|8||Predicting the future of InfoSec|
25|8||Security Views - Malware update|
25|8||Tightening the net: A review of current and next generation spam filtering tools|This paper provides an overview of current and potential future spam filtering approaches. We examine the problems spam introduces, what spam is and how we can measure it. The paper primarily focuses on automated, non-interactive filters, with a broad review ranging from commercial implementations to ideas confined to current research papers. Both machine learning- and non-machine learning-based filters are reviewed as potential solutions and a taxonomy of known approaches is presented. While a range of different techniques have and continue to be evaluated in academic research, heuristic and Bayesian filtering dominate commercial filtering systems; therefore, a case study of these techniques is presented to demonstrate and evaluate the effectiveness of these popular techniques.
25|8||Expected benefits of information security investments|Ideally, decisions concerning investments of scarce resources in new or additional procedures and technologies that are expected to enhance information security will be informed by quantitative analyses. But security is notoriously hard to quantify, since absence of activity challenges us to establish whether lack of successful attacks is the result of good security or merely due to good luck. However, viewing security as the inverse of risk enables us to use computations of expected loss to develop a quantitative approach to measuring gains in security by measuring decreases in risk. In using such an approach, making decisions concerning investments in information security requires calculation of net benefits expected to result from the investment. Unfortunately, little data are available upon which to base an estimate of the probabilities required for developing the expected losses. This paper develops a mathematical approach to risk management based on Kaplan–Meier and Nelson–Aalen non-parametric estimators of the probability distributions needed for using the resulting quantitative risk management tools. Differences between the integrals of these estimators evaluated for enhanced and control groups of systems in an information infrastructure provide a metric for measuring increased security. When combined with an appropriate value function, the expected losses can be calculated and investments evaluated quantitatively in terms of actual enhancements to security.
25|8||A virtual disk environment for providing file system recovery|File system recovery (FSR) is a kind of recovery facility that allows users to roll back the file system state to a previous state. In this paper, we present a virtual disk environment (VDE) which allows previous write operations to a hard disk to be undone, and previous version of files to be recovered. It can be used to recover the file system quickly even when computer system suffers the serious disaster such as system crash or boot failure. The VDE is same as virtual disk in the virtual machine (VM) environment in some way, but it can be applied to the environment without VM supports. Algorithms for implementing the VDE are presented and its implementation on Windows platform is discussed. Based on the implementation, the experimental results of the VDE performance are analyzed. Comparing with other FSRs, the main advantage of the VDE is low overhead and high recovery speed.
25|8||Wavelet based Denial-of-Service detection|Network Denial-of-Service (DoS) attacks that disable network services by flooding them with spurious packets are on the rise. Criminals with large networks (botnets) of compromised nodes (zombies) use the threat of DoS attacks to extort legitimate companies. To fight these threats and ensure network reliability, early detection of these attacks is critical. Many methods have been developed with limited success to date. This paper presents an approach that identifies change points in the time series of network packet arrival rates. The proposed process has two stages: (i) statistical analysis that finds the rate of increase of network traffic, and (ii) wavelet analysis of the network statistics that quickly detects the sudden increases in packet arrival rates characteristic of botnet attacks.
25|8||Guide for Authors.|
25|8||IBC - Calendar of Events|
26|1|http://www.sciencedirect.com/science/journal/01674048/26/1|(iii) Contents|
26|1||What infosec changes are likely to result from the recent US election?|
26|1||Security Views - Malware Update|
26|1||Biometric attack vectors and defences|Much has been reported on attempts to fool biometric sensors with false fingerprints, facial overlays and a myriad of other spoofing approaches. Other attack vectors on biometric systems have, however, had less prominence. This paper seeks to present a broader and more practical view of biometric system attack vectors, placing them in the context of a risk-based systems approach to security and outlining defences.
26|1||Information Lifecycle Security Risk Assessment: A tool for closing security gaps|News media continue to report stories of critical information loss through physical means. Most information security programs include physical protection for information system infrastructure, but not for the physical (non-electronic) forms of the information itself. Thus organizations have persistent critical information vulnerabilities that are not addressed by even the most extensive of information systems security programs.
26|1||Decoding digital rights management|Digital rights management technology is designed to prevent piracy and facilitate the creation of innovative business models around digital content. Its technological limitations may be surpassed only by its economic ones.
26|1||IFIP workshop â Information security culture|
26|1||Value-focused assessment of ICT security awareness in an academic environment|Security awareness is important to reduce human error, theft, fraud, and misuse of computer assets. A strong ICT security culture cannot develop and grow in a company without awareness programmes. This paper focuses on ICT security awareness and how to identify key areas of concern to address in ICT security awareness programmes by making use of the value-focused approach. The result of this approach is a network of objectives where the fundamental objectives are the key areas of concern that can be used in decision making in security planning. The fundamental objectives were found to be in line with the acknowledged goals of ICT security, e.g. confidentiality, integrity and availability. Other objectives that emerged were more on the social and management side, e.g. responsibility for actions and effective use of resources.
26|1||Bridging the gap between general management and technicians â A case study on ICT security in a developing country|The lack of planning, business re-engineering, and coordination in the whole process of computerisation is the most pronounced problem facing organisations. These problems often lead to a discontinuous link between technology and the business processes. As a result, the introduced technology poses some critical risks for the organisations due, in part, to different perceptions of the management and technical staffs in viewing the ICT security problem. This paper discusses a practical experience on bridging the gap between the general management and ICT technicians.
26|1||Organisational security culture: Extending the end-user perspective|The concept of security culture is relatively new. It is often investigated in a simplistic manner focusing on end-users and on the technical aspects of security. Security, however, is a management problem and as a result, the investigation of security culture should also have a management focus. This paper describes a framework of eight dimensions of culture. Each dimension is discussed in terms of how they relate specifically to security culture based on a number of previously published case studies. We believe that use of this framework in security culture research will reduce the inherent biases of researchers who tend to focus on only technical aspects of culture from an end-users perspective.
26|1||A video game for cyber security training and awareness|Although many of the concepts included in cyber security awareness training are universal, such training often must be tailored to address the policies and requirements of a particular organization. In addition, many forms of training fail because they are rote and do not require users to think about and apply security concepts. A flexible, highly interactive video game, CyberCIEGE, is described as a security awareness tool that can support organizational security training objectives while engaging typical users in an engaging security adventure. The game is now being successfully utilized for information assurance education and training by a variety of organizations. Preliminary results indicate the game can also be an effective addition to basic information awareness training programs for general computer users (e.g., annual awareness training.)
26|1||Phishing for user security awareness|User security education and training is one of the most important aspects of an organizations security posture. Using security exercises to reinforce this aspect is frequently done by education and industry alike; however these exercises usually enlist willing participants. We have taken the concept of using an exercise and modified it in application to evaluate a users propensity to respond to email phishing attacks in an unannounced test. This paper describes the considerations in establishing and the process used to create and implement an evaluation of one aspect of our user information assurance education program. The evaluation takes the form of a exercise, where we send out a phishing styled email record the responses.
26|1||A privacy-preserving clustering approach toward secure and effective data analysis for business collaboration|
26|1||Simple three-party key exchange protocol|Three-party authenticated key exchange protocol is an important cryptographic technique in the secure communication areas, by which two clients, each shares a human-memorable password with a trusted server, can agree a secure session key. Over the past years, many three-party authenticated key exchange protocols have been proposed. However, to our best knowledge, not all of them can meet the requirements of security and efficiency simultaneously. Therefore, in this paper, we would like to propose a new simple three-party password based authenticated key exchange protocol. Compared with other existing protocols, our proposed protocol does not require any server's public key, but can resist against various known attacks. Therefore, we believe it is suitable for some practical scenarios.
26|1||Guide for Authors.|
26|1||IBC - Calendar of Events|
26|2|http://www.sciencedirect.com/science/journal/01674048/26/2|(iii) Contents|
26|2||Windows Vista: Microsoft's brave new world|
26|2||Malware Update|
26|2||Advanced user authentication for mobile devices|As mobile devices continue to evolve in terms of the capabilities and services offered, so they introduce additional demands in terms of security. An issue that has traditionally been poorly served is user authentication, with the majority of devices relying upon problematic secret knowledge approaches. This paper proposes the use of more advanced biometric methods as an alternative. After considering the general range of available techniques and their applicability to mobile devices, the discussion focuses upon the concept of keystroke analysis. Results of a practical evaluation are presented based upon the entry of both telephone numbers and text messages on a mobile phone. The findings reveal the technique to have promise for certain users with average error rates below 5%. The paper then proceeds to explain how the accuracy could be further improved by incorporating keystroke analysis within a composite authentication mechanism that utilises a portfolio of authentication techniques to provide robust, accurate and transparent authentication of the user.
26|2||Clustering subjects in a credential-based access control framework|Currently, access control of distributed Internet resources (such as files, documents and web services) has become extremely demanding. Several new access control models have been introduced. Most of the proposed approaches increase the complexity of the access control procedure and at the same time expressing these models is becoming complicated. Improving the execution time of the access control procedures is a challenging task due to the increased number of resources (available over the Internet) and the size of the audience involved. In this paper, we introduce an approach for speeding up the access control procedure under an environment accessed by known subjects (i.e. subjects whose identity and attributes are known apriori through a subscription phase). This approach is based on some update functions (employed at the background during idle times) over files which are associated with subjects. The core task of the proposed update is its dynamic nature and its clustering of subjects according to their interests and credentials. Moreover, this work associates subjects with security policies that are most likely to be triggered according to (the subjects) interests. Credential-based access control is considered to properly protect frameworks distributing resources to known subjects and here emphasis is given to the complexity involved in order to decrease the access request evaluation time under a credential-based access control framework.
26|2||Privacy-preserving programming using sython|Programmers often have access to confidential data that are not strictly needed for program development. Broad privileges from accounts given to programmers allow them to view files, database table entries or even variables in team members' code that are not critical to their own code. The risk inherent in such unchecked access to possibly private and sensitive data is exacerbated in cases where software development is part of a larger functioning system with data already in place, and is especially severe in cases where development is contracted out to third parties. This paper focuses on the problem of providing developers with a programming language that incorporates privacy-preserving features. We present Sython, a preliminary prototype based on the Python programming language that incorporates such features, examining both implementation and the appearance of the system as viewed by a programmer. The main purpose of this paper is to explore the use of language syntax and underlying support for secure variables so that data owners can contract out programming tasks without worrying about information leakage.
26|2||Probabilistic analysis of an algorithm to compute TCP packet round-trip time for intrusion detection|Estimating the length of a connection chain is challenging and critical in detecting stepping-stone intrusion. In this paper, we propose a novel method, called standard deviation-based clustering approach (SDBA), to estimate the length of an interactive connection chain by computing round-trip time (RTT). SDBA takes advantage of RTTs distribution and inter-arrival distribution of “send” packets. We prove that the probability of making a correct selection of RTT through SDBA is bounded by 1 − (1/q2), where q is a number related to standard deviation of RTTs distribution and send packets inter-arrival distribution. Experimental results showed that SDBA can compete against the best known algorithm in packet-matching rate and accuracy. This paper also presents the restrictions of SDBA.
26|2||A study on decision consolidation methods using analytic models for security systems|The successful management of information security within an organization is vital to its survival and success. The necessary security controls need to be implemented and managed effectively. In this paper, using the characteristics of the AHP, a study on information security management systems is selected from the perspective of Process Model and Criteria. A case study has proven potential value of this methodology in helping decision-makers in supporting their selection of security controls.
26|2||A framework for behavior-based detection of user substitution in a mobile context|Personal mobile devices, such as mobile phones, smartphones, and communicators can be easily lost or stolen. Due to the functional abilities of these devices, their use by unintended persons may result in severe security breaches concerning private or corporate data and services. Organizations develop their security policy and employ preventive techniques to combat unauthorized use. Current solutions, however, are still breakable and there is a strong need for means to detect user substitution when it happens. A crucial issue in designing such means is to define the measures to be monitored.
26|2||Information security in networkable Windows-based operating system devices: Challenges and solutions|This paper explores information security risks in networkable Windows-based operating system (NWOS) devices. While these devices face the same information security risks as any other Windows platform, NWOS devices present additional challenges to vendors and buyers throughout the product lifecycle. It appears that NWOS devices are particularly vulnerable to information security threats because of the vendors' and buyers' lack of awareness of the security risks associated with such devices. Based on evidence collected from a manufacturer of Digital Storage Oscilloscopes, the paper offers a set of challenges faced and solution applied by this vendor in its interactions with buyers. In order to reduce the vulnerability of NWOS devices, the paper considers several information security measures for the production, sales and after-sales phases. Lastly, the paper outlines the business reasoning for both vendors and buyers to pursue this information security strategy.
26|2||Investigative response: After the breach|
26|2||Guide for Authors.|
26|3|http://www.sciencedirect.com/science/journal/01674048/26/3|(iii) Contents|
26|3||Mobile computing: The next Pandora's Box|
26|3||Malware update|
26|3||SVision: A novel visual network-anomaly identification technique|We propose a novel graphical technique (SVision) for intrusion detection, which pictures the network as a community of hosts independently roaming in a 3D space defined by the set of services that they use. The aim of SVision is to graphically cluster the hosts into normal and abnormal ones, highlighting only the ones that are considered as a threat to the network. Our experimental results conducted on DARPA 1999 and 2000 intrusion detection and evaluation datasets as well as real network data captured between 2003 and 2005 from the University of New Brunswick main link, and also a private network, show the proposed technique as a good candidate for the detection of various network threats such as vertical and horizontal scanning attacks, Denial of Service (DoS) attacks, Distributed DoS (DDoS) attacks, as well as worm propagation attack. Finally, the visualization technique proves to cope with high number of hosts in the network, the experimental results using network data of up to 1,000,000 distinct IPs per time interval.
26|3||Modeling and analyzing the spread of active worms based on P2P systems|Active worms spread in an automated fashion and can flood the Internet in a very short time. Hit-list scanning is a technique for accelerating the initial spread of a worm. Due to the recent surge of Peer-to-Peer (P2P) systems with large numbers of users, P2P systems can be a potential vehicle for the active worms to achieve fast worm propagation in the Internet too. When the technique of hit-list scanning is used on top of P2P system, some new characters emerge. In this paper, we have defined an L system and an O system. Based on modeling the spread of active worms, we focused all our attention on analyzing the characteristics of the spread of active worms between the L system and the O system which can help us design and control the P2P systems effectively as well as defend against the propagation of worms.
26|3||Measuring, analyzing and predicting security vulnerabilities in software systems|In this work we examine the feasibility of quantitatively characterizing some aspects of security. In particular, we investigate if it is possible to predict the number of vulnerabilities that can potentially be present in a software system but may not have been found yet. We use several major operating systems as representatives of complex software systems. The data on vulnerabilities discovered in these systems are analyzed. We examine the results to determine if the density of vulnerabilities in a program is a useful measure. We also address the question about what fraction of software defects are security related, i.e., are vulnerabilities. We examine the dynamics of vulnerability discovery hypothesizing that it may lead us to an estimate of the magnitude of the undiscovered vulnerabilities still present in the system. We consider the vulnerability discovery rate to see if models can be developed to project future trends. Finally, we use the data for both commercial and open-source systems to determine whether the key observations are generally applicable. Our results indicate that the values of vulnerability densities fall within a range of values, just like the commonly used measure of defect density for general defects. Our examination also reveals that it is possible to model the vulnerability discovery using a logistic model that can sometimes be approximated by a linear model.
26|3||Evaluating information security tradeoffs: Restricting access can interfere with user tasks|Computer security is a balance between protecting information and enabling authorized access. Tightening security by making systems more inaccessible can hinder employees and make them less productive. It can also result in lower security as workers struggle to find ways around the security conditions to enable them to do their jobs. This study analyzes an information systems user survey to evaluate the tradeoffs between protection and accessibility. Over one-third of the respondents report problems with interference from security provisions. A structural equation model explores the impact of these effects on eventual security levels.
26|3||M-CLIQUES: Modified CLIQUES key agreement for secure multicast|In secure multicast applications, members may join or leave frequently and key management is one of the most challenging problems. In this research, we proposed a modified CLIQUES key management protocol. It was the modification of CLIQUES that consisted of two stages: Static CLIQUES and Hierarchical CLIQUES. In Static CLIQUES, a static group controller was used to distribute the partial keys to group members. Compared with traditional CLIQUES, the Static CLIQUES was more secure for key storage, less complex, less processing requirement in the user machine, and easier to provide member privacy protection. In Hierarchical CLIQUES, a hierarchical structure was employed to support larger size of group members. Our experiments showed that the modified CLIQUES protocol was more scalable than CLIQUES. Also, it required less processing power than the Key Tree-Based approaches.
26|3||Adaptable security mechanism for dynamic environments|Electronic services in dynamic environment (e.g. e-government, e-banking, e-commerce, etc.), meet many different barriers reducing their efficient applicability. One of them is the requirement of information security when it is transmitted, transformed, and stored in an electronic service. It is possible to provide the appropriate level of security by applying the present-day information technology. However, the level of protection of information is often much higher than it is necessary to meet potential threats. Since the level of security strongly affects the performance of the whole system, the excessive protection decreases its reliability and availability and, as a result, its global security. In this paper we present a mechanism of adaptable security for, digital information transmission systems (being usually the crucial part of e-service). It makes it possible to guarantee the adequate level of protection for actual level of threats dynamically changing in the environment. In our model the basic element of the security is the Public Key Infrastructure (PKI) is enriched with specific cryptographic modules.
26|3||Holistic security management framework applied in electronic commerce|With the advance of electronic commerce more and more companies have become dependent on their information systems for their daily business operations. This dependency requires the security of these systems to be managed. This paper presents a holistic security management framework that should allow for easy and affordable security management. This process framework is described by hierarchically organized processes which allow for a business, technology and social driven security management. It presents the activities involved in the five core and two support processes which are conducted iteratively. To support this framework three cases of successful applications and an informal evaluation against SSE-CMM are presented.
26|3||Guide for Authors.|
26|4|http://www.sciencedirect.com/science/journal/01674048/26/4|(iii) Contents|
26|4||Struggles in the academic side of infosec|
26|4||Security Views - Malware|
26|4||A qualitative study of users' view on information security|Users play an important role in the information security performance of organisations by their security awareness and cautious behaviour. Interviews of users at an IT-company and a bank were qualitatively analyzed in order to explore users' experience of information security and their personal role in the information security work. The main patterns of the study were: (1) users state to be motivated for information security work, but do not perform many individual security actions; (2) high information security workload creates a conflict of interest between functionality and information security; and (3) documented requirements of expected information security behaviour and general awareness campaigns have little effect alone on user behaviour and awareness. The users consider a user-involving approach to be much more effective for influencing user awareness and behaviour.
26|4||Teaching information systems security courses: A hands-onÂ approach|It has become imperative for companies, governments, and organizations to understand how to guard against hackers, outsiders, and even disgruntled employees who threaten their information security, integrity and daily business operations. To address national needs for computer security education, many universities have incorporated computer and security courses into their undergraduate and graduate curricula. At the Miller College of Business, Department of Information Systems and Operations Management, Ball State University, we have introduced an information systems security option for students majoring in information systems. This paper describes our approach, our experiences and lessons learned for teaching security courses using a hands-on approach.
26|4||Retraining a keystroke dynamics-based authenticator with impostor patterns|In keystroke dynamics-based authentication, novelty detection methods are used since only the valid user's patterns are available when a classifier is first constructed. After a while, however, impostors' keystroke patterns become available from failed login attempts. We propose to employ the retraining framework where a novelty detector is retrained with the impostor patterns to enhance authentication accuracy. In this paper, learning vector quantization for novelty detection and support vector data description are retrained with the impostor patterns. Experimental results show that retraining improves the authentication performance and that learning vector quantization for novelty detection outperforms other widely used novelty detectors.
26|4||Masquerade detection by boosting decision stumps using UNIX commands|Masqueraders who impersonate other users pose a serious threat to computer security. They are generally difficult to detect using firewalls or misuse-based intrusion detection systems. Although anomaly detection techniques provide a promising approach for masquerade detection, these techniques are not widely used due to their poor accuracy and relatively high false alarm rate. Previous studies of anomaly detection have mainly focused on model-based approaches, such as the support vector machine (SVM) and the hidden Markov model (HMM). Characteristics of user behavior were entered, and an evaluation value was calculated by the model. To judge whether or not the user was a masquerader, this value was compared with a predefined threshold within the model. However, the judgment processes in these models were invisible and uninterpretable by the security administrator. This study examines a different method for masquerader detection, a rule-based approach, which compares n-grams of command sequence using a technique known as boosting decision stumps. The main advantage of a rule-based method is that the generated rules are easier to interpret. The decision stump is the simplest form of a decision tree. Its “decision” is made by checking the presence or absence of a specified n-gram of command sequence. The boosting decision stumps method uses the weighted combination of the decision stumps in an application of the AdaBoost algorithm. Experiments were carried out on the common data set of UNIX commands that has been used in previous studies. The boosting decision stumps method results in an accuracy rate of 89.2% with a false alarm rate of 10.1%, while the best previously reported results had an accuracy rate of 80.1% with a false alarm rate of 9.7%. Experimental results show that the boosting decision stumps method is more effective and a more interpretable method for masquerade detection.
26|4||Dual-wrapped digital watermarking scheme for image copyright protection|Digital watermarking is an effective way to protect the rightful ownership of multimedia contents. In this paper, a two-phase watermarking scheme is proposed, which extracts both the grayscale watermark and the binary one from the protected images to achieve the copyright protection goal. In the first phase, the proposed method utilizes the pixel values of the original image to construct a grayscale watermark image. In the second phase, a binary watermark image can be further retrieved via the just-procured-permuted grayscale watermark from the first phase. Under these circumstances, the proposed technique results in lossless embedding; in other words, the protected images are the same as the original ones. The overall verification procedure does not need the original image. Only those who have the original grayscale watermark and the corresponding secret keys can extract the grayscale and binary watermarks sequentially, which enhances security and robustness of the proposed watermarking system. Experimental results show that the proposed approach satisfies the general requirements of image watermarking and is superior to related methods in terms of transparency and robustness. Moreover, it is easier to be implemented than transform-domain techniques. These flexible features make the proposed method more feasible and practical for copyright protection.
26|4||A resource-constrained group key agreement protocol for imbalanced wireless networks|Secure group communication is an important research issue for network security because of the popularity of group-oriented applications such as electronic conferences and collaborative works. The secure group key agreement protocol design is crucial for achieving secure group communications. As we all know, most security technologies are currently deployed in wired networks and are not fully applicable to wireless networks involving mobile devices with limited computing capability. In 2005, Nam et al. proposed a group key agreement protocol for a wireless environment. Unfortunately, in this paper we present that their protocol has a security weakness in which participants cannot confirm that their contributions were actually involved in the group key establishment. This is an important property of group key agreement. Therefore, we propose a new group key agreement protocol for an imbalanced wireless network consisting of many mobile nodes with limited computing capability and a powerful node with less restriction. We show that the proposed protocol produces contributory group key agreement. We demonstrate that the proposed protocol is provably secure against passive attacks under the decisional Diffie–Hellman problem assumption. A simulation result on a personal digital assistant (PDA) shows that the proposed protocol is well suited for mobile devices with limited computing capability.
26|4||Functional similarities between computer worms and biological pathogens|Computer worms pose a serious threat to computer and network security. Interestingly, they share many common tactics with biological pathogens with respect to infecting and propagating. In this paper, we study the six most common fatal infectious diseases—measles, malaria, HIV/AIDS, tuberculosis, influenza and the diarrhoeal diseases—to (1) determine the individual mechanisms and environmental conditions that have contributed to their success, and (2) show the parallels between the mechanisms and behavior of successful biological infections and successful digital infections. Moreover, by identifying the specific areas of similarity and looking at effective preventive and creative measures used against biological pathogens, we draw insights about what steps individual computers and networks can take to protect themselves.
26|4||Guide for Authors.|
26|5|http://www.sciencedirect.com/science/journal/01674048/26/5|(iii) Contents|
26|5||Vulnerability Take Grant (VTG): An efficient approach to analyze network vulnerabilities|Modeling and analyzing information system vulnerabilities help predict possible attacks to computer networks using vulnerabilities information and the network configuration. In this paper, we propose a comprehensive approach to analyze network vulnerabilities in order to answer the safety problem focusing on vulnerabilities. The approach which is called Vulnerability Take Grant (VTG) is a graph-based model consists of subject/objects as nodes and rights/relations as edges to represent the system protection state. Each node may have properties including single vulnerabilities such as buffer overflow. We use the new concept of vulnerability rewriting rule to specify the requirements and consequences of exploiting vulnerabilities. Analysis of the model is achieved using our bounded polynomial algorithm, which generates the most permissive graph in order to verify whether a subject can obtain an access right over an object. The algorithm also finds the likely attack scenarios. Applicability of the approach is investigated by modeling widespread vulnerabilities in their general patterns. A real network is modeled as a case study in order to examine how an attacker can gain unauthorized access via exploiting the chain of vulnerabilities. Our experience shows the efficiency, applicability, and expressiveness in modeling a broader range of vulnerabilities in our approach in comparison to the previous approaches.
26|5||The impact that placing email addresses on the Internet has on the receipt of spam: An empirical analysis|Email communication is encumbered with a mass of email messages which their recipients have neither requested nor require. Even worse, the impacts of these messages are far from being simply an annoyance, as they also involve economic damage. This manuscript examines the resource “email addresses”, which is vital for any potential bulk mailer and spammer. Both a methodology and a honeypot conceptualization for implementing an empirical analysis of the usage of email addresses placed on the Internet are proposed here. Their objective is to assess, on a quantitative basis, the extent of the current harassment and its development over time. This “framework” is intended to be extensible to measuring the effectiveness of address obscuring techniques. The implementation of a pilot honeypot is described, which led to key findings, some of them being: (1) Web placements attract more than two-thirds (70%) of all honeypot spam emails, followed by newsgroup placements (28.6%) and newsletter subscriptions (1.4%). (2) The proportions of spam relating to the email addresses' top-level domain can be statistically assumed to be uniformly distributed. (3) More than 43% of addresses on the web have been abused, whereas about 27% was the case for addresses on newsgroups and only about 4% was the case for addresses used for a newsletter subscription. (4) Regarding the development of email addresses' attractiveness for spammers over time, the service “web sites” features a negative linear relationship, whereas the service “Usenet” shows a negative exponential relationship. (5) Only 1.54% of the spam emails showed an interrelation between the topic of the spam email and that of the location where the recipient's address was placed, so that spammers are assumed to send their emails in a “context insensitive” manner. The results of the empirical analysis motivate the need for the protection of email addresses through obscuration. We analyze this need by formulating requirements for address obscuring techniques and we reveal to which extent today's most relevant approaches fulfill these requirements.
26|5||Authentication in a layered security approach for mobile ad hoc networks|An ad hoc network is a collection of nodes that do not need to rely on a predefined infrastructure to keep the network connected. Nodes communicate amongst each other using wireless radios and operate by following a peer-to-peer network model. In this article we investigate authentication in a layered approach, which results to multiple lines of defense for mobile ad hoc networks. The layered security approach is described and design criteria for creating secure ad hoc network using multiple authentication protocols are analyzed. The performance of several such known protocols, which are based on challenge–response techniques, is presented through simulation results.
26|5||Using header session messages to anti-spamming|The Internet is popular, with email use functioning as the major Internet activity. However, spam has recently become a major problem impeding the use of email. Many spam filtering techniques have been implemented so far. Most current anti-spamming techniques filter out junk emails based on email subjects and body messages. Nevertheless, subjects and email contents are not the only cues for judging spam. This investigation presents a statistical analysis of the header session messages of junk and normal emails, and explores the possibility of utilizing these messages to perform spam filtering. The message head session, including the sender's mail address, receiver's mail address and time, which is of little interest to most users, also provides further information for anti-spamming purpose. A statistical analysis is undertaken on the content of 10,024 junk emails collected from a Spam Archive database, and 599 regular emails in company with 635 solicited listserv or commercial emails contributed by volunteers. Content analysis results demonstrate that up to 92.5% of junk emails are filtered out when utilizing the message-ID, mail user agent, and sender and receiver addresses in the header session as cues. Additionally, the proposed approach may induce a low block error rate for normal emails for the sample utilized in this investigation. This low rate of over-block errors is a significant merit of the proposed anti-spamming approach. The proposed approach of utilizing header session messages to filter out junk emails may coexist with other anti-spamming approaches. Therefore, no conflict arises between the proposed approach and existing spam prevention approaches.
26|5||Security for a Multi-Agent System based on JADE|The present paper explores the challenges, issues and solutions to satisfy the security requirements of a Multi-Agent System (MAS) based on the JADE framework. By means of a prototype system used for Learning Management, an adequate security concept for MAS in general is presented. Hereby several security features are considered, ranging, among others, from the authentication of users over encryption of the exchanged data up to the authorization of the access to services designated only to a determined group of users.
26|5||Comparative studies on authentication and key exchange methods for 802.11 wireless LAN|IEEE 802.11 wireless LAN has become one of the hot topics on the design and development of network access technologies. In particular, its authentication and key exchange (AKE) aspects, which form a vital building block for modern security mechanisms, deserve further investigation. In this paper we first identify the general requirements used for WLAN authentication and key exchange (AKE) methods, and then classify them into three levels (mandatory, recommended, and additional operational requirements). We present a review of issues and proposed solutions for AKE in 802.11 WLANs. Three types of existing methods for addressing AKE issues are identified, namely, the legacy, layered and access control-based AKE methods. Then, we compare these methods against the identified requirements. Based on the analysis, a multi-layer AKE framework is proposed, together with a set of design guidelines, which aims at a flexible, extensible and efficient security as well as easy deployment.
26|5||Assessing the security perceptions of personal Internet users|Personal Internet users are increasingly finding themselves exposed to security threats during their use of home PC systems. However, concern can be raised about users' awareness of these problems, and the extent to which they are consequently protected and equipped to deal with incidents they may encounter. This paper presents results from a survey of 415 home users to assess their perceptions of security issues, and their attitudes towards the use of related safeguards. The findings reveal that although there is a high degree of confidence at a surface level, with respondents claiming to be aware of the threats and utilising many of the relevant safeguards, a deeper inspection suggests that there are several areas in which desirable knowledge and understanding are lacking. Although many of the problems were predictably acute amongst novice users, there were also notable shortcomings amongst users who considered themselves to have advanced levels of computing experience.
26|5||Guide for Authors.|
26|6|http://www.sciencedirect.com/science/journal/01674048/26/6|(iii) Contents|
26|6||From the Publisher|
26|6||Anti-keylogging measures for secure Internet login: An example of the law of unintended consequences|Traditional authentication systems used to protect access to online services (such as passwords) are vulnerable to compromise via the introduction of a keystroke logger to the service user's computer. This has become a particular problem now that many malicious programs have keystroke logging capabilities. When banks first introduced Online Banking services they realised this, and added features to protect users against keystroke logging. In this paper we show, using a real Online Banking system as an example, that if these features are incorrectly implemented they can allow an attacker to bypass them completely and gain access to a user's bank account within a small number of attempts. The vulnerability was initially noticed in a particular Online Banking service, but any system implemented in the way we describe is equally vulnerable.
26|6||An adaptive method for anomaly detection in symmetric network traffic|Symmetry is an obvious phenomenon in two-way communications. In this paper, we present an adaptive nonparametric method that can be used for anomaly detection in symmetric network traffic. Two important features are emphasized in this method: (i) automatic adjustment of the detection threshold according to the traffic conditions; and (ii) timely detection of the end of an anomalous event. Source-end defense against SYN flooding attacks is used to illustrate the efficacy of this method. Experiments on real traffic traces show that this method has high detection accuracy and low detection delays, and excels at detecting low intensity attacks.
26|6||Making security usable: Are things improving?|Given the increased focus on the need for usable security, it is now to be hoped that the issue will receive greater attention in new software releases. Unfortunately, however, there is still evidence to suggest that usable security receives insufficient consideration when the related features are presented in the context of larger applications. As an illustration of this claim, the paper examines how the security-related features have evolved within new releases of Internet Explorer and Word, and identifies that although there have been some improvements when compared to earlier versions, there are also aspects that will represent new or ongoing problems for users. Examples of such problems are highlighted in a number of security-related interfaces from both applications, with the use of technical terminology and/or a lack of accompanying help being amongst the frequent concerns. Nielsen's usability heuristics are then used as the basis for a summary-level evaluation, to illustrate how the identified issues also contravene good practice in user interface design.
26|6||Guide for Authors.|
26|7-8|http://www.sciencedirect.com/science/journal/01674048/26/7-8|(iii) Contents|
26|7-8||An assessment of website password practices|Password-based authentication is frequently criticised on the basis of the ways in which the approach can be compromised by end-users. However, a fundamental point in the defence of many users is that they may not know any better, and lack appropriate guidance and support when choosing their passwords and subsequently attempting to manage them. Given that such support could reasonably be expected to come from the systems upon which the passwords are used, this paper presents an assessment of password practices on 10 popular websites, examining the extent to which they provide guidance for password selection, enforce restrictions on password choices, and support easy and effective recovery or reset if passwords are forgotten. The findings reveal that the situation is extremely variable, with none of the assessed sites performing ideally across all of the assessed criteria. Better efforts are consequently required if password practices amongst the general populous are expected to improve.
26|7-8||Key agreement for key hypergraph|In this paper, we propose a key agreement protocol for a key hypergraph. In a key hypergraph, a party is represented as a vertex and a group of parties is represented as a hyperedge. A key agreement protocol for a key hypergraph establishes all the keys for hyperedges in a key hypergraph at the same time. A naive approach would be to run a group key protocol concurrently for each hyperedge. By using the randomness re-use technique, we propose an efficient key agreement protocol for a key hypergraph, which is of two rounds.
26|7-8||An active learning based TCM-KNN algorithm for supervised network intrusion detection|As network attacks have increased in number and severity over the past few years, intrusion detection is increasingly becoming a critical component of secure information systems and supervised network intrusion detection has been an active and difficult research topic in the field of intrusion detection for many years. However, it hasn't been widely applied in practice due to some inherent issues. The most important reason is the difficulties in obtaining adequate attack data for the supervised classifiers to model the attack patterns, and the data acquisition task is always time-consuming and greatly relies on the domain experts. In this paper, we propose a novel supervised network intrusion detection method based on TCM-KNN (Transductive Confidence Machines for K-Nearest Neighbors) machine learning algorithm and active learning based training data selection method. It can effectively detect anomalies with high detection rate, low false positives under the circumstance of using much fewer selected data as well as selected features for training in comparison with the traditional supervised intrusion detection methods. A series of experimental results on the well-known KDD Cup 1999 data set demonstrate that the proposed method is more robust and effective than the state-of-the-art intrusion detection methods, as well as can be further optimized as discussed in this paper for real applications.
26|7-8||A non-intrusive biometric authentication mechanism utilising physiological characteristics of the human head|This paper proposes and evaluates a non-intrusive biometric authentication technique drawn from the discrete areas of biometrics and Auditory Evoked Responses. The technique forms a hybrid multi-modal biometric in which variations in the human voice due to the propagation effects of acoustic waves within the human head are used to verify the identity of a user. The resulting approach is known as the Head Authentication Technique (HAT). Evaluation of the HAT authentication process is realised in two stages. First, the generic authentication procedures of registration and verification are automated within a prototype implementation. Second, a HAT demonstrator is used to evaluate the authentication process through a series of experimental trials involving a representative user community. The results from the trials confirm that multiple HAT samples from the same user exhibit a high degree of correlation, yet samples between users exhibit a high degree of discrepancy. Statistical analysis of the prototype performance realised system error rates of 6% False Non-Match Rate (FNMR) and 0.025% False Match Rate (FMR).
26|7-8||Mining TCP/IP packets to detect stepping-stone intrusion|An effective approach of detecting stepping-stone intrusion is to estimate the number of hosts compromised through estimating the length of a connection chain. This can be done by studying the changes in TCP packet round-trip time. In this paper, we propose a new algorithm by using data mining method to find the round-trip time from the timestamps of TCP send and echo packets. Previous algorithms produce either good packet matches on very few packets, or poor matches on many packets. This method gives us better round-trip time and more matched packets than other algorithms proposed in the past. It can estimate the length of a connection more accurate than other methods and has largely decreased false positive error and false negative error in detecting stepping-stone intrusion comparing with existing methods.
26|7-8||The security challenges inherent in VoIP|VoIP offers unparalleled flexibility for users, but with that flexibility also come security concerns. Moving both the control and data channels to IP opens the doors to new types of attack not seen before on phone systems.
26|7-8||Intrusion detection using text processing techniques with a kernel based similarity measure|This paper focuses on intrusion detection based on system call sequences using text processing techniques. It introduces kernel based similarity measure for the detection of host-based intrusions. The k-nearest neighbour (kNN) classifier is used to classify a process as either normal or abnormal. The proposed technique is evaluated on the DARPA-1998 database and its performance is compared with other existing techniques available in the literature. It is shown that this technique is significantly better than the other techniques in achieving lower false positive rates at 100% detection rate.
26|7-8||Run-time label propagation for forensic audit data|It is desirable to be able to gather more forensically valuable audit data from computing systems than is currently done or possible. This is useful for the analysis of events that took place on the system for the purpose of digital forensic investigations. In this paper we propose a mechanism that allows arbitrary meta-information bound to principals on a system to be propagated based on causality and influenced by information flow. We further discuss how to implement such a mechanism for the FreeBSD operating system and present a proof-of-concept implementation that has little overhead compared to the system without label propagation.
26|7-8||Guide for Authors.|
27|1-2|http://www.sciencedirect.com/science/journal/01674048/27/1-2|(iii) Contents|
27|1-2||Editorial|
27|1-2||Improvement of keystroke data quality through artificial rhythms and cues|Keystroke dynamics based user authentication (KDA) can achieve a relatively high performance if a fairly large number of typing patterns are available. It is almost always the case that KDA is combined with password based authentication. Users are often required to change their passwords. When a user changes one's password, however, only a handful of new patterns become available. In a mobile situation, moreover, very short passwords are used. Under such a circumstance, the quality of data becomes important. Recently, artificial rhythms and cues were proposed to improve the quality of data. In this paper, we verify the effectiveness of artificial rhythms and cues through hypotheses tests using the data from 25 users under various situations. The experimental results show that artificial rhythms increase the uniqueness while cues increase the consistency.
27|1-2||Efficient identity-based RSA multisignatures|A digital multisignature is a digital signature of a message generated by multiple signers with knowledge of multiple private keys. In this paper, an efficient RSA multisignature scheme based on Shamir's identity-based signature (IBS) scheme is proposed. To the best of our knowledge, this is the first efficient RSA-based multisignature scheme with both fixed length and the verification time. The proposed identity-based multisignature scheme is secure against forgerability under chosen-message attack. It is also secure against multi-signer collusion attack and adaptive chosen-ID attack.
27|1-2||Cryptanalysis of simple three-party key exchange protocol|Recently, Lu and Cao published a novel protocol for password-based authenticated key exchanges (PAKE) in a three-party setting in Journal of Computers and Security, where two clients, each shares a human-memorable password with a trusted server, can construct a secure session key. They argued that their simple three-party PAKE (3-PAKE) protocol can resist against various known attacks. In this paper, we show that this protocol is vulnerable to a kind of man-in-the-middle attack that exploits an authentication flaw in their protocol and is subject to the undetectable on-line dictionary attack. We also conduct a detailed analysis on the flaws in the protocol and provide an improved protocol.
27|1-2||Enterprise information security strategies|Security decisions are made at every level of an organization and from diverse perspectives. At the tactical and operational levels of an organization, decision making focuses on the optimization of security resources, that is, an integrated combination of plans, personnel, procedures, guidelines and technology that minimize damages and losses. While these actions and tactics reduce the frequency and/or consequences of security breaches, they are bounded by the organization's global security budget. At the strategic, enterprise level management must answer the question, “What is the security budget (cost expenditures), where each dollar spent on security must be weighed against alternative non-security expenditures, that is justified by the foregone (prevented) losses and damages?” The answer to that question depends on the tolerances of decision makers for risk and the information employed to reach it.
27|1-2||A global security architecture for intrusion detection on computer networks|Detecting all kinds of intrusions efficiently requires a global view of the monitored network. Built to increase the security of computer networks, traditional IDS's are unfortunately unable to give a global view of the security of a network. To overcome this situation, we are developing a distributed SOC (Security Operation Center) which is able to detect attacks occurring simultaneously on several sites in a network and to give a global view of the security of that network. In this article, we present the global architecture of our system, called DSOC as well as several methods used to test its accuracy and performance.
27|1-2||Containing large-scale worm spreading in the Internet by cooperative distribution of traffic filtering policies|The Internet is crucial to business, government, education and many other facets of society, but the easy access and wide usage of the most common network services make it a primary target for the propagation of viral infections or worms. It has been widely experienced that the massive worldwide spreading of very fast and aggressive worms may easily disrupt or damage the connectivity of large sections of the Internet, affecting millions of users. Classical containment strategies, based on manual application of traffic filters will be almost totally ineffective in the wide area. Consequently, developing an automated self-distributing containment strategy is the most viable way to defeat the worm propagation in an acceptable time The objective of our work is to develop a distributed and cooperative containment strategy based on having traffic filtering information dynamically disseminate throughout the network at a speed that is faster than (or at least comparable with) the propagation of worms. Our framework based on BGP extensions to distribute traffic filtering information has the advantage of using the existing infrastructure and inter-as communication channels. We envision that the above solution will be one of the most effective and challenging lines of defense against next-generation more aggressive worms.
27|3-4|http://www.sciencedirect.com/science/journal/01674048/27/3-4|(iii) Contents|
27|3-4||Editorial|
27|3-4||SSL/TLS session-aware user authentication revisited|Man-in-the-middle (MITM) attacks pose a serious threat to SSL/TLS-based e-commerce applications. In Oppliger R, Hauser R, Basin D [SSL/TLS session-aware user authentication – or how to effectively thwart the man-in-the-middle. Computer Communications August 2006;29(12):2238–46] and Oppliger R, Hauser R, Basin D [SSL/TLS session-aware user authentication. IEEE Computer March 2008;41(3) 59-65], we introduced the notion of SSL/TLS session-aware user authentication to protect SSL/TLS-based e-commerce applications against MITM attacks and we proposed an implementation based on impersonal authentication tokens. In this paper, we present a number of extensions of the basic idea. These include multi-institution tokens, possibilities for changing the PIN, and different ways of making several popular and widely deployed user authentication systems SSL/TLS session-aware.
27|3-4||Standardising vulnerability categories|Each vulnerability scanner (VS) represents, identifies and classifies vulnerabilities in its own way, thus making the different scanners difficult to study and compare. Despite numerous efforts by researchers and organisations to solve the disparity in vulnerability names used in the different VSs, vulnerability categories have still not been standardised. This paper highlights the importance of having a standard vulnerability category set. It also outlines an approach towards achieving this goal by generating a standard set of vulnerability categories. A data-clustering algorithm that employs artificial intelligence is used for this purpose. The significance of this research results from having an intelligent technique that aids in the generation of standardised vulnerability categories in a relatively fast way. In addition, the technique is generic in the sense that it allows one to accommodate any VS currently known on the market to create such vulnerability categories. Another benefit is that the approach followed in this paper allows one to also compare various VSs currently available on the market. A prototype is presented to verify the concept.
27|3-4||A feasible intrusion detector for recognizing IIS attacks based on neural networks|Most activities on the Internet can be recorded as log files of websites and website administrators can inspect log files to locate problems after any network intrusion occurs. However, since log files usually contain a huge quantity of data, without effective methods, it is generally not feasible for administrators to determine the concealed meanings within log files. One method for dealing with this issue is to use neural networks; this is an effective means to distinguish and classify abnormal data in log files, thus alleviating the administrator's burden. This paper presents the results of a study on intrusion detection on IIS (Internet information services) utilizing a hybrid intrusion detection system (IDS). The feasibility of the hybrid IDS is validated based on the Internet scanner system (ISS). In the intrusion detection system proposed, we used four different training data sets: 200, 800, 1400, and 2000. The system is trained either by Taguchi's experimental design or full factorial experimental design under different training data sets; the former can save much more time than the latter. Under Taguchi's experimental design, the best results are obtained when the training data set is of size 1400; overall accuracy in this case is 97.5%. On the contrary, for the full factorial experimental design, the best results are reached when the training data set is of size 2000; overall accuracy is 97.6%. Our study indicates that when to retrain the detector and how much time to allow for this training fully depend on the downgrade percentage of the detection rate, which determines the size of the retraining data set. To reduce the void time for updating the detector, the downgrade percentage should be restricted.
27|3-4||An aspect-oriented approach for the systematic security hardening of code|In this paper, we present an aspect-oriented approach for the systematic security hardening of source code. It aims at allowing developers to perform software security hardening by providing an abstraction over the actions required to improve the security of the program. This is done by giving them the capabilities to specify high-level security hardening plans that leverage a priori defined security hardening patterns. These patterns describe the required steps and actions to harden security code, including detailed information on how and where to inject the security code. We show the viability and relevance of our approach by: (1) elaborating security hardening patterns and plans to common security hardening practices, (2) realizing these patterns by implementing them into aspect-oriented languages, (3) applying them to secure applications, (4) testing the hardened applications. Furthermore, we discuss, in this paper, our insights on the appropriateness, strengths and limitations of the aspect-oriented paradigm for security hardening.
27|3-4||Efficient multi-server authentication scheme based on one-way hash function without verification table|Following advances in network technologies, an increasing number of systems have been provided to help network users via the Internet. In order to authenticate the remote users, password-based security mechanisms have been widely used. They are easily implemented, but these mechanisms must store a verification table in the server. If an attacker steals the verification table from the server, the attacker may masquerade as a legal user. To solve the verification table stolen problem, numerous single server authentication schemes without verification tables have been proposed. These single authentication schemes suffer from a shortcoming. If a remote user wishes to use numerous network services, they must register their identity and password in these servers. In response to this problem, numerous related studies recently have been proposed. These authentication schemes enable remote users to obtain service from multiple servers without separately registering with each server. This study proposes an alternative multi-server authentication scheme using smart cards. The proposed scheme is based on the nonce, uses one-way hash function, and does not need to store any verification table in the server and registration center. The proposed scheme can withstand seven well known network security attacks.
27|5-6|http://www.sciencedirect.com/science/journal/01674048/27/5-6|(iii) Contents|
27|5-6||From the Editor-in-Chief|
27|5-6||Information security requirements â Interpreting the legal aspects|With information security being the focal point of business in the media and in legislatures around the world, organisations face complex requirements to comply with security and privacy standards and regulations. The escalating magnitude of national and international laws and regulations, such as Sarbanes–Oxley, Gramm–Leach–Bliley and Basel II, caused organisations to become increasingly aware of the importance of legal compliance and the obligations that arise from it. The challenge of meeting these obligations has become a complex web of requirements that grows exponentially as organisations cross international boundaries. This paper attempts to provide an interpretation of the legal aspects, as a starting point for clarifying compliance issues, as referred to by ISO/IEC 27002 (ISO/IEC 27002, 2005; previously known as ISO/IEC 17799, 2005). ISO/IEC 27002 further mentions three sources from which information security requirements can be derived, of which one will be focused on within this paper, namely the legal requirements. The interpretation of the legal aspects thus forms the foundation for motivating a proposed model for determining legal requirements, which in turn, indicates relevant information security controls from the list provided in ISO/IEC 27002, to satisfy the identified legal requirements.
27|5-6||A SIP-oriented SPIT Management Framework|Voice over IP (VoIP) telephony is increasingly gaining popularity among home and business users alike, as a viable alternative to traditional telephony, and is expected to achieve a significant market share in the near future. When this happens, it is also expected that several new threats exploiting the Internet vulnerabilities will appear. One of these is the Spam over Internet Telephony (SPIT). This paper examines in detail the SPIT attack, provides a review and assessment of previously proposed SPIT management techniques and proposes the use of an attack-oriented methodology for thwarting the SPIT threat. This results in a generic SPIT management framework, which combines the strengths of existing solutions, while alleviating their insufficiencies.
27|5-6||SMSSec: An end-to-end protocol for secure SMS|Short Message Service is usually used to transport unclassified information, but with the rise of mobile commerce it has become an integral tool for conducting business. However, SMS does not guarantee confidentiality and integrity of the message content. This paper proposes a protocol called SMSSec that can be used to secure an SMS communication sent by Java's Wireless Messaging API. The physical limitations of the intended devices such as mobile phones, made it necessary to develop a protocol which would make minimal use of computing resources. SMSSec has a two-phase protocol with the first handshake using asymmetric cryptography which occurs only once, and a more efficient symmetric nth handshake which is used more dominantly. What distinguishes this work from conventional protocols is the ability to perform the secure transmission with limited size messages. Performance analysis showed that the encryption speed on the mobile device is faster than the duration of the transmission. To achieve security in the mobile enterprise environment, this is deemed a very acceptable overhead. Furthermore, a simple mechanism handles fault tolerance without additional overhead is proposed.
27|5-6||Critical study of neural networks in detecting intrusions|This paper presents a critical study about the use of some neural networks (NNs) to detect and classify intrusions. The aim of our research is to determine which NN classifies well the attacks and leads to the higher detection rate of each attack. This study focused on two classification types of records: a single class (normal, or attack), and a multiclass, where the category of attack is also detected by the NN. Five different types of NNs were tested: multilayer perceptron (MLP), generalized feed forward (GFF), radial basis function (RBF), self-organizing feature map (SOFM), and principal component analysis (PCA) NN. A KDD data subset containing 18,285 records manually chosen was trained in order to be tested on the KDD testing set. Our simulations show that the GFF NN leads to the best confusion matrix in the multiclass case. For the same case, the RBF performs the higher detection rate of the DoS attack category. In the single class case, the PCA NN performs the higher detection rate.
27|5-6||Application-based anomaly intrusion detection with dynamic information flow analysis|This paper presents a new approach to detecting software security failures, whose primary goal is facilitating identification and repair of security vulnerabilities rather than permitting online response to attacks. The approach is based on online capture of executions and offline execution replay, profiling, and analysis. It employs fine-grained dynamic information flow analysis in conjunction with anomaly detection. This approach, which we call information flow anomaly detection, is capable of detecting a variety of security failures, including both ones that involve violations of confidentiality or integrity requirements and ones that do not. A prototype tool called DynFlow implementing the approach has been developed for use with Java byte code programs. To illustrate the potential of the approach, it is applied to detect security failures of four open source systems. Also, its effectiveness is compared to the effectiveness of an approach to anomaly detection that is based on analyzing method call stacks.
27|5-6||Building network attack graph for alert causal correlation|Most network administrators have got unpleasant experience of being overwhelmed by tremendous unstructured network security alerts produced by heterogeneous devices. To date, various approaches have been proposed to correlate security alerts, including the adoption of attack graphs to clarify their causal relationship. However, there still lacks an efficient and operational method to generate attack graphs tailored to alert causal correlation.
27|5-6||Enforcing memory policy specifications in reconfigurable hardware|While general-purpose processor based systems are built to enforce memory protection to prevent the unintended sharing of data between processes, current systems built around reconfigurable hardware typically offer no such protection. Several reconfigurable cores are often integrated onto a single chip where they share external resources such as memory. While this enables small form factor and low cost designs, it opens up the opportunity for modules to intercept or even interfere with the operation of one another. We investigate the design and synthesis of an FPGA memory protection mechanism capable of enforcing access control policies and a methodology for translating formal policy descriptions into FPGA enforcement mechanisms. The efficiency of our access language design flow is evaluated in terms of area and cycle time across a variety of security scenarios. We also describe a technique for ensuring that the internal state of the reference monitor cannot be used as a covert storage channel.
27|5-6||Practical anonymous user authentication scheme with security proof|An authenticated key distribution scheme preserving user anonymity is important to those e-commerce applications where user anonymity is required or desirable. However, the previous works on this issue have security flaws. This paper shows the security weaknesses of a recently published work, and then proposes our new scheme, which not only overcomes the weaknesses but also improves the computational performance. The security of the proposed scheme is rigorously examined in a modified Bellare–Rogaway model.
27|5-6||Information security management: An information security retrieval and awareness model for industry|The purpose of this paper is to present a conceptual view of an Information Security Retrieval and Awareness (ISRA) model that can be used by industry to enhance information security awareness among employees. A common body of knowledge for information security that is suited to industry and that forms the basis of this model is accordingly proposed. This common body of knowledge will ensure that the technical information security issues do not overshadow the non-technical human-related information security issues. The proposed common body of knowledge also focuses on both professionals and low-level users of information. The ISRA model proposed in this paper consists of three parts, namely the ISRA dimensions (non-technical information security issues, IT authority levels and information security documents), information security retrieval and awareness, and measuring and monitoring. The model specifically focuses on the non-technical information security that forms part of the proposed common body of knowledge because these issues have, in comparison with the technical information security issues, always been neglected.
27|7-8|http://www.sciencedirect.com/science/journal/01674048/27/7-8|(iii) Contents|
27|7-8||Editorial|
27|7-8||Security beliefs and barriers for novice Internet users|End-users are now recognized as being at increased risk in online scenarios, with a range of threats that seek to specifically target them and exploit their systems. Novice users are particularly likely to face difficulties in this context, as their unfamiliarity with the technology can limit their ability to recognize the threats and understand the required protection. This paper presents the results from a qualitative study, arising from detailed interviews conducted with 20 novice users in order to assess their views and experiences with Internet security. The findings reveal a general awareness of the existence of threats, but less familiarity with the appropriate safeguards beyond a very basic level. Although users generally recognize that they have a responsibility for their own protection, they often appear unconcerned about the potential impacts of the problems. In other cases, they felt unable to address their concerns as a result of their lack of technical knowledge or obstacles posed by security tools.
27|7-8||Information security awareness in higher education: An exploratory study|The research explores factors that affect information security awareness of staff, including information systems decision makers, in higher education within the context of a developing country, namely the UAE. An interpretive case-study approach is employed using multiple data gathering methods. The research reveals that factors such as conscientiousness, cultural assumptions and beliefs, and social conditions affect university staff behaviour and attitude towards work, in general, and information security awareness, in particular. A number of recommendations are provided to initiate and promote IS security awareness in the studied environment.
27|7-8||Consensus ranking â An ICT security awareness case study|There are many disciplines where the problem of consensus ranking plays a vital role. Decision-makers are frequently asked to express their preferences for a group of objects, e.g. new projects, new products, candidates in an election, etc. The basic problem then becomes one of combining the individual rankings into a group choice or consensus ranking. The objective of this paper is to report on the application of two management science methodologies to the problem of identifying the most important areas to be included in an Information Communications Technology (ICT) security awareness program. The first methodology is based on the concept of minimizing the distance (disagreement) between individual rankings, while the second one employs a heuristic approach. A real-world case study from the mining industry is presented to illustrate the methods.
27|7-8||Criteria to evaluate Automated Personal Identification Mechanisms|The consequences of digital identity compromises suggest that selected Automated Personal Identification Mechanisms, which enable computer systems to identify individuals, may be unsuitable in some contexts. Currently, there is no commonly agreed set of factors upon which to base an evaluation, regardless of purpose or requirements.
27|7-8||An ontology-based policy for deploying secure SIP-based VoIP services|Voice services over Internet Protocol (VoIP) are nowadays much promoted by telecommunication and Internet service providers. However, the utilization of open networks, like the Internet, raises several security issues that must be accounted for. On top of that, there are new sophisticated attacks against VoIP infrastructures that capitalize on vulnerabilities of the protocols employed for the establishment of a VoIP session (for example the Session Initiation Protocol – SIP).
27|7-8||Secure log management for privacy assurance in electronic communications|In this paper we examine logging security in the environment of electronic communication providers. We review existing security threat models for system logging and we extend these to a new security model especially suited for communication network providers, which also considers internal modification attacks. We also propose a framework for secure log management in public communication networks as well as an implementation design, in order to provide traceability under the extended security model. A key role to the proposed framework is given to an independent Regulatory Authority, which is responsible to maintain log integrity proofs in a remote environment and verify the integrity of the provider's log files during security audits.
27|7-8||An optimistic fair exchange protocol based on signature policies|The growth of the e-commerce has allowed companies and individuals to sell and purchase almost any kind of product and service through the Internet. However, during the purchase transaction there is a moment during which the seller has sensitive information from the buyer, typically his/her credit card information, while the buyer has nothing from the seller. This situation clearly places the buyer at disadvantage and is, together with fear of fraud, one of the reasons of the lack of confidence in e-commerce. For resolving this situation a new fair exchange protocol based on signature policies is presented. A signature policy is a set of rules to create and validate electronic signatures, under which an electronic signature can be determined to be valid in a particular transaction context. Due to the signature policy-based design, the proposed protocol allows the buyer to decide if trust or not in the rules that will manage the transaction, increasing the user's confidence in e-commerce. Security, fairness and timeliness characteristics of the protocol are evaluated. Implementation guidelines are also provided taking into consideration latest security standards.
27|7-8||Automated containment of rootkits attacks|Rootkit attacks are a serious threat to computer systems. Packaged with other malwares such as worms, viruses and spyware, rootkits pose a more potent threat than ever before by allowing malware to evade detection. In the absence of appropriate tools to counter such attacks, compromised machines stay undetected for extended periods of time. Leveraging virtual machine technology, we propose a solution for real-time automated detection and containment of rootkit attacks. We have developed a prototype using VMware Workstation to illustrate the solution. Our analysis and experimental results indicate that this approach can very successfully detect and contain the effects of a large percentage of rootkits found for Linux today. We also demonstrate with an example, how this approach is particularly effective against malwares that use rootkits to hide.
27|7-8||Evaluation of a low-rate DoS attack against application servers|In the network security field there is a need to identify new movements and trends that attackers might adopt, in order to anticipate their attempts with defense and mitigation techniques. The present study explores new approaches that attackers could use in order to make denial of service attacks against application servers. We show that it is possible to launch such attacks by using low-rate traffic directed against servers, and apply the proposed techniques to defeat a persistent HTTP server. The low-rate feature is highly beneficial to the attacker for two main reasons: firstly, because the resources needed to carry out the attack are considerably reduced, easing its execution. Secondly, the attack is more easily hidden to security mechanisms that rely on the detection of high-rate traffic. In this paper, a mechanism that allows the attacker to control the attack load in order to bypass an IDS is contributed. We present the fundamentals of the attack, describing its strategy and design issues. The performance is also evaluated in both simulated and real environments. Finally, a study of possible improvement techniques to be used by the attackers is contributed.
27|7-8||Dynamic models for computer viruses|Computer viruses are an important risk to computational systems endangering either corporations of all sizes or personal computers used for domestic applications. Here, classical epidemiological models for disease propagation are adapted to computer networks and, by using simple systems identification techniques a model called SAIC (Susceptible, Antidotal, Infectious, Contaminated) is developed. Real data about computer viruses are used to validate the model.
28|1-2|http://www.sciencedirect.com/science/journal/01674048/28/1-2|Contents|
28|1-2||Editorial|
28|1-2||Building access control models with attribute exploration|The use of lattice-based access control models has been somewhat restricted by their complexity. We argue that attribute exploration from formal concept analysis can help create lattice models of manageable size, while making it possible for the system designer to better understand dependencies between different security categories in the domain and, thus, providing certain guarantees for the relevance of the constructed model to a particular application. In this paper, we introduce the method through an example.
28|1-2||A distributed requirements management framework for legal compliance and accountability|Increasingly, new regulations are governing organizations and their information systems. Individuals responsible for ensuring legal compliance and accountability currently lack sufficient guidance and support to manage their legal obligations within relevant information systems. While software controls provide assurances that business processes adhere to specific requirements, such as those derived from government regulations, there is little support to manage these requirements and their relationships to various policies and regulations. We propose a requirements management framework that enables executives, business managers, software developers and auditors to distribute legal obligations across business units and/or personnel with different roles and technical capabilities. This framework improves accountability by integrating traceability throughout the policy and requirements lifecycle. We illustrate the framework within the context of a concrete healthcare scenario in which obligations incurred from the Health Insurance Portability and Accountability Act (HIPAA) are delegated and refined into software requirements. Additionally, we show how auditing mechanisms can be integrated into the framework and how auditors can certify that specific chains of delegation and refinement decisions comply with government regulations.
28|1-2||Anomaly-based network intrusion detection: Techniques, systems and challenges|The Internet and computer networks are exposed to an increasing number of security threats. With new types of attacks appearing continually, developing flexible and adaptive security oriented approaches is a severe challenge. In this context, anomaly-based network intrusion detection techniques are a valuable technology to protect target systems and networks against malicious activities. However, despite the variety of such methods described in the literature in recent years, security tools incorporating anomaly detection functionalities are just starting to appear, and several important problems remain to be solved. This paper begins with a review of the most well-known anomaly-based intrusion detection techniques. Then, available platforms, systems under development and research projects in the area are presented. Finally, we outline the main challenges to be dealt with for the wide scale deployment of anomaly-based intrusion detectors, with special emphasis on assessment issues.
28|1-2||Huffman-based join-exit-tree scheme for contributory key management|Time efficiency in key establishment and rekeying is one of the major problems contributory key management schemes strive to address. Some schemes have been put forward to improve time efficiencies of key establishment and key update, yet they did not consider the scenario where users have varying costs and capabilities. Although conference key tree based on Huffman coding has been proposed to obtain minimum average cost on key establishment considering users' differences, it did not give the efficient key updating algorithm. We propose a Huffman-based join-exit-tree (HJET) scheme to minimizing the average key establishment time and reducing the key rekeying time for join/departure events. HJET scheme separates users into subgroups according to users' locations, designs the key tree of each subgroup using Huffman coding, and lets the combined weights locate in a higher place of the Huffman tree to minimize the key establishment time. To reduce the key rekeying cost, join tree and exit tree are adopted and served as the temporary buffers for joining and leaving users. Performance analysis and simulation results demonstrate that HJET is efficient in key establishment and update, and achieves the asymptotic time cost of O(1) for join event and nearly O(1)for leave events when group dynamics are known a priori.
28|1-2||RAID-RMS: A fault tolerant stripped mirroring RAID architecture for distributed systems|Disk arrays, or RAIDs, have become the solution to increase the capacity, bandwidth and reliability of most storage systems. In spite of its high redundancy level, disk mirroring is a popular RAID paradigm, because replicating data also doubles the bandwidth available for processing read requests, improves the reliability and achieves fault tolerance. In this paper, we present a new RAID architecture called RAID-RMS in which a special hybrid mechanism is used to map the data blocks to the cluster. The main idea behind the proposed algorithm is to combine the data block striping and disk mirroring technique with a data block rotation. The resulting architecture improves the parallelism reliability and efficiency of the RAID array. We show that the proposed architecture is able to serve many more disk requests compared to the other mirroring-based architectures. We also argue that a more balanced disk load is attained by the given architecture, especially when there are some disk failures.
28|1-2||User perceptions of security, convenience and usability for ebanking authentication tokens|This research compared three different two-factor methods of eBanking authentication. Three devices employing incremental security layers in the generation of one time passcodes (OTPs) were compared in a repeated-measures, controlled experiment with 50 eBanking customers. Attitudes towards usability and usage logs were taken for each experience. Comparisons of the devices in terms of overall quality, security and convenience as perceived by participants were also recorded. There were significant differences between all three methods in terms of usability measures, perceived quality, convenience and security ratings – with the perceived security ratings following a reverse order to the other measures. Almost two thirds of the participant sample chose the device they perceived the least secure as their preference. Participants were asked to use their preferred method again and tended to find their chosen device more usable. This research illustrates the usability-security trade off, where convenience, quality and usability are sacrificed when increasing layers of security are required. In their preferences, customers were driven by their attitudes towards usability and convenience rather than their perceptions of security.
28|1-2||Why Johnny can't surf (safely)? Attacks and defenses for web users|In their seminal article “Why Johnny Can't Encrypt” [Whitten A, Tygar JD. Why Johnny can't encrypt: a usability case study of PGP 5.0. In: Proceedings of the eighth USENIX security symposium; August 1999.], Whitten and Tygar showed that usability weaknesses of encryption software may result in failure to protect users, in spite of good cryptography. A similar situation happens, on a huge scale, on the Web: the widely deployed SSL/TLS protocols provide good cryptography, yet there is a growing amount of successful attacks on web users, causing massive damages. In this article, we focus on password theft via fake websites, to which we refer as phishing. We believe that phishing is currently the most severe threat facing web users.
28|1-2||Recognition of electro-magnetic leakage information from computer radiation with SVM|This paper focuses on the far-field reception of electromagnetic (EM) radiation and the recognition of letters recovered from the EM leakage. EM radiation captured by a wideband antenna is strengthened by a pre-manipulation system with amplifiers and filters, and the useful information is extracted. After being recovered from the EM radiation with signal processing method, the text image is further recognized by support vector machine (SVM) algorithm. Here, a two-layered SVM network with 60 SVMs is constructed to recognize the letters. In the process, the text image is cut apart into single letters to meet the input requirement of letter-based SVMs. To handle those exceptions of stroke connections, an interactive method and an automatic method are proposed. The received text image is usually obtained in low resolution with characteristics of large stroke distortion, font variation and variable size. In our two applications, however, the recognition accuracy reached 99.2% for the larger font size texts and 96.4% for the smaller size texts. From this, we may draw a conclusion that the proposed SVMs network works well in recognizing textual information and emphasize the potential risk of information leakage for computer system.
28|1-2||Securing communication using function extraction technology for malicious code behavior analysis|Since computer hardware and Internet is growing so fast today, security threats of malicious executable code are getting more serious. Basically, malicious executable codes are categorized into three kinds – virus, Trojan Horse, and worm. Current anti-virus products cannot detect all the malicious codes, especially for those unseen, polymorphism malicious executable codes. The newly developed virus will create the damages before it has been found and updated in database. The basic idea of the proposed system is, it will analyze the behavior of the malicious codes and based on the behavior signature of the malicious code content filtering mechanism will be used to filter out contents, so that, the system will be secured from the future communication processes. The behavior of the code is analyzed using the function extraction technology. The function extraction technology will replace the function codes into algebraic expressions. Based on the behavior of the malicious codes, it will be categorized into different kinds of malicious codes. The detected malicious code will be prevented from execution. Based on the type of malicious code, appropriate security mechanism will be used for further communication.
28|1-2||Keystroke dynamics-based authentication for mobile devices|Recently, mobile devices are used in financial applications such as banking and stock trading. However, unlike desktops and notebook computers, a 4-digit personal identification number (PIN) is often adopted as the only security mechanism for mobile devices. Because of their limited length, PINs are vulnerable to shoulder surfing and systematic trial-and-error attacks. This paper reports the effectiveness of user authentication using keystroke dynamics-based authentication (KDA) on mobile devices. We found that a KDA system can be effective for mobile devices in terms of authentication accuracy. Use of artificial rhythms leads to even better authentication performance.
28|1-2||An oblique-matrix technique for data integrity assurance|
28|1-2||Digital multisignature on the generalized conic curve over Zn|This paper proposes a digital multi-signature scheme on the generalized conic curve over Zn. The generalized conic curve Rn (a, b, c) over residue class ring Zn and its group structure are presented. The paper gives the representation of the order and base point on Rn (a, b, c), and introduces the representation of operation by parameters to simplify its calculation. The scheme security is based on the factoring and discrete logarithms simultaneously. This scheme is easy to accomplish for convenient embedding plaintext, computing element order and points in the conic curve. In addition, it can speed up the inverse operation and resist Pohlig–Hellman's attack and Wiener's attack.
28|1-2||Guide for Authors.|
28|3-4|http://www.sciencedirect.com/science/journal/01674048/28/3-4|Contents|
28|3-4||Understanding the limitations of S/MIME digital signatures for e-mails: A GUI based approach|S/MIME (Secure/Multipurpose Internet Mail Extensions) is a well-known standard for secure e-mail exchange. S/MIME builds its identity management on e-mail addresses, rather than real names. This fact may sometimes cause sending a signed e-mail with a bogus name on it. Moreover, header information of a signed e-mail message, such as subject and name, can be altered without affecting the verifiability of the signature. This paper spots the details of such problems of S/MIME and discusses some solutions from both developer and user points of view. Moreover, GUI considerations about these problems are also analyzed in this paper. An ideal GUI is modeled and developed.
28|3-4||SDriver: Location-specific signatures prevent SQL injection attacks|SQL injection attacks involve the construction of application input data that will result in the execution of malicious SQL statements. Many web applications are prone to SQL injection attacks. This paper proposes a novel methodology of preventing this kind of attacks by placing a secure database driver between the application and its underlying relational database management system. To detect an attack, the driver uses stripped-down SQL queries and stack traces to create SQL statement signatures that are then used to distinguish between injected and legitimate queries. The driver depends neither on the application nor on the RDBMS and can be easily retrofitted to any system. We have developed a tool, SDriver, that implements our technique and used it on several web applications with positive results.
28|3-4||From desktop to mobile: Examining the security experience|The use of mobile devices is becoming more commonplace, with data regularly able to make the transition from desktop systems to pocket and handheld devices such as smartphones and PDAs. However, although these devices may consequently contain or manipulate the same data, their security capabilities are not as mature as those offered in fully-fledged desktop operating systems. This paper explores the availability of security mechanisms from the perspective of a user who is security-aware in the desktop environment and wishes to consider utilising similar protection in a mobile context. Key issues of concern are whether analogous functionality can be found, and if so, whether it is offered in a manner that parallels the desktop experience (i.e. to ensure understanding and usability). The discussion is supported by an examination of the Windows XP and Windows Mobile environments, with specific consideration given to the facilities available for user authentication, secure connectivity, and content protection on the devices. It is concluded that although security aspects receive some attention, the provided means generally suffer from usability issues or limitations that would prevent a user from achieving the same level of protection that they might enjoy in the desktop environment.
28|3-4||An ID-based remote mutual authentication with key agreement scheme for mobile devices on elliptic curve cryptosystem|Recently, remote user authentication schemes are implemented on elliptic curve cryptosystem (ECC) to reduce the computation loads for mobile devices. However, most remote user authentication schemes on ECC are based on public-key cryptosystem, in which the public key in the system requires the associated certificate to prove its validity. Thus, the user needs to perform additional computations to verify the certificate in these schemes. In addition, we find these schemes do not provide mutual authentication or a session key agreement between the user and the remote server. Therefore, we propose an ID-based remote mutual authentication with key agreement scheme on ECC in this paper. Based upon the ID-based concept, the proposed scheme does not require public keys for users such that the additional computations for certificates can be reduced. Moreover, the proposed scheme not only provides mutual authentication but also supports a session key agreement between the user and the server. Compared with the related works, the proposed scheme is more efficient and practical for mobile devices.
28|3-4||Detecting rogue access points using client-side bottleneck bandwidth analysis|A rogue access point (AP) is an unauthorized AP plugged into a network. This poses a serious security threat. To detect an AP, a network manager traditionally takes the electric wave sensor across an entire protected place. This task is very labor-intensive and inefficient. This study presents a new AP detection method without extra hardware or hard work. This new method determines whether the network packets of an IP address are routed from APs, according to client-side bottleneck bandwidth. The network manager can perform his job from his office by monitoring the packets passing through the core switch. The accuracies remain above 99% when the parameter, sliding window size, of the proposed algorithm is larger than 20, according to experimental results. The proposed method effectively reduces the network manager's workload, and increases network security.
28|3-4||An incremental frequent structure mining framework for real-time alert correlation|With the large volume of alerts produced by low-level detectors, management of intrusion alerts is becoming more challenging. Manual analysis of a large number of raw alerts is both time consuming and labor intensive. Alert Correlation addresses this issue by finding similarity and causality relationships between raw alerts to provide a condensed, yet more meaningful view of the network from the intrusion standpoint. While some efforts have been made in the literature by researchers to find the relationships between alerts automatically, not much attention has been given to the issue of real-time correlation of alerts. Previous learning-based approaches either fail to cope with a large number of generated alerts in a large-scale network or do not address the problem of concept drift directly.
28|3-4||Preventing massive automated access to web resources|Automated web tools are used to achieve a wide range of different tasks, some of which are legal activities, whilst others are considered attacks to the security and data integrity of online services. Effective solutions to counter the threat represented by such programs are therefore required. In this work, we present MosaHIP, a Mosaic-based Human Interactive Proof (HIP), which is able to prevent massive automated access to web resources. Properties of the proposed solution grant an improved security over usual text-based and image-based HIPs, whereas the user-friendliness of the system alleviates the user from the discomfort of typing any text before accessing to a web content. Experimental evidence of the effectiveness of the proposed technique is given by submitting our system to a series of tests simulating possible bot attacks.
28|3-4||Information security: The moving target|Information security has evolved from addressing minor and harmless security breaches to managing those with a huge impact on organisations' economic growth. This paper investigates the evolution of information security; where it came from, where it is today and the direction in which it is moving. It is argued that information security is not about looking at the past in anger of an attack once faced; neither is it about looking at the present in fear of being attacked; nor about looking at the future with uncertainty about what might befall us. The message is that organisations and individuals must be alert at all times. Research conducted for this paper explored literature on past security issues to set the scene. This is followed by the assessment and analysis of information security publications in conjunction with surveys conducted in industry. Results obtained are compared and analysed, enabling the development of a comprehensive view regarding the current status of the information security landscape. Furthermore, this paper also highlights critical information security issues that are being overlooked or not being addressed by research efforts currently undertaken. New research efforts are required that minimise the gap between regulatory issues and technical implementations.
28|3-4||Reliable and fully distributed trust model for mobile ad hoc networks|A mobile ad hoc network (MANET) is a wireless communication network which does not rely on a pre-existing infrastructure or any centralized management. Securing the exchanges in MANETs is compulsory to guarantee a widespread development of services for this kind of networks. The deployment of any security policy requires the definition of a trust model that defines who trusts who and how. Our work aims to provide a fully distributed trust model for mobile ad hoc networks. In this paper, we propose a fully distributed public key certificate management system based on trust graphs and threshold cryptography. It permits users to issue public key certificates, and to perform authentication via certificates' chains without any centralized management or trusted authorities. Moreover, thanks to the use of threshold cryptography; our system resists against false public keys certification. We perform an overall evaluation of our proposed approach through simulations. The results indicate out performance of our approach while providing effective security.
28|3-4||How significant is human error as a cause of privacy breaches? An empirical study and a framework for error management|Privacy breaches and their regulatory implications have attracted corporate attention in recent times. An often overlooked cause of privacy breaches is human error. In this study, we first apply a model based on the widely accepted GEMS error typology to analyze publicly reported privacy breach incidents within the U.S. Then, based on an examination of the causes of the reported privacy breach incidents, we propose a defense-in-depth solution strategy founded on error avoidance, error interception, and error correction. Finally, we illustrate the application of the proposed strategy to managing human error in the case of the two leading causes of privacy breach incidents. This study finds that mistakes in the information processing stage constitute the most cases of human error-related privacy breach incidents, clearly highlighting the need for effective policies and their enforcement in organizations.
28|3-4||Design, implementation and analysis of hardware efficient stream ciphers using LFSR based hash functions|Design and implementation of hardware efficient stream ciphers using hash functions and analysis of their periodicity and security are presented in this paper. The hash generation circuits used for the design and development of stream ciphers are low power, low hardware complexity Linear Feedback Shift Register (LFSR) based circuits. One stream cipher design uses LFSR based Toeplitz hash generation circuit together with LFSR keystream generator circuit, while the other design combines LFSR based filter generator circuit with LFSR based polynomial modular division circuit. Both designs possess good security and periodicity properties for the keystreams generated. The developed circuits can compete with the most popular classic LFSR based stream ciphers in hardware complexity at the same time providing additional advantage that the same circuit can be used for hash generation.
28|3-4||IFIP SEC 2009 Advert|
28|5|http://www.sciencedirect.com/science/journal/01674048/28/5|Contents|
28|5||Editorial|
28|5||Multi-dimensional credentialing using veiled certificates: Protecting privacy in the face of regulatory reporting requirements|Traditional certificates are designed to establish and document characteristics belonging to a specific individual, be it an identification number (i.e., social security number, driver's license number), a level of achievement (i.e., college degree, license to practice a profession), or membership status (i.e., company ID, trade union card). The digital certificate extends this concept into the electronic world, identifying and linking the certificate holder to a public encryption key that is subsequently used as a means of identification. Current identity certificates provide unique identification and tracking, however it is exactly these characteristics that have led to concerns over identity theft and privacy of personal information. The veiled certificate introduced in this paper addresses these issues by providing means of linking certificates from multiple certifying authorities while masking the user's identity from non-authorized individuals and satisfying the regulatory need of unique, explicit identification. With the ability to be implemented within existing X.509 standards, veiled certification extends traditional digital certificates with features useful in combating identity theft and invasion of privacy.
28|5||Towards secure dynamic collaborations with group-based RBAC model|Role-Based Access Control (RBAC) has become a popular technique for security purposes with increasing accessibility of information and data, especially in large-scale enterprise environments. However, authorization management in dynamic and ad-hoc collaborations between different groups or domains in these environments is still an unresolved problem. Traditional RBAC models cannot solve this problem because they cannot support security policy composition from different groups, and lack efficient administrative models for dynamic collaborations. In this paper, we propose a group-based RBAC model (GB-RBAC) for secure collaborations which is based on RBAC96 and extended with group concept to capture dynamic users and permissions. We propose a decentralized security administrative model for GB-RBAC to address the management issues of RBAC in collaborations. As a unique property, our model supports two levels of authorization management: global or system level management by system administrators and local or group level management by group administrators. In this way, our model implements the principles of management autonomy and separation of duty (SoD) in security administrations. We apply our model for authorization management in collaborations by introducing the concept of virtual group. A virtual group is built for a collaboration between multi-groups, where all members build trust relation within the group and are authorized to join and perform operations for the collaborative work. Compared with existing work, our model supports dynamic and ad-hoc collaborations in large-scale systems with the properties of controllable, decentralized, and fine-grained security management.
28|5||Fast detection and visualization of network attacks on parallel coordinates|This article presents what we call the parallel coordinate attack visualization (PCAV) for detecting unknown large-scale Internet attacks including Internet worms, DDoS attacks and network scanning activities. PCAV displays network traffic on the plane of parallel coordinates using the flow information such as the source IP address, destination IP address, destination port and the average packet length in a flow. The parameters are used to draw each flow as a connected line on the plane, where a group of polygonal lines form a particular shape in case of attack. From the observation that each attack type of significance forms a unique pattern, we develop nine signatures and their detection mechanism based on an efficient hashing algorithm. Using the graphical signatures, PCAV can quickly detect new attacks and enable network administrators to intuitively recognize and respond to the attacks. Compared with existing visualization works, PCAV can handle hyper-dimensions, i.e., can visualize more than 3 parameters if necessary, which significantly reduces false positives. As a consequence, Internet worms are more precisely detectable by machine and more easily recognizable by human. Another strength of PCAV is handling flows instead of packets. Per-flow visualization greatly reduces the processing time and further provides compatibility with legacy routers which export flow information, e.g., as NetFlow does in Cisco routers. We demonstrate the effectiveness of PCAV using real-life Internet traffic traces. The PCAV program is publicly available.
28|5||Secure multiparty payment with an intermediary entity|During the last years, many secure electronic payment solutions have been proposed but most of them are focused on the traditional two-party business models with a customer and just one provider. In this paper we propose a new secure multiparty payment model with an intermediary, who helps the customer to make purchases and payments with many providers simultaneously. In our secure infrastructure it is assumed that the intermediary does not need to be a trusted entity (it does not need to be a TTP). One of the most important issues of this contribution is the Intermediary-3D: we propose a simple adaptation of the 3D Secure™ payment protocol in order to maintain the 3D Secure™ working modes but offering the possibility of making multiple secure payments through an intermediary. By means of this slight adaptation, our model avoids the provider's enrolment process in a centralized system (e.g. Visa Domain) and it makes more robust and secure the multipayment scenarios, as well as, it favors its deployment in global networks like Internet.
28|5||A real-time network intrusion detection system for large-scale attacks based on an incremental mining approach|None of the previously proposed Network Intrusion Detection Systems (NIDSs), which are subject to fuzzy association rules, can meet real-time requirements because they all apply static mining approaches. This study proposed a real-time NIDS with incremental mining for fuzzy association rules. By consistently comparing the two rule sets, one mined from online packets and the other mined from training attack-free packets, the proposed system can render a decision every 2 seconds. Thus, compared with traditional static mining approaches, the proposed system can greatly improve efficiency from offline detection to real-time online detection. Since the proposed system derives features from packet headers only, like the previous works based on fuzzy association rules, large-scale attack types are focused. Many DoS attacks were experimented in this study. Experiments were performed to demonstrate the excellent effectiveness and efficiency of the proposed system. The system may not cause false alarms because normal programs supposedly would not generate enough mal-formatted packets, or packets that violate normal network protocols.
28|5||Specifying authentication using signal events in CSP|The formal analysis of cryptographic protocols has developed into a comprehensive body of knowledge, building on a wide variety of formalisms and treating a diverse range of security properties, foremost of which is authentication. The formal specification of authentication has long been a subject of examination. In this paper, we discuss the use of correspondence to formally specify authentication and focus on Schneider's use of signal events in the process algebra Communicating Sequential Processes (CSP) to specify authentication. The purpose of this effort is to strengthen this formalism further. We develop a formal structure for these events and use them to specify a general authentication property. We then develop specifications for recentness and injectivity as sub-properties, and use them to refine authentication further. Finally, we use signal events to specify a range of authentication definitions and protocol examples to clarify their use and make explicit related theoretical issues. Our work is motivated by the desire to effectively analyse and express security properties in formal terms, so as to make them precise and clear.
28|6|http://www.sciencedirect.com/science/journal/01674048/28/6|Contents|
28|6||Editorial|
28|6||A personal mobile DRM manager for smartphones|In this paper we report on our experience in building the experimental Personal Digital Rights Manager for Motorola smartphones, an industry first.
28|6||New aspect-oriented constructs for security hardening concerns|In this paper, we present new pointcuts and primitives to Aspect-Oriented Programming (AOP) languages that are needed for systematic hardening of security concerns. The two proposed pointcuts allow to identify particular join points in a program's control-flow graph (CFG). The first one is the GAFlow, Closest Guaranteed Ancestor, which returns the closest ancestor join point to the pointcuts of interest that is on all their runtime paths. The second one is the GDFlow, Closest Guaranteed Descendant, which returns the closest child join point that can be reached by all paths starting from the pointcut of interest. The two proposed primitives are called ExportParameter and ImportParameter and are used to pass parameters between two pointcuts. They allow to analyze a program's call graph in order to determine how to change function signatures for passing the parameters associated with a given security hardening. We find these pointcuts and primitives to be necessary because they are needed to perform many security hardening practices and, to the best of our knowledge, none of the existing ones can provide their functionalities. Moreover, we show the viability and correctness of the proposed pointcuts and primitives by elaborating and implementing their algorithms and presenting the result of explanatory case studies.
28|6||RIP â A robust IP access architecture|This research is of the view that only tightly coordinated work among security components as we know them today including firewalls, traffic analysis modules, intrusion detection systems, antivirus remediation systems, etc., is likely to take us closer to a more effective solution against some security threats. A structured Robust IP (RIP) access architecture is described and its components are analyzed through the use of a proof of concept testbed. Examples of the use of RIP and the heuristics it implements are evaluated. We also compare RIP performance to existing work. We show that there are currently some tradeoffs that need to be made between accuracy and responsiveness. We believe that this collaborative communication style between the components represents a significant step in the direction of self-defending networks and innovation in the area.
28|6||A survey of signature based methods for financial fraud detection|Fraud detection mechanisms support the successful identification of fraudulent system transactions performed through security flaws within deployed technology frameworks while maintaining optimal levels of service delivery and a minimal numbers of false alarms. Knowledge discovery techniques have been widely applied in fraud detection for data analysis and training of supervised learning algorithms to support the extraction of fraudulent account behaviour within static data sets. Escalating costs associated with fraud however have continued to drive the migration towards increasingly proactive methods of fraud detection, to support the real-time screening of transactional data and detection of ambiguous user behaviour prior to transaction completion. This shift in data processing from post to pre data storage significantly reduces the available time within which to evaluate newly arriving system requests and produce an accurate fraud decision, demanding increasingly robust and intelligent user profiling technologies to support advanced fraud detection. This paper provides a comprehensive survey of existing research into account signatures, an innovative account profiling technology which maintains a statistical representation of normal account usage for rapid recalculation in real-time. Fraud detection architectures, processing models and applications to date are critically examined and evaluated with respect to their proactive capabilities for detection of fraud within streaming financial data. Discussion is also presented on challenges which remain within the proactive profiling of account behaviour and future research directions within the signature domain.
28|6||A robust software watermarking for copyright protection|This paper advocates protecting software copyright through hiding watermarks in various data structures used by the code, e.g., B+-trees, R-trees, linked lists, etc. Prior proposals hide the watermarks in dummy data structures, e.g., linked lists and graphs that are created, solely for this reason, during the execution of the hosting software. This makes them vulnerable to subtractive attacks, because the attacker can remove the dummy data structures without altering the functionality or the semantic of the software program. We argue that hiding watermarks in one or more data structures that are used by the program would make the watermark more robust because disturbing the watermark would affect the semantic and the functionality of the underlying software. The challenge is that the insertion of the watermark should have a minimal effect on the operations and performance of the data structure.
28|6||Reducing threats from flawed security APIs: The banking PIN case|Despite best efforts from security API designers, flaws are often found in widely deployed security APIs. Even APIs with a formal proof of security may not guarantee absolute security when used in a real-world device or application. In parallel to spending research efforts to improve security of these APIs, we argue that it may be worthwhile to explore design criteria that would reduce the impact of an API exploit, assuming flaws cannot completely be removed from security APIs. We use such a design philosophy in dealing with PIN cracking attacks on financial PIN processing APIs; several of these attacks have been reported in the last few years, e.g., Berkman and Ostrovsky (FC 2007), Bond (CHES 2001). Our solution is called salted-PIN: a randomly generated salt value of adequate length (e.g., 128 bits) is stored on a bank card in plaintext, and in an encrypted form at a verification facility under a bank-chosen salt key. Instead of sending the regular user PIN, salted-PIN requires an ATM to generate a Transport Final PIN from a user PIN, account number, and the salt value (stored on the bank card) through, e.g., a pseudo-random function. We explore different attacks on this solution, and propose variants of salted-PIN that can protect against known attacks. Depending on the solution variation, attacks at a malicious intermediate switch now may only reveal the Transport Final PIN; both the user PIN and salt value remain beyond the reach of an attacker's switch. Salted-PIN requires modifications to service points (e.g., ATM, point-of-sale), issuer/verification facilities, and bank cards; however, changes to intermediate switches are not required.
28|6||A formal framework for real-time information flow analysis|We view Multi-Level Secure (MLS) real-time systems as systems in which MLS real-time tasks are scheduled and execute, according to a scheduling algorithm employed by the system. From this perspective, we develop a general trace-based framework that can carry out a covert-timing channel analysis of a real-time system. In addition, we propose a set of covert-timing channel free policies: If a system satisfies one of our proposed security policies, we demonstrated that the system can achieve a certain level of real-time information flow security. Finally, we compare the relative strength of the proposed covert-timing channel free security policies and analyze whether each security policy can be regarded as a property (a set of execution sequences).
28|6||Providing true end-to-end security in converged voice over IP infrastructures|Voice over Internet Protocol (VoIP) is the future for voice communication and, by using a unique IP infrastructure as the common transport platform, it brings invaluable benefits such as deployment cost reduction, ease of management, ubiquitous coverage and convergence of data and voice together. On the other side, VoIP introduces new security vulnerabilities, since it comes with completely different operational and security settings than the old telephone network: the physical location of clients is not fixed and great flexibility is required to provide enhanced mobile services. Furthermore, the integration with wireless LANs, with their inherent security weaknesses, introduces the need of new security features: the payloads of voice packets should be protected during conversations and no-replay as well as user authentication must be ensured on and end-to-end basis. The above concerns are actually the major barrier that may prevent the wide deployment of VoIP technologies, and coping with them is a truly challenging task. Consequently, we developed a novel hybrid framework for enhanced end-to-end security in the new generation SIP-empowered VoIP environments, based on the introduction of proven technologies such as digital signatures and efficient streamline encryption to enforce calling party identification, privacy, no-replay and non-repudiation throughout the whole IP Telephony system. All the security mechanisms used have been carefully chosen so that no systematic method is known to break the framework in realistic times and the overall voice quality will not be affected.
28|6||Probabilistic model checking for the quantification of DoS security threats|Secure authentication features of communication and electronic commerce protocols involve computationally expensive and memory intensive cryptographic operations that have the potential to be turned into denial-of-service (DoS) exploits. Recent proposals attempt to improve DoS resistance by implementing a trade-off between the resources required for the potential victim(s) with the resources used by a prospective attacker. Such improvements have been proposed for the Internet Key Exchange (IKE), the Just Fast Keying (JFK) key agreement protocol and the Secure Sockets Layer (SSL/TLS) protocol. In present article, we introduce probabilistic model checking as an efficient tool-assisted approach for systematically quantifying DoS security threats. We model a security protocol with a fixed network topology using probabilistic specifications for the protocol participants. We attach into the protocol model, a probabilistic attacker model which performs DoS related actions with assigned cost values. The costs for the protocol participants and the attacker reflect the level of some resource expenditure (memory, processing capacity or communication bandwidth) for the associated actions. From the developed model we obtain a Discrete Time Markov Chain (DTMC) via property preserving discrete-time semantics. The DTMC model is verified using the PRISM model checker that produces probabilistic estimates for the analyzed DoS threat. In this way, it is possible to evaluate the level of resource expenditure for the attacker, beyond which the likelihood of widespread attack is reduced and subsequently to compare alternative design considerations for optimal resistance to the analyzed DoS threat. Our approach is validated through the analysis of the Host Identity Protocol (HIP). The HIP base-exchange is seen as a cryptographic key-exchange protocol with special features related to DoS protection. We analyze a serious DoS threat, for which we provide probabilistic estimates, as well as results for the associated attacker and participants' costs.
28|6||Building lightweight intrusion detection system using wrapper-based feature selection mechanisms|Intrusion Detection System (IDS) is an important and necessary component in ensuring network security and protecting network resources and network infrastructures. How to build a lightweight IDS is a hot topic in network security. Moreover, feature selection is a classic research topic in data mining and it has attracted much interest from researchers in many fields such as network security, pattern recognition and data mining. In this paper, we effectively introduced feature selection methods to intrusion detection domain. We propose a wrapper-based feature selection algorithm aiming at building lightweight intrusion detection system by using modified random mutation hill climbing (RMHC) as search strategy to specify a candidate subset for evaluation, as well as using modified linear Support Vector Machines (SVMs) iterative procedure as wrapper approach to obtain the optimum feature subset. We verify the effectiveness and the feasibility of our feature selection algorithm by several experiments on KDD Cup 1999 intrusion detection dataset. The experimental results strongly show that our approach is not only able to speed up the process of selecting important features but also to yield high detection rates. Furthermore, our experimental results indicate that intrusion detection system with feature selection algorithm has better performance than that without feature selection algorithm both in detection performance and computational cost.
28|6||The information security digital divide between information security managers and users|Empirical findings from surveys and in-depth interviews with information security managers and users indicate that a digital divide exists between these groups in terms of their views on and experience of information security practices. Information security professionals mainly regard users as an information security threat, whereas users believe themselves that they are an untapped resource for security work. The limited interaction between users and information security managers results in a lack of understanding for the other's point of view. These divergent views on and interpretations of information security mean that managers tend to base their practical method on unrealistic assumptions, resulting in management approaches that are poorly aligned with the dynamics of the users' working day.
28|6||IFIP TC11 - Aims and Scope|
28|6||IFIP Technical Committee|
28|7|http://www.sciencedirect.com/science/journal/01674048/28/7|Contents|
28|7||Editorial|
28|7||Information security policy: An organizational-level process model|To protect information systems from increasing levels of cyber threats, organizations are compelled to institute security programs. Because information security policies are a necessary foundation of organizational security programs, there exists a need for scholarly contributions in this important area. Using a methodology involving qualitative techniques, we develop an information security policy process model based on responses from a sample of certified information security professionals. As the primary contribution of this research study, the proposed model illustrates a general yet comprehensive policy process in a distinctive form not found in existing professional standards or academic publications. This study's model goes beyond the models illustrated in the literature by depicting a larger organizational context that includes key external and internal influences that can materially impact organizational processes. The model that evolved from the data in this research reflects the recommended practices of our sample of certified professionals, thus providing a practical representation of an information security policy process for modern organizations. Before offering our concluding comments, we compare the results of the study with the literature in both theory and practice and also discuss limitations of the study. To the benefit of the practitioner and research communities alike, the model in this study offers a step forward, as well as an opportunity for making further advancements in the increasingly critical area of information security policy.
28|7||Human and organizational factors in computer and information security: Pathways to vulnerabilities|The purpose of this study was to identify and describe how human and organizational factors may be related to technical computer and information security (CIS) vulnerabilities. A qualitative study of CIS experts was performed, which consisted of 2, 5-member focus groups sessions. The participants in the focus groups each produced a causal network analysis of human and organizational factors pathways to types of CIS vulnerabilities. Findings suggested that human and organizational factors play a significant role in the development of CIS vulnerabilities and emphasized the relationship complexities among human and organizational factors. The factors were categorized into 9 areas: external influences, human error, management, organization, performance and resource management, policy issues, technology, and training. Security practitioners and management should be aware of the multifarious roles of human and organizational factors and CIS vulnerabilities and that CIS vulnerabilities are not the sole result of a technological problem or programming mistake. The design and management of CIS systems need an integrative, multi-layered approach to improve CIS performance (suggestions for analysis provided).
28|7||Risk profiles and distributed risk assessment|Risk assessment is concerned with discovering threat paths between potential attackers and critical assets, and is generally carried out during a system's design and then at fixed intervals during its operational life. However, the currency of such analysis is rapidly eroded by system changes; in dynamic systems these include the need to support ad-hoc collaboration, and dynamic connectivity between the system's components. This paper resolves these problems by showing how risks can be assessed incrementally as a system changes, using risk profiles, which characterize the risk to a system from subverted components. We formally define risk profiles, and show that their calculation can be fully distributed; each component is able to compute its own profile from neighbouring information. We further show that profiles converge to the same risks as systematic threat path enumeration, that changes in risk are efficiently propagated throughout a distributed system, and that the distributed computation provides a criterion for when the security consequences of a policy change are local to a component, or will propagate into the wider system. Risk profiles have the potential to supplement conventional risk assessments with useful new metrics, maintain accurate continuous assessment of risks in dynamic distributed systems, link a risk assessment to the wider environment of the system, and evaluate defence-in-depth strategies.
28|7||Adapting usage control as a deterrent to address the inadequacies of access controls|Access controls are difficult to implement and evidently deficient under certain conditions. Traditional controls offer no protection for unclassified information, such as a telephone list of employees that is unrestricted, yet available only to members of the company. On the opposing side of the continuum, organizations such as hospitals that manage highly sensitive information require stricter access control measures. Yet, traditional access control may well have inadvertent consequences in such a context. Often, in unpredictable circumstances, users that are denied access could have prevented a calamity had they been allowed access. It has been proposed that controls such as auditing and accountability policies be enforced to deter rather than prevent unauthorized usage. In dynamic environments preconfigured access control policies may change dramatically depending on the context. Moreover, the cost of implementing and maintaining complex preconfigured access control policies sometimes far outweighs the benefits. This paper considers an adaptation of usage control as a proactive means of deterrence control to protect information that cannot be adequately or reasonably protected by access control.
28|7||Security threats scenarios in trust and reputation models for distributed systems|Trust and reputation management over distributed systems has been proposed in the last few years as a novel and accurate way of dealing with some security deficiencies which are inherent to those environments. Thus, many models and theories have been developed in order to effective and accurately manage trust and reputation in those communities. Nevertheless, very few of them take into consideration all the possible security threats that can compromise the system. In this paper, we present some of the most important and critical security threats that could be applied in a trust and reputation scheme. We will describe and analyze each of those threats and propose some recommendations to face them when developing a new trust and reputation mechanism. We will also study how some trust and reputation models solve them. This work expects to be a reference guide when designing secure trust and reputation models.
28|7||DFANS: A highly efficient strategy for automated trust negotiation|Automated trust negotiation (ATN) is an approach establishing mutual trust between strangers wishing to share resources or conduct business by gradually requesting and disclosing digitally signed credentials. The digital credentials themselves are usually sensitive, so they have corresponding access control policies to control their disclosure. Therefore, an ATN strategy must be adopted to determine the search for a successful negotiation based on the access control policies. Previously proposed negotiation strategies are either not complete, disclosing irrelevant credentials, or not efficient enough. In this paper, we propose a novel ATN strategy, that is, Deterministic Finite Automaton Negotiation Strategy (DFANS). DFANS is complete and ensures that no irrelevant credentials are disclosed during the negotiation. Furthermore, DFANS is highly efficient. In the worst case, its communication complexity is O(n), where n is the total number of credentials requested, and its computational complexity is O(m) when not involving the cyclic dependencies, where m is the total size of the both sides' policies looked up during the negotiation. When cyclic dependencies exist, a reasonable additional cost of running OSBE protocol that is a provably secure and quite efficient scheme will be added to the computational cost of DFANS to guarantee the negotiation success whenever possible.
28|7||What the heck is this application doing? â A security-by-contract architecture for pervasive services|Future pervasive environments are characterized by non-fixed architectures made of users and ubiquitous computers. They will be shaped by pervasive client downloads, i.e. new (untrusted) applications will be dynamically downloaded to make a better use of the computational power available in the ubiquitous computing environment.
28|7||Utilizing bloom filters for detecting flooding attacks against SIP based services|Any application or service utilizing the Internet is exposed to both general Internet attacks and other specific ones. Most of the times the latter are exploiting a vulnerability or misconfiguration in the provided service and/or in the utilized protocol itself. Consequently, the employment of critical services, like Voice over IP (VoIP) services, over the Internet is vulnerable to such attacks and, on top of that, they offer a field for new attacks or variations of existing ones. Among the various threats–attacks that a service provider should consider are the flooding attacks, at the signaling level, which are very similar to those against TCP servers but have emerged at the application level of the Internet architecture. This paper examines flooding attacks against VoIP architectures that employ the Session Initiation Protocol (SIP) as their signaling protocol. The focus is on the design and implementation of the appropriate detection method. Specifically, a bloom filter based monitor is presented and a new metric, named session distance, is introduced in order to provide an effective protection scheme against flooding attacks. The proposed scheme is evaluated through experimental test bed architecture under different scenarios. The results of the evaluation demonstrate that the required time to detect such an attack is negligible and also that the number of false alarms is close to zero.
28|7||Client-side cross-site scripting protection|Web applications are becoming the dominant way to provide access to online services. At the same time, web application vulnerabilities are being discovered and disclosed at an alarming rate. Web applications often make use of JavaScript code that is embedded into web pages to support dynamic client-side behavior. This script code is executed in the context of the user's web browser. To protect the user's environment from malicious JavaScript code, browsers use a sand-boxing mechanism that limits a script to access only resources associated with its origin site. Unfortunately, these security mechanisms fail if a user can be lured into downloading malicious JavaScript code from an intermediate, trusted site. In this case, the malicious script is granted full access to all resources (e.g., authentication tokens and cookies) that belong to the trusted site. Such attacks are called cross-site scripting (XSS) attacks.
28|7||Measuring IDS-estimated attack impacts for rational incident response: A decision theoretic approach|Intrusion detection system (IDS) plays a vital role in defending our cyberspace against attacks. Either misuse-based IDS or anomaly-based IDS, or their combinations, however, can only partially reflect the true system state due to excessive false alerts, low detection rate, and inaccurate incident diagnosis. An automated response component built upon IDS therefore must consider the stale and imperfect picture inferred from them and takes action accordingly.
28|7||Confidence in smart token proximity: Relay attacks revisited|Contactless and contact smart card systems use the physical constraints of the communication channel to implicitly prove the proximity of a token. These systems, however, are potentially vulnerable to an attack where the attacker relays communication between the reader and a token. Relay attacks are not new but are often not considered a major threat, like eavesdropping or skimming attacks, even though they arguably pose an equivalent security risk. In this paper we discuss the feasibility of implementing passive and active relay attacks against smart tokens and the possible security implications if an attacker succeeds. Finally, we evaluate the effectiveness of time-out constraints, distance bounding and the use of a additional verification techniques for making systems relay-resistant and explain the challenges still facing these mechanisms.
28|7||Defending passive worms in unstructured P2P networks based on healthy file dissemination|Propagation of passive worms in unstructured peer-to-peer (P2P) networks can result in significant damages and the loss of network security. This paper obtains the average delay for all peers in the entire transmitting process, and proposes a mathematical model for simulating unstructured P2P networks-based passive worms' propagation taking into account network throughput. According to the file popularity which follows the Zipf distribution, we propose a new healthy file dissemination-based defense strategy. Some parameters related to the propagation of passive worms are studied based on the proposed model. Finally, the simulation results verify the effectiveness of our model, which can provide an important guideline in the control of passive worms in unstructured P2P networks.
28|7||On the development of an internetwork-centric defense for scanning worms|Studies of worm outbreaks have found that the speed of worm propagation makes manual intervention ineffective. Consequently, many automated containment mechanisms have been proposed to contain worm outbreaks before they grow out of control. These containment systems, however, only provide protection for hosts within networks that implement them. Such a containment strategy requires complete participation to protect all vulnerable hosts. Moreover, collaborative containment systems, where participants share alert data, face a tension between resilience to false alerts and quick reaction to worm outbreaks.
28|7||A concise cost analysis of Internet malware|In this paper we present a cost model to analyze impacts of Internet malware in order to estimate the cost of incidents and risk caused by them. The model is useful in determining parameters needed to estimate recovery efficiency, probabilistic risk distributions, and cost of malware incidents. Many users tend to underestimate the cost of curiosity coming with stealth malware such as email-attachments, freeware/shareware, spyware (including keyloggers, password thieves, phishing-ware, network sniffers, stealth backdoors, and rootkits), popups, and peer-to-peer fileshares. We define two sets of functions to describe evolution of attacks and potential loss caused by malware, where the evolution functions analyze infection patterns, while the loss functions provide risk-impact analysis of failed systems. Due to a wide range of applications, such analyses have drawn the attention of many engineers and researchers. Analysis of malware propagation itself has little to contribute unless tied to analysis of system performance, economic loss, and risks.
28|7||Providing secure execution environments with a last line of defense against Trojan circuit attacks|Integrated circuits (ICs) are often produced in foundries that lack effective security controls. In these foundries, sophisticated attackers are able to insert malicious Trojan circuits that are easily hidden in the large, complex circuitry that comprises modern ICs. These so-called Trojan circuits are capable of launching attacks directly in hardware, or, more deviously, can facilitate software attacks. Current defense against Trojan circuits consists of statistical detection techniques to find such circuits before product deployment. The fact that statistical detection can result in false negatives raises the obvious questions: can attacks be detected post-deployment, and is secure execution nonetheless possible using chips with undetected Trojan circuits? In this paper we present the Secure Heartbeat And Dual-Encryption (SHADE) architecture, a compiler–hardware solution for detecting and preventing a subset of Trojan circuit attacks in deployed systems. Two layers of hardware encryption are combined with a heartbeat of off-chip accesses to provide a secure execution environment using untrusted hardware. The SHADE system is designed to complement pre-deployment detection techniques and to add a final, last-chance layer of security.
28|7||A new steganography algorithm based on color histograms for data embedding into raw video streams|Steganography, embedding secret data into unsuspected objects, has emerged as a significant sub-discipline of data-embedding methods. While mostly applied to still images in the past, it has become very popular for video streams recently. When steganographic methods are applied to digital video streams, the selection of target pixels, which are used to store the secret data, is especially crucial for an effective and successful-embedding process; if pixels are not selected carefully, undesired spatial and temporal perception problems occur in the stego-video. In this paper, two new steganographic algorithms are proposed utilizing similar histograms and dissimilar histograms. Both algorithms are based on selecting appropriate pixel approaches by focusing on perceptibility and capacity parameters of the cover video. When compared to traditional steganographic techniques, they not only result in improved temporal and spatial perception levels in the stego-video but also offer a relatively high data-embedding capacity.
28|7||Blind image steganalysis based on content independent statistical measures maximizing the specificity and sensitivity of the system|This paper reports the design principles and evaluation results of a new experimental universal, blind image steganalysing system. This system investigates the use of content independent statistical evidences left by the steganograms, as features for an image steganalyzer. The work is aimed at maximizing the sensitivity and specificity of the steganalyzer and to accomplish both security and system performance. A genetic-X-means classifier is constructed to realize the proposed model. For performance evaluation, a database composed of 5600 plain and stego images (generated by using seven different embedding schemes) was established. The results of our empirical experiment prove the vitality of the proposed scheme in detecting stego anomalies in images. In addition, the simulation results show that the effectiveness of steganalytic system can be enhanced by considering the content independent distortion measures and maximizing the sensitivity and specificity of the system.
28|7||A schema for protecting the integrity of databases|Unauthorized changes to databases can result in significant losses for organizations as well as individuals. Watermarking can be used to protect the integrity of databases against unauthorized alterations. Prior work focused on watermarking database tables or relations. Malicious alteration cannot be detected in all cases. In this paper we argue that watermarking database indexes in addition to the database tables would improve the detection of unauthorized alterations. Usually, each database table in commercial applications has more than one index attached to it. Thus, watermarking the database table and all its indexes improve the likelihood of detecting malicious attacks. In general, watermarking different indexes like R-trees, B-trees, Hashes, require different watermarking techniques and exploit different redundancies in the underlying data structure. This diversity in watermarking techniques contributes to the overall integrity of the databases.
28|7||Design and implementation of highly reliable dual-computer systems|Two of the main parameters of real-time computer systems are reliability and performance. Researchers are always looking for solutions to increase the values of these parameters, which is the goal of this study. To this end, we propose an architecture for a dual-computer system that operates in real-time with fault tolerance implemented purely by hardware. The hardware, as designed and implemented, performs the following key services: 1) determination of the fault type (temporary or permanent) and 2) localization of the faulty computer without using self-testing techniques or diagnostic routines. Our design has several benefits: 1) the designed hardware shortens the recovery point time period; 2) the proposed nontrivial sequence of fault-tolerant services reduces (to two) the number of logical segments that must be re-run to recover computational processes; and 3) the determination of the fault type allows for the elimination of only computers with permanent faults. These contributions yield improvements in both the performance and reliability of the system.
28|7||IFIP TC11 - Aims and Scope|
28|7||IFIP Technical Committee|
28|7||Call For Papers|
28|8|http://www.sciencedirect.com/science/journal/01674048/28/8|Contents|
28|8||Editorial|
28|8||DNS-based email sender authentication mechanisms: A critical review|We describe and compare three predominant email sender authentication mechanisms based on DNS: SPF, DKIM and Sender-ID Framework (SIDF). These mechanisms are designed mainly to assist in filtering of undesirable email messages, in particular spam and phishing emails. We clarify the limitations of these mechanisms, identify risks, and make recommendations. In particular, we argue that, properly used, SPF and DKIM can both help improve the efficiency and accuracy of email filtering.
28|8||Issues and challenges in securing VoIP|Voice over the Internet protocol (VoIP) is being rapidly deployed, and the convergence of the voice and data worlds is introducing exciting opportunities. Lower cost and greater flexibility are the key factors luring enterprises to transition to VoIP. Some security problems may surface with the widespread deployment of VoIP. In this article, we discuss these security problems and propose a high-level security architecture that captures required features at each boundary-network-element in the VoIP infrastructure. We describe mechanisms to efficiently integrate information between distributed security components in the architecture.
28|8||PENET: A practical method and tool for integrated modeling of security attacks and countermeasures|With the rise of cyber attack activities in the recent years, research in this area has gained immense emphasis. One of such research efforts is modeling of cyber attacks and countermeasures. In this context, several modeling approaches have been developed, such as approaches based on attack trees and on various stochastic tools. Attack tree model is one of the most intuitive and widely used tool. Although its simple design possesses various strengths, some unaddressed weaknesses such as imprecise analysis, limited modeling capabilities, and static nature plague its full potential. We propose a new modeling approach, called PENET, by extending the attack trees with new modeling constructs and analysis approaches. We add dynamic constructs for modeling dynamic behavior of system, arrival constructs that model periodic nature of attacks based on their cost, and defense constructs that model reparability of an insecure system. Petri Net Attack Modeling (PENET) approach has ability to convert and enhance existing attack trees with finer parameters, dynamic constructs, Petri net representation power, and intuitive time-domain analysis. We show how attack trees can be converted and analyzed in Petri net domain. We provide algorithm for time-domain analysis of PENET model, and performance metrics that are used to quantitatively describe survivability of a vulnerable system and effectiveness of attacker and victim's efforts. Next, we introduce PENET Tool as a practical software implementation of our approach. Finally, we provide a case study that illustrates the PENET approach. Security, dependability evaluation, security evaluation, performability evaluation, stochastic modeling.
28|8||Proposal, design and evaluation of a mechanism to limit the length of anonymous overlay network paths|An alternative to guarantee anonymity in overlay networks may be achieved by building a multi-hop path between the initiator and the destination. Random walks (also known by means of the Crowds algorithm) have been widely used for this purpose in IP networks. Therefore, we explore the use of a Crowds-based mechanism to provide anonymity in overlay networks. However, the original algorithm does not limit the length of the paths, and in an overlay network the associated costs may grow excessively. Thus, controlling the length of the Crowds-based paths is a crucial issue in this scenario. A straightforward implementation makes use of a time-to-live (TTL) field. However, this implementation will immediately reveal whether the predecessor node is the initiator or not. This paper presents a novel mechanism to control the path length without using the TTL field. We propose an analytical model to evaluate the degree of anonymity when the path length is limited using our scheme. We conclude that limiting the multi-hop path length does not have any relevant impact over the degree of anonymity. We also prove that the new mechanism does not increase the vulnerability of Crowds over the traffic analysis and predecessor attacks.
28|8||A study of on/off timing channel based on packet delay distribution|An on/off timing channel is a typical network covert timing channel, which can be used by attackers to steal information from compromised systems without triggering network firewalls and intrusion detection systems. In this paper, we discuss the principle of the information transmission in an on/off timing channel and categorize such channels into two types: deterministic channels and non-deterministic channels. We then analyze the components of packet delay and their characteristics, and provide a method of calculating the maximum transmission rate of a non-deterministic channel based on the packet delay distribution. After that, we conduct experiments to obtain the packet delay distribution in real network, and calculate the maximum transmission rate via our method. Then we construct an actual channel, and attain the actual transmission rate based on the observed symbol transmission probabilities. Our experiments show that the transmission rate calculated through our method is close to the real one, and can reveal the risk of the information leakage via on/off time channels in a network. In addition, the results indicate that non-deterministic channels may bring more threat than deterministic ones in the same network, and the information leakage via on/off timing channels should gain more intention.
28|8||Classification of web robots: An empirical study based on over one billion requests|Many studies on detection and classification of web robots have focused their attention mostly on text crawlers, and empirical experiments used relatively small data collected at universities. In this paper, we analyzed more than one billion requests to www.microsoft.com in 24 h. Web logs were made anonymous to eliminate potential privacy concerns while preserving essential characteristics (e.g., frequency, queries, etc). We have developed an effective characterization metrics, based on workload characteristics and resource types, in detecting and classifying various web robots including text crawlers, link checkers, and icon crawlers. As expected, web robot behavior was clearly different from that of typical interactive users, and different types of web robots also exhibited different characteristics. However, comparison of the similar type of web robots, text crawlers in particular, revealed different characteristics, thereby enabling characterization with reasonably high confidence level. We divided various feature metrics into five groups, and effectiveness of each group in classification is shown in polar diagram in the decreasing order of effectiveness in the clockwise direction. One can use the findings to classify likely identify of unknown web robots, and organizations can develop appropriate measures to deal with them. Our analysis is based on recent web log data collected at one of the best known site which offers truly global service.
28|8||OSNP: Secure wireless authentication protocol using one-time key|Handover security and efficiency have become more and more important in modern wireless network designs. In this paper, we propose a new protocol using the one-time key for user authentication. The proposed protocol can support both intra-domain and inter-domain authentications efficiently. Our protocol requires five messages for intra-domain initial authentication; three for subsequent authentication; and five for handover authentication. No authentication server is needed during handover, and our design reduces the computing load on the authentication server. We show an integration and implementation of EAP from 802.1X and our protocol, giving an easy way to apply our protocol on existing 802.11 wireless networks. The proposed protocol is realized and verified on the SWOON secure wireless testbed.
28|8||Self-efficacy in information security: Its influence on end users' information security practice behavior|The ultimate success of information security depends on appropriate information security practice behaviors by the end users. Based on social cognitive theory, this study models and tests relationships among self-efficacy in information security, security practice behavior and motivation to strengthen security efforts. This study also explores antecedents to individuals' self-efficacy beliefs in information security. Results provide support for the many hypothesized relationships. This study provides an initial step toward understanding of the applicability of social cognitive theory in a new domain of information security. The results suggest that simply listing what not to do and penalties associated with a wrong doing in the users' information security policy alone will have a limited impact on effective implementation of security measures. The findings may help information security professionals design security awareness programs that more effectively increase the self-efficacy in information security.
28|8||Using a bioinformatics approach to generate accurate exploit-based signatures for polymorphic worms|In this paper, we propose Simplified Regular Expression (SRE) signature, which uses multiple sequence alignment techniques, drawn from bioinformatics, in a novel approach to generating more accurate exploit-based signatures. We also provide formal definitions of what is “a more specific” and what is “the most specific” signature for a polymorphic worm and show that the most specific exploit-based signature generation is NP-hard. The approach involves three steps: multiple sequence alignment to reward consecutive substring extractions, noise elimination to remove noise effects, and signature transformation to make the SRE signature compatible with current IDSs. Experiments on a range of polymorphic worms and real-world polymorphic shellcodes show that our bioinformatics approach is noise-tolerant and as that because it extracts more polymorphic worm characters, like one-byte invariants and distance restrictions between invariant bytes, the signatures it generates are more accurate and precise than those generated by some other exploit-based signature generation schemes.
28|8||Reverse OAuth: A solution to achieve delegated authorizations in single sign-on e-learning systems|Current scientific and technological progress has led to the proliferation of e-learning systems known as Learning Management Systems. These systems consist of a central application for managing the sequencing of students' tasks, and also on several other educational applications that allow its users (teachers and learners) to communicate, carry out experiments, etc. However, despite the widespread use of these systems they show a usability problem when both kinds of applications require spare authentication processes. Indeed, users have to introduce several kinds of credentials, preventing them from focusing their efforts on their studies and increasing the so-called “password stress”. Several initiatives such as OAuth or Delegation Permits have dealt with the problem of delegated authorizations, but their requirements are different from those that arise from an e-learning environment. In this paper we introduce Reverse OAuth – a protocol to enable the granting of authorizations to access protected resources in educational environments.
28|8||IFIP TCII - Aims and Scope|
28|8||IFIP Technical Committee|
28|8||Call for Papers|
29|1|http://www.sciencedirect.com/science/journal/01674048/29/1|Contents|
29|1||Editorial|
29|1||A survey of video encryption algorithms|The popularity of multimedia applications is rapidly growing nowadays. The confidentiality of video communication is of primary concern for commercial usage, e.g. in video on demand services or business meetings. A variety of video encryption algorithms have been proposed in order to fulfill the specific requirements raised by the peculiarities of video communication. Video encryption algorithms can be classified according to their association with video compression into joint compression and encryption algorithms, and compression-independent encryption algorithms. From this classification perspective, we give a complete survey of the representative video encryption algorithms proposed so far and present their properties and limitations. We show by comparing and assessing the surveyed schemes that each scheme has its own strengths and weaknesses and no scheme can meet all specific requirements. Hence, video applications have to select an appropriate video encryption algorithm that meets their confidentiality requirements.
29|1||An intruder model with message inspection for model checking security protocols|Model checking security protocols is based on an intruder model that represents the eavesdropping or interception of the exchanged messages, while at the same time performs attack actions against the ongoing protocol session(s). Any attempt to enumerate all messages that can be deduced by the intruder and the possible actions in all protocol steps results in an enormous branching of the model's state-space. In current work, we introduce a new intruder model that can be exploited for state-space reduction, optionally in combination with known techniques, such as partial order and symmetry reduction. The proposed intruder modeling approach called Message Inspection (MI) is based on enhancing the intruder's knowledge with metadata for the exchanged messages. In a preliminary simulation run, the intruder tags the analyzed messages with protocol-specific values for a set of predefined parameters. This metadata is used to identify possible attack actions, for which it is a priori known that they cannot cause a security violation. The MI algorithm selects attack actions that can be discarded, from an open-ended base of primitive attack actions. Thus, model checking focuses only on attack actions that may disclose a security violation. The most interesting consequence is a non-negligible state-space pruning, but at the same time our approach also allows customizing the behavior of the intruder model, in order e.g. to make it appropriate for model checking problems that involve liveness. We provide experimental results obtained with the SPIN model checker, for the Needham–Schroeder security protocol.
29|1||Reducing false positives in intrusion detection systems|A post-processing filter is proposed to reduce false positives in network-based intrusion detection systems. The filter comprises three components, each one of which is based upon statistical properties of the input alert set. Special characteristics of alerts corresponding to true attacks are exploited. These alerts may be observed in batches, which contain similarities in the source or destination IPs, or they may produce abnormalities in the distribution of alerts of the same signature. False alerts can be recognized by the frequency with which their signature triggers false positives. The filter architecture and design are discussed. Evaluation results performed using the DARPA 1999 dataset indicate that the proposed approach can significantly reduce the number and percentage of false positives produced by Snort© (Roesch, 1999). Our filter limited false positives by a percentage up to 75%.
29|1||On the detection and identification of botnets|We develop and discuss automated and self-adaptive systems for detecting and classifying botnets based on machine learning techniques and integration of human expertise. The proposed concept is purely passive and is based on analyzing information collected at three levels: (i) the payload of single packets received, (ii) observed access patterns to a darknet at the level of network traffic, and (iii) observed contents of TCP/IP traffic at the protocol level.
29|1||Anonymization models for directional location based service environments|Location based services (LBS) aim to deliver information based on a mobile user's location. However, knowledge of the location can be used by an adversary to physically locate the person, leading to the risk of physical harm, as well as possible leakage of certain personal information. This has serious consequences on privacy. The concept of location k-anonymity has been proposed to address this. Under this notion of anonymity, the adversary only has the knowledge that the LBS request is originating from a region containing at least k people, and therefore cannot individually distinguish the user. However, the existing anonymity models ignore the movement information of mobile users, assuming that it has no impact on privacy. Thus, existing work cannot ensure complete privacy while serving advanced type of LBS requests that require information about direction as well as speed of motion. We denote such LBS services as directional LBS. The key observation we make in this paper is that, in addition to the user's location, the user's movement direction should also be considered to ensure true anonymization. In this paper, we extend the notion of location k-anonymity by incorporating user's moving direction into the anonymization process while serving directional LBS. Specifically, our anonymization methods generalize both location and direction to the extent specified by the user. Our experimental results demonstrate that such anonymization can be achieved with marginal increase in computational cost when compared to the traditional location k-anonymity, while providing increased anonymity.
29|1||Runtime monitoring for next generation Java ME platform|Many modern mobile devices, such as mobile phones or Personal digital assistants (PDAs), are able to run Java applications, such as games, Internet browsers, chat tools and so on. These applications perform some operations on the mobile device, that are critical from the security point of view, such as connecting to the Internet, sending and receiving SMS messages, connecting to other devices through the Bluetooth interface, browsing the user's contact list, and so on. Hence, an adequate security support is required to protect the device from malicious applications.
29|1||The inference problem: Maintaining maximal availability in the presence of database updates|In this paper, we present the Dynamic Disclosure Monitor (D2Mon) architecture to prevent illegal inferences via database constraints. D2Mon extends the functionality of Disclosure Monitor (DiMon) to address database updates while preserving the soundness and completeness properties of the inference algorithms. We study updates from the perspective of increasing data availability. That is, updates on tuples that were previously released may affect the correctness of the user inferences over these tuples. We develop a mechanism, called Update Consolidator (UpCon), that propagates updates to a history file to ensure that no query is rejected based on inferences derived from outdated data. The history file is used by the Disclosure Inference Engine (DiIE) to compute inferences. We show that UpCon and DiIE working together guarantee confidentiality (completeness property of the data-dependent disclosure inference algorithm) and maximal availability (soundness property of the data-dependent disclosure inference algorithm) even in the presence of updates. We also present our implementation of D2Mon and our empirical results.
29|1||Worm virulence estimation for the containment of local worm outbreak|A worm-infected host scanning globally may not cause any new infection in its underlying local network before it is detected and quarantined by a worm detector. To defend this type of scanning hosts, a number of worm scanner detection methods such as failed scan detection, honeypot, and dark port detection are proposed. However, for a stealthier worm limiting its scan inside an enterprise network, the chance of a successful local outbreak increases substantively due to the more limited scan space.
29|1||A survey of coordinated attacks and collaborative intrusion detection|Coordinated attacks, such as large-scale stealthy scans, worm outbreaks and distributed denial-of-service (DDoS) attacks, occur in multiple networks simultaneously. Such attacks are extremely difficult to detect using isolated intrusion detection systems (IDSs) that monitor only a limited portion of the Internet. In this paper, we summarize the current research directions in detecting such attacks using collaborative intrusion detection systems (CIDSs). In particular, we highlight two main challenges in CIDS research: CIDS architectures and alert correlation algorithms. We review the current CIDS approaches in terms of these two challenges. We conclude by highlighting opportunities for an integrated solution to large-scale collaborative intrusion detection.
29|1||Pitfalls in CAPTCHA design and implementation: The Math CAPTCHA, a case study|We present a black-box attack against an already deployed CAPTCHA that aims to protect a free service delivered using the Internet. This CAPTCHA, referred to as “Math CAPTCHA” or “QRBGS CAPTCHA”, requests the user to solve a mathematical problem in order to prove human. We study significant problems both in its design and its implementation, and how those flaws can be used to completely solve this CAPTCHA using a low-cost attack. This attack requires no development in Artificial Intelligence or automatic character recognition, the intended path, thus becoming a side-channel attack, based on the previously mentioned CAPTCHAs flaws. We relate these flaws to common flaws found in other CAPTCHA proposals. We conclude with some tips for enhancing this CAPTCHA that can be considered as general guidelines.
29|1||IFIP TCII - Aims and Scope|
29|1||IFIP Technical Committee|
29|1||Call for papers|
29|2|http://www.sciencedirect.com/science/journal/01674048/29/2|Contents|
29|2||Editorial|
29|2||Certified electronic mail: Properties revisited|Certified electronic mail is an added value to traditional electronic mail. In the definition of this service some differences arise: a message in exchange for a reception proof, a message and a non repudiation of origin token in exchange for a reception proof, etc. It greatly depends on whether we want to emulate the courier service or improve the service in the electronic world. If the definition of the service seems conflictive, the definition of the properties and requirements of a good certified electronic mail protocol is even more difficult. The more consensuated features are the need of a fair exchange and the existence of a trusted third party (TTP). Each author chooses the properties that considers the most important, and many times the list is conditioned by the proposal. Which kind of TTP must be used? Must it be verifiable, transparent and/or stateless? Which features must the communication channel fulfil? Which temporal requirements must be established? What kind of fairness is desired? What efficiency level is required? Are confidentiality or transferability of the proofs compulsory properties? In this paper we collect the definitions, properties and requirements related with certified electronic mail. The aim of the paper is to create a clearer situation and analyze how some properties cannot be achieved simultaneously. Each protocol designer will have to decide which properties are the most important in the environment in where the service is to be deployed.
29|2||A secure peer-to-peer backup service keeping great autonomy while under the supervision of a provider|Making backup is so cumbersome and expensive that individuals hardly ever backup their data and companies usually duplicate their data into a secondary server. This paper proposes a novel Peer-to-Peer (P2P) backup system known as the SecureBackup service, which was defined in the DisPairse research project. It utilizes the unused personal hard disk spaces attached to the Internet to implement a distributed backup service that is reliable, performant, and secure. Additionally to the existing approaches like pStore (Batten et al., 2001), Pastiche (Landon et al., 2002), and PeerStore (Landers et al., 2004), addressing the integrity, confidentiality and availability of data in a P2P backup system, the SecureBackup service implements the access control of the peers to the service, the detection of malicious peers, the evaluation of the reliability level of each peer, the rewarding or charging of the peers for the consumed resources or the resources they made available, and the incentives for peers to actively participate to the service.
29|2||A framework and assessment instrument for information security culture|An organisation's approach to information security should focus on employee behaviour, as the organisation's success or failure effectively depends on the things that its employees do or fail to do. An information security-aware culture will minimise risks to information assets and specifically reduce the risk of employee misbehaviour and harmful interaction with information assets. Organisations require guidance in establishing an information security-aware or implementing an acceptable information security culture. They need to measure and report on the state of information security culture in the organisation. Various approaches exist to address the threats that employee behaviour could pose. However, these approaches do not focus specifically on the interaction between the behaviour of an employee and the culture in an organisation. Organisations therefore have need of a comprehensive framework to cultivate a security-aware culture. The objective of this paper is to propose a framework to cultivate an information security culture within an organisation and to illustrate how to use it. An empirical study is performed to aid in validating the proposed Information Security Culture Framework.
29|2||WARP: A wormhole-avoidance routing protocol by anomaly detection in mobile ad hoc networks|The infrastructure of a Mobile Ad hoc Network (MANET) has no routers for routing, and all nodes must share the same routing protocol to assist each other when transmitting messages. However, almost all common routing protocols at present consider performance as first priority, and have little defense capability against the malicious nodes. Many researches have proposed various protocols of higher safety to defend against attacks; however, each has specific defense objects, and is unable to defend against particular attacks. Of all the types of attacks, the wormhole attack poses the greatest threat and is very difficult to prevent; therefore, this paper focuses on the wormhole attack, and proposes a secure routing protocol based on the AODV (Ad hoc On-demand Distance Vector) routing protocol, which is named WARP (Wormhole-Avoidance Routing Protocol). WARP considers link-disjoint multipaths during path discovery, and provides greater path selections to avoid malicious nodes, but eventually uses only one path to transmit data. Based on the characteristic that wormhole nodes can easily grab the route from the source node to the destination node, WARP enables the neighbors of the wormhole nodes to discover that the wormhole nodes have abnormal path attractions. Then, the wormhole nodes would be gradually isolated by their normal neighboring nodes, and finally be quarantined by the whole network.
29|2||Survey of network security systems to counter SIP-based denial-of-service attacks|Session Initiation Protocol is a core protocol for coming real time communication networks, including VoIP, IMS and IPTV networks. Based on the open IP stack, it is similarly susceptible to Denial-of-Service Attacks launched against SIP servers. More than 20 different research works have been published to address SIP-related DoS problems. In this survey we explain three different types of DoS attacks on SIP networks, called SIP message payload tampering, SIP message flow tampering and SIP message flooding. We survey different approaches to counter these three types of attacks. We show that there are possible solutions for both payload and flow tampering attacks, and partial solutions for message flooding attacks. We conclude by giving hints how open flooding attacks issues could be addressed.
29|2||Two proposed identity-based three-party authenticated key agreement protocols from pairings|The use of pairings has been shown promising for many two-party and three-party identity-based authenticated key agreement protocols. In recent years, several identity-based authenticated key agreement protocols have been proposed and most of them broken. In this paper, we propose two three-party identity-based authenticated key agreement protocols applying bilinear pairings. We show that the proposed protocols are secure (i.e. conform to defined security attributes) while being efficient.
29|2||On the symbiosis of specification-based and anomaly-based detection|As the number of attacks on computer systems increases and become more sophisticated, there is an obvious need for intrusion detection systems to be able to effectively recognize the known attacks and adapt to novel threats. The specification-based intrusion detection has been long considered as a promising solution that integrates the characteristics of ideal intrusion detection system: the accuracy of detection and ability to recognize novel attacks. However, one of the main challenges of applying this technique in practice is its dependence on the user guidance in developing the specification of normal system behavior. In this work, we present an approach for automatic generation of specifications for any software systems executing on a single host based on the combination of two techniques: specification-based and anomaly-based approaches. The proposed technique allows automatic development of the normal and abnormal behavioral specifications in a form of variable-length patterns classified via anomaly-based approach. Specifically, we use machine-learning algorithm to classify fixed-length patterns generated via sliding window technique to infer the classification of variable-length patterns from the aggregation of the machine learning based classification results. We describe the design and implementation of our technique and show its practical applicability in the domain of security monitoring through simulation and experiments.
29|2||An efficient and fair buyerâseller fingerprinting scheme for large scale networks|In digital watermarking, most existing schemes focus on the owners' copyright protection rather than protection of the customers' rights. Therefore, these schemes are unfair to legitimate customers who have no certificate to prove their right to use the watermarked digital content that they have purchased. In addition, these schemes are also unable to identify those who leak pirated copies of the watermarked digital content. To protect customers' rights and to identify the users of unauthorized copies, the fingerprinting technique is a feasible method for embedding a watermark so that content owners can identify users who have purchased the right to use the content and users who have not purchased this right. Although some fingerprinting schemes have been proposed in recent years, most of them are inefficient due to their homomorphic architecture that is based on public key cryptography. Therefore, in this paper, we propose a fair, traceable, and efficient watermarking scheme with a novel architecture. Due to the high computational complexity of the asymmetric cryptography, such as modular multiplications and exponentiations which lead much heavier burden than operations in symmetric cryptography, the proposed protocol transfers the demanding computational requirements from the buyer to a powerful server in protocol design. The proposed method can achieve these benefits: 1) the rights of legitimate buyers can be protected; 2) the proposed scheme is traceable; 3) the proposed scheme is more efficient than the previous schemes because public key cryptography is not frequently used; and 4) the buyer's anonymity can be well-protected until there is an infringement accusation.
29|2||PKI-based trust management in inter-domain scenarios|Hierarchical cross-certification fits well within large organizations that want their root CA to have direct control over all subordinate CAs. However, both Peer-to-Peer and Bridge CA cross-certification models suits better than the hierarchical one with organizations where a certain level of flexibility is needed to form and revoke trust relationships with other organizations as changing policy or business needs dictate. It seems that this second approach better fits the current and next-generation inter-domain networking models existing in both the wired and wireless Internet. In this context, this paper analyses some relevant inter-domain scenarios and derives the main requirements in terms of cross-certification from them. It then describes the design and lab implementation of a pan-European scenario which is based on a research network composed by a set of organizations that may have their own PKIs running, and that are interested to link with others in terms of certification services. It provides a complete design, implementation and performance analysis for this complex scenario, including a procedure and practical recommendations for building and validating certification paths.
29|2||IFIP TCII - Aims and Scope|
29|2||IFIP Technical Committee|
29|2||Call for papers|
29|2||Call for papers|
29|3|http://www.sciencedirect.com/science/journal/01674048/29/3|Contents|
29|3||Special issue on software engineering for secure systems|
29|3||Provably correct Java implementations of Spi Calculus security protocols specifications|Spi Calculus is an untyped high level modeling language for security protocols, used for formal protocols specification and verification. In this paper, a type system for the Spi Calculus and a translation function are formally defined, in order to formalize the refinement of a Spi Calculus specification into a Java implementation. The Java implementation generated by the translation function uses a custom Java library. Formal conditions on such library are stated, so that, if the library implementation code satisfies such conditions, then the generated Java implementation correctly simulates the Spi Calculus specification. A verified implementation of part of the custom library is further presented.
29|3||Runtime verification of cryptographic protocols|There has been a significant amount of work devoted to the static verification of security protocol designs. Virtually all of these results, when applied to an actual implementation of a security protocol, rely on certain implicit assumptions on the implementation (for example, that the cryptographic checks that according to the design have to be performed by the protocol participants are carried out correctly). So far there seems to be no approach that would enforce these implicit assumptions for a given implementation of a security protocol (in particular regarding legacy implementations which have not been developed with formal verification in mind).
29|3||A knowledgeable security model for distributed health information systems|Realising the vision of pervasive healthcare will generate new challenges to system security. Such challenges are fundamentally different from issues and problems that we face in centralised approaches as well as non-clinical scenarios. In this paper, we reflect upon our experiences in the HealthAgents project wherein a prototype system was developed and a novel approach employed that supports data transfer and decision making in human brain tumour diagnosis and treatment. While the decision making needs to rely on different clinical expertise, the HealthAgents system leveraged a domain ontology to align different sub-domain vocabularies and we have experimented with a process calculus to glue together distributed services. We examine the capability of the Lightweight Coordination Calculus (LCC), a process calculus based language, in meeting security challenges in pervasive settings, especially in the healthcare domain. The key difference in approach lies in making the representational abstraction reflect the relative autonomy of the various clinical specialisms involved in contributing to patient management. The scope within LCC of accommodating Boolean-valued constraints allows for flexible integration of heterogeneous sources in multiple formats, which are characteristic features of a pervasive healthcare environment.
29|3||A framework of composable access control features: Preserving separation of access control concerns from models to code|Modeling of security policies, along with their realization in code, must be an integral part of the software development process, to achieve an acceptable level of security for a software application. Among all of the security concerns (e.g. authentication, auditing, access control, confidentiality, etc.), this paper addresses the incorporation of access control into software. The approach is to separate access control concerns from the rest of the design. To assist designers to visualize access control policies separated from non-security concerns, this paper proposes a set of access control diagrams, i.e., extensions to the UML to represent three main access control models: role-based access control (RBAC), mandatory access control (MAC), and discretionary access control (DAC). To better adapt to changing requirements, and assist designers to customize access control policies, this paper proposes a set of access control features, i.e., small components that realize specific capabilities of access control models. Designers can select the features they require, and compose them to yield different access control policies. When transitioning into code, the main focus is to preserve separation of access control concerns. This paper describes an approach to realize access control diagrams and features in code through structure-preserving mappings, describes three different approaches to enforce access control in code, and evaluates the way each of them separate access control from other concerns.
29|3||IFIP TCII - Aims and Scope|
29|3||IFIP Technical Committee|
29|4|http://www.sciencedirect.com/science/journal/01674048/29/4|Contents|
29|4||Editorial|
29|4||Extensions to the source path isolation engine for precise and efficient log-based IP traceback|IP traceback is used to determine the source and path traversed by a packet received from the Internet. In this work we first show that the Source Path Isolation Engine (SPIE), a classical log-based IP traceback system, can return misleading attack graphs in some particular situations, which may even make it impossible to determine the real attacker. We show that by unmasking the TTL field SPIE returns a correct attack graph that precisely identifies the route traversed by a given packet allowing the correct identification of the attacker. Nevertheless, an unmasked TTL poses new challenges in order to preserve the confidentiality of the communication among the system's components. We solve this problem presenting two distributed algorithms for searching across the network overlay formed by the packet log bases. Two other extensions to SPIE are proposed that improve the efficiency of source discovery: separate logs are kept for each router interface improving the distributed search procedure; an efficient dynamic log paging strategy is employed, which is based on the actual capacity factor instead of the fixed time interval originally employed by SPIE. The system was implemented and experimental results are presented.
29|4||A security privacy aware architecture and protocol for a single smart card used for multiple services|In the face of the expanding Internet and an ever-growing number of threats, today's society is becoming more geared towards greater security and protection of privacy and personal information. Smart cards provide protection for information at the hardware level, however, smart cards are designed for use with a single specific application. In this paper we introduce the concept of utilising a single smart card with multiple applications. Such a scheme would, however, increase the reward of an attack on the smart card due to the amount of information stored on a smart card. This paper proposes an architecture to allow a single smart card to be used in a dynamic multiple application environment. In conjunction with the architecture, a protocol messaging scheme is provided to protect all information communicated between the smart card and an application through the use of one-time passwords, whilst maintaining the privacy of one's personal information.
29|4||Stability analysis of a SEIQV epidemic model for rapid spreading worms|Internet worms have drawn significant attention owing to their enormous threats to the Internet. Due to the rapid spreading nature of Internet worms, it is necessary to implement automatic mitigation on the Internet. Inspired by worm vaccinations, we propose a novel epidemic model which combines both vaccinations and dynamic quarantine methods, referred to as SEIQV model. Using SEIQV model, we obtain the basic reproduction number that governs whether or not a worm is extinct. The impact of different parameters on this model is studied. Simulation results show that the performance of our model is significantly better than other models, in terms of decreasing the number of infected hosts and reducing the worm propagation speed.
29|4||Significantly improved performances of the cryptographically generated addresses thanks to ECC and GPGPU|Cryptographically Generated Addresses (CGA) are today mainly used with the Secure Neighbor Discovery Protocol (SEND). Despite CGA generalization, current standards only show how to construct CGA with the RSA algorithm and SHA-1 hash function. This limitation may prevent new usages of CGA and SEND in mobile environments where nodes are energy and storage limited.
29|4||Improving information security awareness and behaviour through dialogue, participation and collective reflection. An intervention study|The paper discusses and evaluates the effects of an information security awareness programme. The programme emphasised employee participation, dialogue and collective reflection in groups. The intervention consisted of small-sized workshops aimed at improving information security awareness and behaviour. An experimental research design consisting of one survey before and two after the intervention was used to evaluate whether the intended changes occurred. Statistical analyses revealed that the intervention was powerful enough to significantly change a broad range of awareness and behaviour indicators among the intervention participants. In the control group, awareness and behaviour remained by and large unchanged during the period of the study. Unlike the approach taken by the intervention studied in this paper, mainstream information security awareness measures are typically top-down, and seek to bring about changes at the individual level by means of an expert-based approach directed at a large population, e.g. through formal presentations, e-mail messages, leaflets and posters. This study demonstrates that local employee participation, collective reflection and group processes produce changes in short-term information security awareness and behaviour.
29|4||Hybrid spam filtering for mobile communication|Spam messages are an increasing threat to mobile communication. Several mitigation techniques have been proposed, including white and black listing, challenge-response and content-based filtering. However, none are perfect and it makes sense to use a combination rather than just one. We propose an anti-spam framework based on the hybrid of content-based filtering and challenge-response. A message, that has been classified as uncertain through content-based filtering, is checked further by sending a challenge to the message sender. An automated spam generator is unlikely to send back a correct response, in which case, the message is classified as spam.
29|4||A generic mechanism for efficient authentication in B3G networks|
29|4||Information security culture: A management perspective|Information technology has become an integral part of modern life. Today, the use of information permeates every aspect of both business and private lives. Most organizations need information systems to survive and prosper and thus need to be serious about protecting their information assets. Many of the processes needed to protect these information assets are, to a large extent, dependent on human cooperated behavior. Employees, whether intentionally or through negligence, often due to a lack of knowledge, are the greatest threat to information security. It has become widely accepted that the establishment of an organizational sub-culture of information security is key to managing the human factors involved in information security. This paper briefly examines the generic concept of corporate culture and then borrows from the management and economical sciences to present a conceptual model of information security culture. The presented model incorporates the concept of elasticity from the economical sciences in order to show how various variables in an information security culture influence each other. The purpose of the presented model is to facilitate conceptual thinking and argumentation about information security culture.
29|4||Power system DNP3 data object security using data sets|Power system cyber security demand is escalating with the increased number of security incidents and the increased stakeholder participation in power system operations, specifically consumers. Rule-based cyber security is proposed for Distributed Network Protocol (DNP3) outstation devices, with a focus on smart distribution system devices. The security utilizes the DNP3 application layer function codes and data objects to determine data access authorization for outstations, augmenting other security solutions that include firewalls, encryption, and authentication. The cyber security proposed in this article protects outstation devices when masters are compromised or attempt unauthorized access that bypass the other security solutions. In this article, non-utility stakeholder data access is limited through DNP3 data sets rather than granting direct access to the data points within an outstation. The data set utilization greatly constrains possible attack methods against a device by reducing the interaction capabilities with an outstation. The data sets also decrease the security complexity through rule reduction, thereby increasing the security applicability for retrofitted or process constrained devices. Temporal security constraints are supported for the data sets, increasing security against denial of service attacks.
29|4||Pervasive authentication and authorization infrastructures for mobile users|Network and device heterogeneity, nomadic mobility, intermittent connectivity and, more generally, extremely dynamic operating conditions, are major challenges in the design of security infrastructures for pervasive computing. Yet, in a ubiquitous computing environment, limitations of traditional solutions for authentication and authorization can be overcome with a pervasive public key infrastructure (pervasive-PKI). This choice allows the validation of credentials of users roaming between heterogeneous networks, even when global connectivity is lost and some services are temporarily unreachable. Proof-of-concept implementations and testbed validation results demonstrate that strong security can be achieved for users and applications through the combination of traditional PKI services with a number of enhancements like: (i) dynamic and collaborative trust model, (ii) use of attribute certificates for privilege management, and (iii) modular architecture enabling nomadic mobility and enhanced with reconfiguration capabilities.
29|4||IFIP TCII - Aims and Scope|
29|4||IFIP Technical Committee|
29|5|http://www.sciencedirect.com/science/journal/01674048/29/5|Contents|
29|5||Editorial|
29|5||Reconstruction of electronic signatures from eDocument printouts|Governments and public administrations produce documents: laws, orders, permits, notifications, etc. With the transition from traditional paper-based administration to eGovernment that we have seen in the last decade, authentic electronic documents gain importance. Electronic signatures promise to be a tool of choice. However, given the choice of access channels public administrations offer, i.e. electronic or conventional access to services, eDocuments will have to co-exist with traditional paper documents for several years, if not for decades. In this paper we present a solution that visually adds electronic signatures to documents. This is done in a way that allows for verifying the electronic signature from printouts. The purpose is to make the electronic signature resistant against media-breaks and to allow the authority issuing electronic documents, even if the receiver prefers conventional paper documents. We discuss the Austrian practical experience gained with such eSignatures and eDocuments in eGovernment.
29|5||Managing key hierarchies for access control enforcement: Heuristic approaches|Data outsourcing is emerging today as a successful paradigm allowing individuals and organizations to resort to external servers for storing their data, and sharing them with others. The main problem of this trend is that sensitive data are stored on a site that is not under the data owner's direct control. This scenario poses a major security problem since often the external server is relied upon for ensuring high availability of the data, but it is not authorized to read them. Data need therefore to be encrypted. In such a context, the application of an access control policy requires different data to be encrypted with different keys so to allow the external server to directly enforce access control and support selective dissemination and access. The problem therefore emerges of designing solutions for the efficient management of an encryption policy enforcing access control, with the goal of minimizing the number of keys to be maintained by the system and distributed to users.
29|5||Taming role mining complexity in RBAC|In this paper we address the problem of reducing the role mining complexity in RBAC systems. To this aim, we propose a three steps methodology: first, we associate a weight to roles; second, we identify user-permission assignments that cannot belong to roles with a weight exceeding a given threshold; and third, we restrict the role-finding problem to user-permission assignments identified in the second step. We formally show—the proofs of our results are rooted in graph theory—that this methodology allows role engineers for the elicitation of stable candidate roles, by contextually simplifying the role selection task. Efficient algorithms to implement our strategy are also described. Further, we discuss practical applications of our approach. Finally, we tested our methodology on real dataset. Results achieved confirm both the viability of our proposal and the analytical findings.
29|5||On a taxonomy of delegation|
29|5||Collaborative privacy management|The landscape of the World Wide Web with all its versatile services heavily relies on the disclosure of private user information. Unfortunately, the growing amount of personal data collected by service providers poses a significant privacy threat for Internet users. Targeting growing privacy concerns of users, privacy-enhancing technologies emerged. One goal of these technologies is the provision of tools that facilitate a more informative decision about personal data disclosures. A famous PET representative is the PRIME project that aims for a holistic privacy-enhancing identity management system. However, approaches like the PRIME privacy architecture require service providers to change their server infrastructure and add specific privacy-enhancing components. In the near future, service providers are not expected to alter internal processes. Addressing the dependency on service providers, this paper introduces a user-centric privacy architecture that enables the provider-independent protection of personal data. A central component of the proposed privacy infrastructure is an online privacy community, which facilitates the open exchange of privacy-related information about service providers. We characterize the benefits and the potentials of our proposed solution and evaluate a prototypical implementation.
29|5||Roving bugnet: Distributed surveillance threat and mitigation|Advanced mobile devices such as laptops and smartphones make convenient hiding places for surveillance spyware. They commonly have a microphone and camera built-in, are increasingly network accessible, frequently within close proximity of their users, and almost always lack mechanisms designed to prevent unauthorized microphone or camera access.
29|5||Audio CAPTCHA: Existing solutions assessment and a new implementation for VoIP telephony|SPam over Internet Telephony (SPIT) is a potential source of future annoyance in Voice over IP (VoIP) systems. A typical way to launch a SPIT attack is the use of an automated procedure (i.e., bot), which generates calls and produces unsolicited audio messages. A known way to protect against SPAM is a Reverse Turing Test, called CAPTCHA (Completely Automated Public Turing Test to Tell Computer and Humans Apart). In this paper, we evaluate existing audio CAPTCHA, as this type of format is more suitable for VoIP systems, to help them fight bots. To do so, we first suggest specific attributes-requirements that an audio CAPTCHA should meet in order to be effective. Then, we evaluate this set of popular audio CAPTCHA, and demonstrate that there is no existing implementation suitable enough for VoIP environments. Next, we develop and implement a new audio CAPTCHA, which is suitable for SIP-based VoIP telephony. Finally, the new CAPTCHA is tested against users and bots and demonstrated to be efficient.
29|5||A provably secure secret handshake with dynamic controlled matching|A Secret Handshake is a protocol that allows two users to mutually verify one another's properties, with the assurance that only authorized parties are able to engage in a successful protocol run. In case of simultaneous matching, the two parties share a key that can be used to secure subsequent communications. In this paper, we present the first Secret Handshake scheme that allows dynamic matching of properties under stringent security requirements: in particular, the right to prove and to verify are strictly under the control of an authority. This work merges characteristics of Secret Handshake with features peculiar to Secure Matchmaking.
29|5||A note about the identifier parent property in Reed-Solomon codes|Codes with traceability properties are used in schemes where the identification of users that illegally redistribute content is required. For any code with traceability properties, the Identifiable Parent Property (c-IPP) seems to be less restrictive than the Traceability (c-TA) property. In this paper, we show that for Reed-Solomon codes both properties are in many cases equivalent. More precisely, we show that for an [n, k, d] Reed-Solomon code, defined over a field that contains the n – d roots of unity, both properties are equivalent. Also, we show how the strategy we propose can be applied to other cases by proving the equivalence of both properties for a particular code of characteristic 2. This answers a question posted by  12 and 13, for a large family of Reed-Solomon codes.
29|5||IFIP TCII - Aims, Scope and Technical Committee|
29|6|http://www.sciencedirect.com/science/journal/01674048/29/6|Contents|
29|6||Editorial|
29|6||A multi-layer Criticality Assessment methodology based on interdependencies|In this paper we propose a holistic Criticality Assessment methodology, suitable for the development of an infrastructure protection plan in a multi-sector or national level. The proposed methodology aims to integrate existing security plans and risk assessments performed in isolated infrastructures, in order to assess sector-wide or intra-sector security risks. In order to achieve this, we define three different layers of security assessments with different requirements and goals; the operator layer, the sector layer and the intra-sector or national layer. We determine the characteristics of each layer, as well as their interdependencies. In this way, existing security plans can be fully exploited in order to provide a “shortcut” for the development of security plans for complex inter-dependent infrastructures. A key element in the proposed methodology is the formal definition of interdependencies between different infrastructures and their respective sectors. Interdependencies between infrastructures belonging to the same or to a different sector, as well as interdependencies between different sectors, act as interfaces through which threats and their impacts occurring on different layers or different sectors, are conveyed to others. Current risk assessment methodologies fail to address effectively this issue, thus, the formalization of these interfaces and their interference is an important element for the definition of a holistic Criticality Assessment methodology.
29|6||A probabilistic relational model for security risk analysis|Information system security risk, defined as the product of the monetary losses associated with security incidents and the probability that they occur, is a suitable decision criterion when considering different information system architectures. This paper describes how probabilistic relational models can be used to specify architecture metamodels so that security risk can be inferred from metamodel instantiations.
29|6||On the detection of pod slurping attacks|Time is recognised to be a dimension of paramount importance in computer forensics. In this paper, we report on the potential of identifying past pod slurping type of attacks by constructing a synthetic metric based on information contained in filesystem timestamps. More specifically, by inferring the transfer rate of a file from last access timestamps and correlating that to the characteristic transfer rate capabilities of a suspicious USB found in the Windows registry, one could assess the probability of having suffered an unauthorised copy of files. Preliminary findings indicate that file transfer rates can be associated with the make and model of the USB storage device and give supporting information to the forensic analyst to identify file leakages.
29|6||Implementing a passive network covert timing channel|The paper concerns passive network covert timing channels, in which the channel senders reside in intermediate nodes (e.g. router, gateway) and forward the passing-by packets in a carefully planned manner to covertly transmit the information. In this study, we focus on constructing and testing a kind of passive network covert timing channel, in which the information is hidden in the transmission interval between two adjacent packets. We first introduce three channel states to cope with the fluctuation in the traffic used as carrier, and explore how to select suitable values for the channel parameters to obtain high communication performance. We then implement an actual channel using Video On Demand (VOD) traffic as carrier, and obtain the communication characteristics of the channel. Finally, we investigate an information transmission scheme over the channel, including frame design, frame synchronization and error correction.
29|6||PREON: An efficient cascade revocation mechanism for delegation paths|In decentralized network-based environments, resource sharing occurs more frequently as computing becomes more pervasive. Access to shared resources must be protected allowing access only to authorized entities. Delegation is a powerful mechanism to provide flexible and distributed access control when a user acts on another user’s behalf. User’s rights/attributes are contained in digital certificates and successive delegations generate chains of certificates. When an access control decision related to a delegation path has to be taken, its corresponding certificate chain has to be validated. Validation of long delegation paths is a costly process that might be critical when constrained devices are involved. In this article, we propose a mechanism called PREON (Prefix Revocation) which is based on prefix codes. PREON allows a privilege verifier to efficiently check a delegation chain when cascade revocation is enabled. We show by statistical analysis that our proposal outperforms delegation systems without prefix coding especially for long delegation paths and high revocation probabilities.
29|6||A preliminary two-stage alarm correlation and filtering system using SOM neural network and K-means algorithm|Intrusion Detection Systems (IDSs) play a vital role in the overall security infrastructure. Although the IDS has become an essential part of corporate network infrastructure, the art of detecting intrusion is still far from perfect. A significant problem is that of false alarms, as generating a huge volume of such alarms could render the system inefficient. In this paper, we propose a new method to reduce the number of false alarms. We develop a two-stage classification system using a SOM neural network and K-means algorithm to correlate the related alerts and to further classify the alerts into classes of true and false alarms. Preliminary experiments show that our approach effectively reduces all superfluous and noisy alerts, which often contribute to more than 50% of false alarms generated by a common IDS.
29|6||IFIP TCII - Aims, Scope and Technical Committee|
29|7|http://www.sciencedirect.com/science/journal/01674048/29/7|Contents|
29|7||Editorial|
29|7||Ethical decision making: Improving the quality of acceptable use policies|While there is extensive literature on the positive effects of institutionalising ethics in organisational culture, our extensive research in information security culture has found no evidence of organisations encouraging ethical decision making in situations where information security might be at risk. Security policies, in particular acceptable use policies, have traditionally been written with a strategy of deterrence in mind, but in practice they rely mostly on deontological ethics, i.e. employees doing the right thing, to work. As far back as 1990, evidence has been reported of a widening socio-technical gap, where employees no longer always act according to expected social norms in an organisation. This change in moral behaviour is reducing the effectiveness of acceptable use policies in an organisation. In this paper, an alternative approach to the development of security policies is proposed to encourage ethical decision making based on consequential ethics. Acceptable use policies will need to distinguish between guidelines, standards and procedures, and guidelines will need to be written in such a way that the policy continuously acknowledges that employees are no longer expected to blindly follow these guidelines. And, as acceptable use policies can no longer cover all the possible risks related to an employee’s behaviour, the policy will need to emphasise both explicitly an implicitly that employees are expected to make an ethical judgement on all their actions that may possibly endanger the organisation’s security. This will in turn have positive effects on the usability and suitability of the acceptable use policy to the organisation.
29|7||Network anomaly detection through nonlinear analysis|Nowadays every network is susceptible on a daily basis to a significant number of different threats and attacks both from the inside and outside world. Some attacks only exploit system vulnerabilities and their traffic pattern is undistinguishable from normal behavior, but in many cases the attack mechanisms combine protocol or OS tampering activity with a specific traffic pattern having its own particular characteristics. Since these traffic anomalies are now conceived as a structural part of the overall network traffic, it is more and more important to automatically detect, classify and identify them in order to react promptly and adequately. In this work we present a novel approach to network-based anomaly detection based on the analysis of non-stationary properties and “hidden” recurrence patterns occurring in the aggregated IP traffic flows. In the observation of the above transition patterns for detecting anomalous behaviors, we adopted recurrence quantification analysis, a nonlinear technique widely used in many science fields to explore the hidden dynamics and time correlations of statistical time series. Our model demonstrated to be effective for providing a deterministic interpretation of recurrence patterns originated by the complex traffic dynamics observable during the occurrence of “noisy” network anomaly phenomena (characterized by measurable variations in the statistical properties of the traffic time series), and hence for developing qualitative and quantitative observations that can be reliably used in detecting such events.
29|7||Efficient hardware support for pattern matching in network intrusion detection|Deep packet inspection forms the backbone of any Network Intrusion Detection (NID) system. It involves matching known malicious patterns against the incoming traffic payload. Pattern matching in software is prohibitively slow in comparison to current network speeds. Due to the high complexity of matching, only FPGA (Field-Programmable Gate Array) or ASIC (Application-Specific Integrated Circuit) platforms can provide efficient solutions. FPGAs facilitate target architecture specialization due to their field programmability. Costly ASIC designs, on the other hand, are normally resilient to pattern updates. Our FPGA-based solution performs high-speed pattern matching while permitting pattern updates without resource reconfiguration. To its advantage, our solution can be adopted by software and ASIC realizations, however at the expense of much lower performance and higher price, respectively. Our solution permits the NID system to function while pattern updates occur. An off-line optimization method first finds common sub-patterns across all the patterns in the SNORT database of signatures. A novel technique then compresses each pattern into a bit vector, where each bit represents such a sub-pattern. This approach reduces drastically the required on-chip storage as well as the complexity of pattern matching. The bit vectors for newly discovered patterns can be generated easily using a simple high-level language program before storing them into the on-chip RAM. Compared to earlier approaches, not only is our strategy very efficient while supporting runtime updates but it also results in impressive area savings; it utilizes just 0.052 logic cells for processing and 17.77 bits for storage per character in the current SNORT database of 6455 patterns. Also, the total number of logic cells for processing the traffic payload does not change with pattern updates.
29|7||A framework for security assurance of access control enforcement code|Modeling of access control policies, along with their implementation in code, must be an integral part of the software development process, to ensure that the proper level of security in an application is attained. Previous work of the authors in this area yielded a framework that incorporates access control at the design and code levels, through a set of new extensions to UML and a set of approaches to enfoce access control in an application (Pavlich-Mariscal et al., 2010). An essential property of the code that has not been addressed by that framework is security assurance, which, in the context of this research, is to insure that the application code behaves consistently with the access control policy. This paper proposes a security assurance mechanism that formalizes the application behavior using labeled transition systems and structural operational semantics ( Plotkin, 1981). Simulation relations ( Milner, 1971) are used to demonstrate the correctness of the access control code with respect to the design. To validate the approach, this paper proves correctness of two access control enforcement mechanisms that are part of our case study: a basic approach to implement access control in code and an aspect-oriented approach.
29|7||Approach for selecting the most suitable Automated Personal Identification Mechanism (ASMSA)|
29|7||IFIP TCII - Aims, Scope and Technical Committee|
29|8|http://www.sciencedirect.com/science/journal/01674048/29/8|Contents|
29|8||Security, technology, publishing, and ethics (Part I)|
29|8||Modeling and preventing TOCTTOU vulnerabilities in Unix-style file systems|TOCTTOU (Time-of-Check-To-Time-Of-Use) is a file-based race condition in Unix-style systems and characterized by a pair of file object access by a vulnerable program: a check operation establishes certain conditions about the file object (e.g., the file exists), followed by a use operation that assumes that the established condition still holds. Due to the lack of support for transactions in Unix-style file systems, an attacker can modify the established file condition in-between the check and use steps, thus causing significant harm. In this paper, we present a model of the TOCTTOU problem (called STEM), which enumerates all the potential file system call pairs (called exploitable TOCTTOU pairs) that form the check/use steps. The model shows that a successful TOCTTOU attack requires a change in the mapping of pathname to logical disk blocks between the check and use steps. We apply STEM to POSIX and Linux to demonstrate its practical value for Unix-style file systems. Then we propose a defense mechanism (called EDGI) that prevents an attacker from tampering with the file condition between exploitable TOCTTOU pairs during a vulnerable program’s execution. EDGI works at the file system level and does not require existing applications to change. We have implemented EDGI on Linux kernel 2.4.28 and our evaluation shows that EDGI is effective and incurs little overhead to application benchmarks such as Andrew and Postmark.
29|8||A behaviorist perspective on corporate harassment online: Validation of a theoretical model of psychological motives|Cyber harassment has been seen in the literature as a problem among school-aged children, or at the adult-level, as a legal problem involving attacks that have typically been associated with retaliation for some psychosocial or monetary gain. However, with the rise of the social networking phenomenon, cyber harassment has spread from school-aged children to a wider developmental and behavioral phenomenon. Companies and professionals are increasingly the targets of personal attacks on social network sites and in blog postings by “trolls” and “cyber bullies”. Thus rather than gaining information, these kinds of attacks often disseminate misleading or false information to damage their targets, to interfere with them, or for the purposes of extortion. We conducted a randomized field study of a bounded population into motivations that may lead people to conduct these kinds of attacks, we validated the model empirically, and we drew some potential behavioral responses along with implications for further research.
29|8||Cyber security for home users: A new way of protection through awareness enforcement|We are currently living in an age, where the use of the Internet has become second nature to millions of people. Not only businesses depend on the Internet for all types of electronic transactions, but more and more home users are also experiencing the immense benefit of the Internet.
29|8||Access control for smarter healthcare using policy spaces|A fundamental requirement for the healthcare industry is that the delivery of care comes first and nothing should interfere with it. As a consequence, the access control mechanisms used in healthcare to regulate and restrict the disclosure of data are often bypassed in case of emergencies. This phenomenon, called “break the glass”, is a common pattern in healthcare organizations and, though quite useful and mandatory in emergency situations, from a security perspective, it represents a serious system weakness. Malicious users, in fact, can abuse the system by exploiting the break the glass principle to gain unauthorized privileges and accesses.
29|8||A game-based intrusion detection mechanism to confront internal attackers|Insiders might threaten organizations’ systems any time. By interacting with a system, an insider plays games with the security mechanisms employed to protect it. We apply game theory to model these interactions in an extensive form game that is being played repeatedly with an Intrusion Detection System (IDS). The outcomes of the game are quantified by first specifying players’ preferences, and then, by using the von Neumann–Morgenstern utility function, to assign numbers that reflect these preferences. Examining players’ best responses, the solution of the game follows by locating all the Nash Equilibria (NE). We extend the NE notion to the logit Quantal Response Equilibrium (QRE), to capture players’ bounded rationality and model insider’s behavior. The QRE results are more realistic, and show that the solution of the game might be significantly different than the corresponding NE solution. Thus, we determine how an insider will interact in the future, and how an IDS will react to protect the system. To easily exploit QRE results in ID, we propose the use of a detection mechanism. To present a possible implementation scheme of the detection mechanism, we give the application model and a detailed game-based detection algorithm.
29|8||Classifying data from protected statistical datasets|Statistical Disclosure Control (SDC) is an active research area in the recent years. The goal is to transform an original dataset X into a protected one X′, such that X′ does not reveal any relation between confidential and (quasi-)identifier attributes and such that X′ can be used to compute reliable statistical information about X.
29|8||A method for forensic analysis of control|This paper examines technical underpinnings for the notion of control as identified in laws and regulations in order to provide a technical basis for performing forensic analysis of digital forensic evidence in cases where taking control over systems or mechanisms is the issue.
29|8||IFIP TCII - Aims, Scope and Technical Committee|
29|8||Invitation to IFIP sec 2011|
30|1|http://www.sciencedirect.com/science/journal/01674048/30/1|Contents|
30|1||Editorial|
30|1||Security, technology, publishing, and ethics (part II)|
30|1||WhiteScript: Using social network analysis parameters to balance between browser usability and malware exposure|Drive-by-download malware exposes internet users to infection of their personal computers, which can occur simply by visiting a website containing malicious content. This can lead to a major threat to the user’s most sensitive information. Popular browsers such as Firefox, Internet Explorer and Maxthon have extensions that block JavaScript, Flash and other executable content. Some extensions globally block all dynamic content, and in others the user needs to specifically enable the content for each site (s)he trusts. Since most of the web-pages today contain dynamic content, disabling them damages user experience and page usability, and that prevents many users from installing security extensions. We propose a novel approach, based on Social Network Analysis parameters, that predicts the user trust perspective for the HTML page currently being viewed. Our system examines the URL that appears in the address bar of the browser and each of the inner HTML URL reputations, and only if all of them have a reputation greater than our predetermined threshold, it marks the webpage as trusted. Each URL reputation is calculated based on the number and quality of the links on the whole web pointing back to the URL. The method was examined on a corpus of 44,429 malware domains and on the top 2000 most popular Alexa sites. Our system managed to enable dynamic content of 70% of the most popular websites and block 100% of malware web-pages, all without any user intervention. Our approach can augment most browser security applications and enhance their effectiveness, thus encouraging more users to install these important applications.
30|1||A study of self-propagating mal-packets in sensor networks: Attacks and defenses|Since sensor applications are implemented in embedded computer systems, cyber attacks that compromise regular computer systems via exploiting memory-related vulnerabilities present similar threats to sensor networks. However, the paper shows that memory fault attacks in sensors are not the same as in regular computers due to sensor’s hardware and software architecture. In contrast to worm attacks, mal-code carried by exploiting packets cannot be executed in sensors built upon Harvard architecture. Therefore, the paper proposes a range of attack approaches to illustrate that a mal-packet, which only carries specially crafted data, can exploit memory-related vulnerabilities and utilize existing application code in a sensor to propagate itself without disrupting the sensor’s functionality. The paper shows that such a mal-packet can have as few as 17 bytes. A prototype of a 27-byte mal-packet has been implemented and tested in Mica2 sensors. Simulation shows that the propagation pattern of such a mal-packet in a sensor network is very different from worm propagation. Mal-packets can either quickly take over the whole network or hardly propagate under different traffic situations. The paper also develops two defense schemes (S2Guard and S2Shuffle) based on existing defense techniques to protect sensor applications. The analysis shows that they only incur a little overhead and can stop the propagation of mal-packets.
30|1||An efficient and non-interactive hierarchical key agreement protocol|The non-interactive identity-based key agreement schemes are believed to be applicable to mobile ad-hoc networks (MANETs) that have a hierarchical structure such as hierarchical military MANETs. It was observed by Gennaro et al. (2008) that there is still an open problem on the security of the existing schemes, i.e., how to achieve the desirable security against corrupted nodes in the higher levels of a hierarchy? In this paper, we propose a novel and very efficient non-interactive hierarchical identity-based key agreement scheme that solves the open problem and outperforms all existing schemes in terms of computational efficiency and data storage.
30|1||Designing a cluster-based covert channel to evade disk investigation and forensics|Data confidentiality on a computer can be achieved using encryption. However, encryption is ineffective under a forensic investigation mainly because the presence of encrypted data on a disk can be easily detected and disk owners can subsequently be forced (by law or other means) to release decryption keys. To evade forensic investigation, intelligent information hiding techniques that support plausible deniability have been proposed as an alternative to encryption; plausible deniability allows an evader to hide data in a manner such that he/she can deny the very existence of the data.
30|1||Modeling vulnerability discovery process in Apache and IIS HTTP servers|Vulnerability discovery models allow prediction of the number of vulnerabilities that are likely to be discovered in the future. Hence, they allow the vendors and the end users to manage risk by optimizing resource allocation. Most vulnerability discovery models proposed use the time as an independent variable. Effort-based modeling has also been proposed, which requires the use of market share data. Here, the feasibility of characterizing the vulnerability discovery process in the two major HTTP servers, Apache and IIS, is quantitatively examined using both time and effort-based vulnerability discovery models, using data spanning more than a decade. The data used incorporates the effect of software evolution for both servers. In addition to aggregate vulnerabilities, different groups of vulnerabilities classified using both the error types and severity levels are also examined. Results show that the selected vulnerability discovery models of both types can fit the data of the two HTTP servers very well. Results also suggest that separate modeling for an individual class of vulnerabilities can be done. In addition to the model fitting, predictive capabilities of the two models are also examined. The results demonstrate the applicability of quantitative methods to widely-used products, which have undergone evolution.
30|1||A comparative evaluation of intrusion detection architectures for mobile ad hoc networks|Mobile Ad Hoc Networks (MANETs) are susceptible to a variety of attacks that threaten their operation and the provided services. Intrusion Detection Systems (IDSs) may act as defensive mechanisms, since they monitor network activities in order to detect malicious actions performed by intruders, and then initiate the appropriate countermeasures. IDS for MANETs have attracted much attention recently and thus, there are many publications that propose new IDS solutions or improvements to the existing. This paper evaluates and compares the most prominent IDS architectures for MANETs. IDS architectures are defined as the operational structures of IDSs. For each IDS, the architecture and the related functionality are briefly presented and analyzed focusing on both the operational strengths and weaknesses. Moreover, methods/techniques that have been proposed to improve the performance and the provided security services of those are evaluated and their shortcomings or weaknesses are presented. A comparison of the studied IDS architectures is carried out using a set of critical evaluation metrics, which derive from: (i) the deployment, architectural, and operational characteristics of MANETs; (ii) the special requirements of intrusion detection in MANETs; and (iii) the carried analysis that reveals the most important strengths and weaknesses of the existing IDS architectures. The evaluation metrics of IDSs are divided into two groups: the first one is related to performance and the second to security. Finally, based on the carried evaluation and comparison a set of design features and principles are presented, which have to be addressed and satisfied in future research of designing and implementing IDSs for MANETs.
30|1||IFIP TCII - Aims, Scope and Technical Committee|
30|1||IFIPTM 2011 â Call for Contributions|
30|1||Call for membership|
30|1||WISE 7 - Call for Papers|
30|2-3|http://www.sciencedirect.com/science/journal/01674048/30/2-3|Contents|
30|2-3||Editorial|
30|2-3||Fine-grained integration of access control policies|Collaborative and distributed applications, such as dynamic coalitions and virtualized grid computing, often require integrating access control policies of collaborating parties. Such an integration must be able to support complex authorization specifications and the fine-grained integration requirements that the various parties may have. In this paper, we introduce an algebra for fine-grained integration of sophisticated policies. The algebra, which consists of three binary and two unary operations, is able to support the specification of a large variety of integration constraints. For ease of use, we also introduce a set of derived operators and provide guidelines for users to edit a policy with desired properties. To assess the expressive power of our algebra, we define notion of completeness and prove that our algebra is complete and minimal with respect to the notion. We then propose a framework that uses the algebra for the fine-grained integration of policies expressed in XACML. We also present a methodology for generating the actual integrated XACML policy, based on the notion of Multi-Terminal Binary Decision Diagrams. Experimental results have demonstrated both effectiveness and efficiency of our approach. In addition, we also discuss issues regarding obligations.
30|2-3||Semantic web-based social network access control|The existence of online social networks that include person specific information creates interesting opportunities for various applications ranging from marketing to community organization. On the other hand, security and privacy concerns need to be addressed for creating such applications. Improving social network access control systems appears as the first step toward addressing the existing security and privacy concerns related to online social networks. To address some of the current limitations, we have created an experimental social network using synthetic data which we then use to test the efficacy of the semantic reasoning based approaches we have previously suggested.
30|2-3||Patient-centric authorization framework for electronic healthcare services|In modern healthcare environments, a fundamental requirement for achieving continuity of care is the seamless access to distributed patient health records in an integrated and unified manner, directly at the point of care. However, Electronic Health Records (EHRs) contain a significant amount of sensitive information, and allowing data to be accessible at many different sources increases concerns related to patient privacy and data theft. Access control solutions must guarantee that only authorized users have access to such critical records for legitimate purposes, and access control policies from distributed EHR sources must be accurately reflected and enforced accordingly in the integrated EHRs. In this paper, we propose a unified access control scheme that supports patient-centric selective sharing of virtual composite EHRs using different levels of granularity, accommodating data aggregation and privacy protection requirements. We also articulate and address issues and mechanisms on policy anomalies that occur in the composition of discrete access control policies from different data sources.
30|2-3||Security analysis of GTRBAC and its variants using model checking|Security analysis is a formal verification technique to ascertain certain desirable guarantees on the access control policy specification. Given a set of access control policies, a general safety requirement in such a system is to determine whether a desirable property is satisfied in all the reachable states. Such an analysis calls for the use of formal verification techniques. While formal analysis on traditional Role Based Access Control (RBAC) has been done to some extent, recent extensions to RBAC lack such an analysis. In this paper, we consider the temporal RBAC extensions and propose a formal technique using timed automata to perform security analysis by analyzing both safety and liveness properties. Using safety properties one ensures that something bad never happens while liveness properties show that some good state is also achieved. GTRBAC is a well accepted generalized temporal RBAC model which can handle a wide range of temporal constraints while specifying different access control policies. Analysis of such a model involves a process of mapping a GTRBAC based system into a state transition system. Different reduction rules are proposed to simplify the modeling process depending upon the constraints supported by the system. The effect of different constraints on the modeling process is also studied.
30|2-3||Symbolic reachability analysis for parameterized administrative role-based access control|Role-based access control (RBAC) is a widely used access control paradigm. In large organizations, the RBAC policy is managed by multiple administrators. An administrative role-based access control (ARBAC) policy specifies how each administrator may change the RBAC policy. It is often difficult to fully understand the effect of an ARBAC policy by simple inspection, because sequences of changes by different administrators may interact in unexpected ways. ARBAC policy analysis algorithms can help by answering questions, such as user-role reachability, which asks whether a given user can be assigned to given roles by given administrators.
30|2-3||IFIP TCII - Aims, Scope and Technical Committee|
30|2-3||HAISA - Call for papers|
30|4|http://www.sciencedirect.com/science/journal/01674048/30/4|Contents|
30|4||Editorial|
30|4||Universally composable and customizable post-processing for practical quantum key distribution|In quantum key distribution (QKD), a secret key is generated between two distant parties by transmitting quantum states. Experimental measurements on the quantum states are then transformed to a secret key by classical post-processing. Here, we propose a construction framework in which QKD classical post-processing can be custom made. Though seemingly obvious, the concept of concatenating classical blocks to form a whole procedure does not automatically apply to the formation of a quantum cryptographic procedure since the security of the entire QKD procedure rests on the laws of quantum mechanics and classical blocks are originally designed and characterized without regard to any properties of these laws. Nevertheless, we justify such concept of concatenating classical blocks in constructing QKD classical post-processing procedures, along with a relation to the universal-composability-security parameter. Consequently, effects arising from an actual QKD experiment, such as those due to the finiteness of the number of signals used, can be dealt with by employing suitable post-processing blocks. Lastly, we use our proposed customizable framework to build a comprehensive generic recipe for classical post-processing that one can follow to derive a secret key from the measurement outcomes in an actual experiment.
30|4||Legally âreasonableâ security requirements: A 10-year FTC retrospective|Growth in electronic commerce has enabled businesses to reduce costs and expand markets by deploying information technology through new and existing business practices. However, government laws and regulations require businesses to employ reasonable security measures to thwart risks associated with this technology. Because many security vulnerabilities are only discovered after attacker exploitation, regulators update their interpretation of reasonable security to stay current with emerging threats. With a focus on determining what businesses must do to comply with these changing interpretations of the law, we conducted an empirical, multi-case study to discover and measure the meaning and evolution of “reasonable” security by examining 19 regulatory enforcement actions by the U.S. Federal Trade Commission (FTC) over a 10 year period. The results reveal trends in FTC enforcement actions that are institutionalizing security knowledge as evidenced by 39 security requirements that mitigate 110 legal security vulnerabilities.
30|4||Extending the enforcement power of truncation monitors using static analysis|Runtime monitors are a widely used approach to enforcing security policies. Truncation monitors are based on the idea of truncating an execution before a violation occurs. Thus, the range of security policies they can enforce is limited to safety properties. The use of an a priori static analysis of the target program is a possible way of extending the range of monitorable properties. This paper presents an approach to producing an in-lined truncation monitor, which draws upon the above intuition. Based on an a priori knowledge of the program behavior, this approach allows, in some cases, to enforce more than safety properties and is more powerful than a classical truncation mechanism. We provide and prove a theorem stating that a truncation enforcement mechanism considering only the set of possible executions of a specific program is strictly more powerful than a mechanism considering all the executions over an alphabet of actions.
30|4||User perceptions of security and usability of single-factor and two-factor authentication in automated telephone banking|This paper describes an experiment to investigate user perceptions of the usability and security of single-factor and two-factor authentication methods in automated telephone banking. In a controlled experiment with 62 banking customers a knowledge-based, single-factor authentication procedure, based on those commonly used in the financial services industry, was compared with a two-factor approach where in addition to the knowledge-based step, a one-time passcode was generated using a hardware security token. Results were gathered on the usability and perceived security of the two methods described, together with call completion rates and call durations for the two methods. Significant differences were found between the two methods, with the two-factor version being perceived as offering higher levels of security than the single-factor authentication version; however, this gain was offset by significantly lower perceptions of usability, and lower ratings for convenience and ease of use for the two-factor version. In addition, the two-factor authentication version took longer for participants to complete. This research provides valuable empirical evidence of the trade-off between security and usability in automated systems.
30|4||HMMPayl: An intrusion detection system based on Hidden Markov Models|Nowadays the security of Web applications is one of the key topics in Computer Security. Among all the solutions that have been proposed so far, the analysis of the HTTP payload at the byte level has proven to be effective as it does not require the detailed knowledge of the applications running on the Web server. The solutions proposed in the literature actually achieved good results for the detection rate, while there is still room for reducing the false positive rate.
30|4||Modeling the behavior of users who are confronted with security mechanisms|In this paper, we describe a new approach to analyze the trade-off between usability and security frequently found in security-related user interfaces. The approach involves the simulation of potential user interaction behavior by a mixed probabilistic and rule-driven state machine. On the basis of the simulations, user behavior in security-relevant situations can be predicted and user interfaces optimizing intended behavior can be designed. The approach is evaluated in an artificial microworld setting which provides good control over the experimental factors guiding the behavior. A comparison of empirical and simulated behavior in this microworld shows that the approach is already able to accurately predict important aspects of user behavior toward security interfaces, but also identifies future work necessary to better cover all relevant aspects guiding this behavior in a real-world setting.
30|4||Quantitative analysis of a certified e-mail protocol in mobile environments: A probabilistic model checking approach|Formal analysis techniques, such as probabilistic model checking, offer an effective mechanism for model-based performance and verification studies of communication systems’ behavior that can be abstractly described by a set of rules i.e., a protocol. This article presents an integrated approach for the quantitative analysis of the Certified E-mail Message Delivery (CEMD) protocol that provides security properties to electronic mail services. The proposed scheme employs a probabilistic model checking analysis and provides for the first time insights on the impact of CEMD’s error tolerance on computational and transmission cost. It exploits an efficient combination of quantitative analysis and specific computational and communication parameters, i.e., the widely used Texas Instruments TMS320C55x Family operating in an High Speed Downlink Packet Access (HSDPA) mobile environment, where multiple CEMD participants execute parallel sessions with high bit error rates (BERs). Furthermore, it offers a tool-assistant approach for the protocol designers and analysts towards the verification of their products under varying parameters. Finally, this analysis can be also utilized towards reliably addressing cost-related issues of certain communication protocols and deciding on their cost-dependent viability, taking into account limitations that are introduced by hardware specifications of mobile devices and noisy mobile environments.
30|4||A secure multi-item e-auction mechanism with bid privacy|
30|4||IFIP TCII - Aims, Scope and Technical Committee|
30|4||Call for membership|
30|5|http://www.sciencedirect.com/science/journal/01674048/30/5|Contents|
30|5||Editorial: Advances in network and system security|
30|5||Masquerade mimicry attack detection: A randomised approach|A masquerader is an (often external) attacker who, after succeeding in obtaining a legitimate user’s credentials, attempts to use the stolen identity to carry out malicious actions. Automatic detection of masquerading attacks is generally undertaken by approaching the problem from an anomaly detection perspective: a model of normal behaviour for each user is constructed and significant departures from it are identified as potential masquerading attempts. One potential vulnerability of these schemes lies in the fact that anomaly detection algorithms are generally susceptible to deception. In this work, we first investigate how a resourceful masquerader can successfully evade detection while still accomplishing his goals. For this, we introduce the concept of masquerade mimicry attacks, consisting of carefully constructed attacks that are not identified as anomalous. We then explore two different detection schemes to thwart such attacks. We first study the introduction of a blind randomisation strategy into a baseline anomaly detector. We then propose a more accurate algorithm, called Probabilistic Padding Identification (PPI) and based on the Kullback–Leibler divergence, which attempts to identify if a sufficiently anomalous attack is present within an apparently normal behavioural pattern. Our experimental results indicate that the PPI algorithm achieves considerably better detection quality than both blind randomised strategies and adversarial-unaware approaches.
30|5||A pitfall in fingerprint bio-cryptographic key generation|The core of bio-cryptography lies in the stability of cryptographic keys generated from uncertain biometrics. It is essential to minimize every possible uncertainty during the biometric feature extraction process. In fingerprint feature extraction, it is perceived that pixel-level image rotation transformation is a lossless transformation process. In this paper, an investigation has been conducted on analyzing the underlying mechanisms of fingerprint image rotation processing and potential effect on the major features, mainly minutiae and singular point, of the rotation transformed fingerprint. Qualitative and quantitative analyses have been provided based on intensive experiments. It is observed that the information integrity of the original fingerprint image can be significantly compromised by image rotation transformation process, which can cause noticeable singular point change and produce a non-negligible number of fake minutiae. It is found that the quantization and interpolation process can change the fingerprint features significantly without affecting the visual image. Experiments show that up to 7% bio-cryptographic key bits can be affected due to this rotation transformation.
30|5||Hierarchical attribute-based encryption and scalable user revocation for sharing data in cloud servers|With rapid development of cloud computing, more and more enterprises will outsource their sensitive data for sharing in a cloud. To keep the shared data confidential against untrusted cloud service providers (CSPs), a natural way is to store only the encrypted data in a cloud. The key problems of this approach include establishing access control for the encrypted data, and revoking the access rights from users when they are no longer authorized to access the encrypted data. This paper aims to solve both problems. First, we propose a hierarchical attribute-based encryption scheme (HABE) by combining a hierarchical identity-based encryption (HIBE) system and a ciphertext-policy attribute-based encryption (CP-ABE) system, so as to provide not only fine-grained access control, but also full delegation and high performance. Then, we propose a scalable revocation scheme by applying proxy re-encryption (PRE) and lazy re-encryption (LRE) to the HABE scheme, so as to efficiently revoke access rights from users.
30|5||Injecting purpose and trust into data anonymisation|Data anonymisation is of increasing importance for allowing sharing individual data among various data requesters for a variety of social network data analysis and mining applications. Most existing works of data anonymisation target at the optimization of the anonymisation metrics to balance the data utility and privacy, whereas they ignore the effects of a requester’s trust level and application purposes during the data anonymisation. Our aim of this paper is to propose a much finer level anonymisation scheme with regard to the data requester’s trust and specific application purpose. We firstly prioritize the attributes for anonymisation based on their importance to application purposes. Secondly, we build the projection between the trust value and the degree of data anonymiztion, which intends to determine to what extent the data should be anonymized. The decomposition algorithm is developed to find the desired anonymous solution, which ensures the uniqueness and correctness. Finally, we conduct extensive experiments on two real-world data sets and the results show the benefits of our approach for both data requesters and providers.
30|5||IFIP TCII - Aims, Scope and Technical Committee|
30|6-7|http://www.sciencedirect.com/science/journal/01674048/30/6-7|Contents|
30|6-7||Editorial|
30|6-7||Data preprocessing for anomaly based network intrusion detection: A review|Data preprocessing is widely recognized as an important stage in anomaly detection. This paper reviews the data preprocessing techniques used by anomaly-based network intrusion detection systems (NIDS), concentrating on which aspects of the network traffic are analyzed, and what feature construction and selection methods have been used. Motivation for the paper comes from the large impact data preprocessing has on the accuracy and capability of anomaly-based NIDS. The review finds that many NIDS limit their view of network traffic to the TCP/IP packet headers. Time-based statistics can be derived from these headers to detect network scans, network worm behavior, and denial of service attacks. A number of other NIDS perform deeper inspection of request packets to detect attacks against network services and network applications. More recent approaches analyze full service responses to detect attacks targeting clients. The review covers a wide range of NIDS, highlighting which classes of attack are detectable by each of these approaches.
30|6-7||Logic-based approach for digital forensic investigation in communication Networks|In this paper, we provide a logic for digital investigation of security incidents and its high-level-specification language. The logic is used to prove the existence or non-existence of potential attack scenarios which, if executed on the investigated system, would produce the different forms of specified evidence. To generate executable attack scenarios showing with details how the attack scenario was conducted and how the system behaved accordingly, we develop in this paper a Model Checker tool which provides tolerance to unknown attacks and integrates a technique for hypothetical actions generation
30|6-7||Modeling behavioral considerations related to information security|The authors present experimental and simulation results of an outcome-based learning model for the identification of threats to security systems. This model integrates judgment, decision-making, and learning theories to provide a unified framework for the behavioral study of upcoming threats.
30|6-7||Compliance by design â Bridging the chasm between auditors and IT architects|System and process auditors assure – from an information processing perspective – the correctness and integrity of the data that is aggregated in a company’s financial statements. To do so, they assess whether a company’s business processes and information systems process financial data correctly. The audit process is a complex endeavor that in practice has to rely on simplifying assumptions. These simplifying assumptions mainly result from the need to restrict the audit scope and to focus it on the major risks. This article describes a generalized audit process. According to our experience with this process, there is a risk that material deficiencies remain undiscovered when said simplifying assumptions are not satisfied. To address this risk of deficiencies, the article compiles thirteen control patterns, which – according to our experience – are particularly suited to help information systems satisfy the simplifying assumptions. As such, use of these proven control patterns makes information systems easier to audit and IT architects can use them to build systems that meet audit requirements by design. Additionally, the practices and advice offered in this interdisciplinary article help bridge the gap between the architects and auditors of information systems and show either role how to benefit from an understanding of the other role’s terminology, techniques, and general work approach.
30|6-7||Unconstrained keystroke dynamics authentication with shared secret|Among all the existing biometric modalities, authentication systems based on keystroke dynamics present interesting advantages. These solutions are well accepted by users and cheap as no additional sensor is required for authenticating the user before accessing to an application. In the last thirty years, many researchers have proposed, different algorithms aimed at increasing the performance of this approach. Their main drawback lies on the large number of data required for the enrollment step. As a consequence, the verification system is barely usable, because the enrollment is too restrictive. In this work, we propose a new method based on the Support Vector Machine (SVM) learning satisfying industrial conditions (i.e., few samples per user are needed during the enrollment phase to create its template). In this method, users are authenticated through the keystroke dynamics of a shared secret (chosen by the system administrator). We use the GREYC keystroke database that is composed of a large number of users (100) for validation purposes. We compared the proposed method with six methods from the literature (selected based on their ability to work with few enrollment samples). Experimental results show that, even though the computation time to build the template can be longer with our method (54 s against 3 s for most of the others), its performance outperforms the other methods in an industrial context (Equal Error Rate of 15.28% against 16.79% and 17.02% for the two best methods of the state-of-the-art, on our dataset and five samples to create the template, with a better computation time than the second best method).
30|6-7||PrivaKERB: A user privacy framework for Kerberos|Kerberos is one of the most well-respected and widely used authentication protocols in open and insecure networks. It is envisaged that its impact will increase as it comprises a reliable and scalable solution to support authentication and secure service acquisition in the Next Generation Networks (NGN) era. This means however that security and privacy issues related to the protocol itself must be carefully considered. This paper proposes a novel two-level privacy framework, namely PrivaKERB, to address user privacy in Kerberos. Our solution offers two privacy levels to cope with user anonymity and service access untraceability. We detail how these modes operate in preserving user privacy in both single-realm and cross-realm scenarios. By using the extensibility mechanisms already available in Kerberos, PrivaKERB does not change the semantics of messages and enables future implementations to maintain interoperability. We also evaluate our solution in terms of service time and resource utilization. The results show that PrivaKERB is a lightweight solution imposing negligible overhead in both the participating entities and network.
30|6-7||A survey of certified mail systems provided on the Internet|Over the last several years, an increasing number of certified mail systems have been put into place on the Internet. Governments, postal operators and private businesses now provide value-added electronic services that match the quality of postal certified mail. So far, there is no common view on the security properties that an electronic certified mail system has to provide. This applies to implementers and, surprisingly, also applies to the research community. All certified mail systems provided on the Internet are autonomous, and most are closed systems. However, recent developments call for cross-border certified mail communications that are similar to what we have become accustomed to in e-mail. This demand is emphasized by the ongoing implementation of the European Union (EU) Services Directive. The interoperability of certified mail systems is a new and challenging research field. The aim of this paper is to assess and discuss various standards and certified mail systems deployed on a large scale by drawing on the literature. This will facilitate interoperability efforts by offering a clearer view on the security properties that are actually applied in practice, as opposed to what is in research. We do this by classifying systems according to the security properties defined to date in the literature. Our findings show that standards and systems provided on the Internet have adopted many aspects of postal certified mail with respect to fairness, non-repudiation services and applied trust models. Nevertheless, there are still differences and incompatibilities, and the community must work toward common and interoperable systems. We encourage research into additional properties that could be applied in practice.
30|6-7||Understanding the mindset of the abusive insider: An examination of insidersâ causal reasoning following internal security changes|Employees can have a profound, detrimental influence on information security that costs organizations billions of U.S. dollars annually. As a result, organizations implement stringent security controls, which can inadvertently foster the behaviors that they are designed to deter. This research attempts to understand this phenomenon of increased internal computer abuses by applying causal reasoning theory to explain employees’ causal-search process following the implementation of information security measures. Our findings show how interpersonal and environmental factors influence insiders’ beliefs that the organization trusts them (i.e., attributed trust) and how low attributed trust perceptions drive computer abuse incidents subsequent to security changes. We also highlight the need for both managers and security researchers to assess the frequency with which employees encounter information security changes within dynamic, organizational environments.
30|6-7||Rights violation detection in multi-level digital rights management system|A multi-level digital rights management (DRM) system consists of owner, multiple levels of distributors and consumers. The distributors and consumers are given redistribution and usage rights using redistribution and usage licenses respectively. However, both redistribution and usage licenses can be violated by a malicious distributor/consumer to bypass the permissions and constraints in them. Thus, it is required to detect such violations. In this paper, we deal with the redistribution rights violation using a process called license validation. The problem of validation becomes complex when there exists multiple redistribution licenses for the same content with the distributors. In such cases, large number of comparisons may be needed and it may become difficult in real time in a large and distributed multi-level DRM system. Hence, we propose a bit-vector transform based license organization technique to do the validation efficiently. The proposed license organization method is compatible with the DRM related tasks such as license revocation and constraint range revocation. Mathematical analysis and experimental results show that the proposed license organization technique can do the validation efficiently.
30|6-7||Feature representation and selection in malicious code detection methods based on static system calls|Currently almost all static methods for detecting malicious code are signature-based, this leads the result that viruses can easily escape detection by simple mechanisms such as code obfuscation. In this paper, a behavior-based detection approach is proposed to address this problem. The behaviors of interest are defined as static system call sequences. Unlike the traditional approach, which derives system call sequences by running executables (i.e., dynamic system call sequences), this approach statically analyzes binary code to derive system call sequences. In this paper, a method for deriving static system call sequences is presented, and two automatic feature-selection methods based on n-grams are proposed. We use machine-learning methods, including the K-nearest neighbor, Support Vector Machine, and decision tree methods to classify executables. The proposed approach is compared with the dynamic detection approach using dynamic system call sequences. The experimental results show that the proposed approach has higher accuracy and a lower false positive rate than the dynamic detection approach.
30|6-7||Toward cost-sensitive self-optimizing anomaly detection and response in autonomic networks|While anomaly detection and response play a significant role in attaining auto defense, one of core functionalities of autonomic networks, the design and deployment of Anomaly Detection and Response Systems (ADRS) herein is a non-trivial issue because of the special network characteristic, namely self-managing, which requires candidate ADRS to automatically and optimally balance performance objectives and potential negative consequence. In this paper, we propose a decision-theoretic framework to systematically analyze ADRS in autonomic networks, with an objective to achieve its cost-sensitive and self-optimizing operation. In particular, each ADRS agent is viewed as an autonomous entity, making decision as its local operating environment. A global reward signal is then used to quantify the performance of ADRS as a whole in terms of those identified metrics. Furthermore, the analytical framework serves as a basis for developing an adaptive, robust, and near-optimal prototype termed ARSoS, along with a reinforcement learning algorithm for approximately inferring the optimal behavior of a reputation-based ADRS in a specific autonomic network variant, mobile ad-hoc network. The performance of ARSoS is validated through extensive simulations.
30|6-7||Correlating TCP/IP Packet contexts to detect stepping-stone intrusion|Stepping-stone intrusion is one of the most popular techniques for attacking other computers, and detecting this form of intrusion and resisting intruders’ evasion are critical security issues. In this paper, we propose a new approach to this problem by introducing packet context to help detect stepping-stone intrusion. Pearson product-moment correlation coefficient is introduced to correlate packet context. The proposed approach does not need a threshold, and it is easily implemented. The experimental results show that the proposed approach can detect stepping-stone intrusion and resist intruders’ time-jittering and chaff-perturbation manipulation to an extent.
30|6-7||IFIP TCII - Aims, Scope and Technical Committee|
30|8|http://www.sciencedirect.com/science/journal/01674048/30/8|Contents|
30|8||Editorial for 30/8|
30|8||Distributed Court System for intrusion detection in mobile ad hoc networks|Securing routing layer functions in mobile ad hoc networks is an important issue, which includes many challenges like how to enhance detection accuracy when facing the highly dynamic characteristic of such networks, and how to distinguish malicious accusations under a totally autonomous structure. In this paper, we propose Distributed Court System (DCS), a complete Intrusion Detection System that intends to solve these challenges in a low-cost and robust way. We do not deploy any centralized entity, but rely on the collaboration among the nodes neighbouring the suspected node, to integrate information, improve the detection accuracy, and reject dissemination of malicious accusation. Through mathematical analysis and simulation, the proposed DCS is proved to be effective in a highly mobile and hostile network environment.
30|8||Countering unauthorized code execution on commodity kernels: A survey of common interfaces allowing kernel code modification|Motivated by the goal of hardening operating system kernels against rootkits and related malware, we survey the common interfaces and methods which can be used to modify (either legitimately or maliciously) the kernel which is run on a commodity desktop computer. We also survey how these interfaces can be restricted or disabled. While we concentrate mainly on Linux, many of the methods for modifying kernel code also exist on other operating systems, some of which are discussed.
30|8||Fair digital signing: The structural reliability of signed documents|The exchange of digitally signed data inherits all the problems related to the indeterminacy of human communication, which are further intensified by the legal implications of signing. One of the fundamental intrinsic weaknesses of digital signatures is that the signer creates a signature on a series of bits, which may be differently transformed and perceived by the verifier (or relying party), due to the inevitable differences in the intention and the purpose of the two agents. As a result, syntactic and semantic distance is introduced between a signer and a relying party. In this paper we suggest a framework that models the process of digital signing, using several virtual and interrelated levels of communication, thereby promoting the analytic and synthetic exploration of the entities and the transformations involved. Based on this exploration, it is possible to indicate the favorable conditions for mutual understanding between the signer and the relying party. We focus on the syntactic and presentation levels of the communication process and we introduce the notion of structural reliability of a syntactic component, as a measure of how securely and accurately a signed document can be used. It is argued that structural reliability depends on a quantitative metric, such as the structural informativeness along with other qualitative characteristics of the syntactic component. The structural reliability of several document representation protocols is evaluated and it is concluded that the higher the informativeness of the protocol, the less the semantic distance produced, provided that the communicating parties have the capacity to handle this protocol.
30|8||An analysis of the statistical disclosure attack and receiver-bound cover|Anonymous communications provides an important privacy service by keeping passive eavesdroppers from linking communicating parties. However, an attacker can use long-term statistical analysis of traffic sent to and from such a system to link senders with their receivers. Cover traffic is an effective, but somewhat limited, counter strategy against this attack. Earlier work in this area proposes that privacy-sensitive users generate and send cover traffic to the system. However, users are not online all the time and cannot be expected to send consistent levels of cover traffic; use of inconsistent cover traffic drastically reduces its impact. We propose that the anonymity system generate cover traffic that mimics the sending patterns of users in the system. This receiver-bound cover (RBC) helps to make up for users that aren’t there, confusing the attacker. To study the statistical disclosure attack and different cover traffic methods, we introduce an analytical method to bound the time for an attacker to identify a contact of Alice with high probability. We use these bounds to show that cover traffic sent by Alice greatly increases the time for attacker success, especially as the amount of traffic from other users increases. Further, we show that RBC greatly enhances the defense, forcing the attacker to take additional time proportional to the amount of cover used. We also examine the effectiveness of the attack and cover traffic when the attacker can only observe part of the traffic in the system. We validate our analysis through simulations that extend to realistic social networks. When RBC is used in combination with user generated cover traffic, the attack takes a very long time to succeed.
30|8||Analysis of update delays in signature-based network intrusion detection systems|Network Intrusion Detection Systems (NIDS) play a fundamental role on security policy deployment and help organizations in protecting their assets from network attacks. Signature-based NIDS rely on a set of known patterns to match malicious traffic. Accordingly, they are unable to detect a specific attack until a specific signature for the corresponding vulnerability is created, tested, released and deployed. Although vital, the delay in the updating process of these systems has not been studied in depth. This paper presents a comprehensive statistical analysis of this delay in relation to the vulnerability disclosure time, the updates of vulnerability detection systems (VDS), the software patching releases and the publication of exploits. The widely deployed NIDS Snort and its detection signatures release dates have been used. Results show that signature updates are typically available later than software patching releases. Moreover, Snort rules are generally released within the first 100 days from the vulnerability disclosure and most of the times exploits and the corresponding NIDS rules are published with little difference. Implications of these results are drawn in the context of security policy definition. This study can be easily kept up to date due to the methodology used.
30|8||Swarm intelligence in intrusion detection: A survey|Intrusion Detection Systems (IDS) have nowadays become a necessary component of almost every security infrastructure. So far, many different approaches have been followed in order to increase the efficiency of IDS. Swarm Intelligence (SI), a relatively new bio-inspired family of methods, seeks inspiration in the behavior of swarms of insects or other animals. After applied in other fields with success SI started to gather the interest of researchers working in the field of intrusion detection. In this paper we explore the reasons that led to the application of SI in intrusion detection, and present SI methods that have been used for constructing IDS. A major contribution of this work is also a detailed comparison of several SI-based IDS in terms of efficiency. This gives a clear idea of which solution is more appropriate for each particular case.
30|8||Constant round group key agreement protocols: A comparative study|The scope of this paper is to review and evaluate all constant round Group Key Agreement (GKA) protocols proposed so far in the literature. We have gathered all GKA protocols that require 1,2,3,4 and 5 rounds and examined their efficiency. In particular, we calculated each protocol’s computation and communication complexity and using proper assessments we compared their total energy cost. The evaluation of all protocols, interesting on its own, can also serve as a reference point for future works and contribute to the establishment of new, more efficient constant round protocols.
30|8||A taxonomy of self-modifying code for obfuscation|Self-modifying code is frequently used as an additional layer of complexity when obfuscating code. Although it does not provide a provable level of obfuscation, it is generally assumed to make attacks more expensive. This paper attempts to quantify the cost of attacking self-modified code by defining a taxonomy for it and systematically categorising an adversary’s capabilities. A number of published methods and techniques for self-modifying code are then classified according to both the taxonomy and the model.
30|8||Combining sketches and wavelet analysis for multi time-scale network anomaly detection|With the rapid development and the increasing complexity of computer and communication systems and networks, traditional security technologies and measures can not meet the demand for integrated and dynamic security solutions. In this scenario, the use of Intrusion Detection Systems has emerged as a key element in network security.
30|8||Enforcing privacy in e-commerce by balancing anonymity and trust|Privacy is a major concern in e-commerce. There exist two main paradigms to protect the customer’s privacy: one relies on the customer’s trust that the network will conform to his privacy policy, the other one insists on the customer’s anonymity. A new paradigm is advanced here as a natural balance between these two. It sees the customer act using his real identity but only circulate cover data that conceal the resources he requires. Privacy enforcement is thus shifted from the customer’s identity to his purchase preferences. The new paradigm is suitable for scenarios such as eBay purchases where trust that a network sticks to a privacy policy is problematic, while anonymity is either forbidden or impossible.
30|8||The cyber threat landscape: Challenges and future research directions|Cyber threats are becoming more sophisticated with the blending of once distinct types of attack into more damaging forms. Increased variety and volume of attacks is inevitable given the desire of financially and criminally-motivated actors to obtain personal and confidential information, as highlighted in this paper. We describe how the Routine Activity Theory can be applied to mitigate these risks by reducing the opportunities for cyber crime to occur, making cyber crime more difficult to commit and by increasing the risks of detection and punishment associated with committing cyber crime. Potential research questions are also identified.
30|8||Masquerade detection using profile hidden Markov models|In this paper, we consider the problem of masquerade detection, based on user-issued UNIX commands. We present a novel detection technique based on profile hidden Markov models (PHMMs). For comparison purposes, we implement an existing modeling technique based on hidden Markov models (HMMs). We compare these approaches and show that, in general, our PHMM technique is competitive with HMMs. However, the standard test data set lacks positional information. We conjecture that such positional information would give our PHMM a significant advantage over HMM-based detection. To lend credence to this conjecture, we generate a simulated data set that includes positional information. Based on this simulated data, experimental results show that our PHMM-based approach outperforms other techniques when limited training data is available.
30|8||Roles in information security â A survey and classification of the research area|The concept of roles has been prevalent in the area of Information Security for more than 15 years already. It promises simplified and flexible user management, reduced administrative costs, improved security, as well as the integration of employees’ business functions into the IT administration. A comprehensive scientific literature collection revealed more than 1300 publications dealing with the application of sociological role theory in the context of Information Security up to now. Although there is an ANSI/NIST standard and an ISO standard proposal, a variety of competing models and interpretations of the role concept have developed. The major contribution of this survey is a categorization of the complete underlying set of publications into different classes. The main part of the work is investigating 32 identified research directions, evaluating their importance and analyzing research tendencies. An electronic bibliography including all surveyed publications together with the classification information is provided additionally. As a final contribution potential future developments in the area of role-research are considered.
30|8||Windows driver memory analysis: A reverse engineering methodology|In a digital forensics examination, the capture and analysis of volatile data provides significant information on the state of the computer at the time of seizure. Memory analysis is a premier method of discovering volatile digital forensic information. While much work has been done in extracting forensic artifacts from Windows kernel structures, less focus has been paid to extracting information from Windows drivers. There are two reasons for this: (1) source code for one version of the Windows kernel (but not associated drivers) is available for educational use and (2) drivers are generally called asynchronously and contain no exported functions. Therefore, finding the handful of driver functions of interest out of the thousands of candidates makes reverse code engineering problematic at best. Developing a methodology to minimize the effort of analyzing these drivers, finding the functions of interest, and extracting the data structures of interest is highly desirable. This paper provides two contributions. First, it describes a general methodology for reverse code engineering of Windows drivers memory structures. Second it applies the methodology to tcpip.sys, a Windows driver that controls network connectivity. The result is the extraction from tcpip.sys of the data structures needed to determine current network connections and listeners from the 32 and 64 bit versions of Windows Vista and Windows 7.
30|8||Enable delegation for RBAC with Secure Authorization Certificate|Our motivation in this paper is to explore a Secure Delegation Scheme that could keep access control information hidden through network transmission. This approach introduces the quasirandom structure, 3-Uniform Hypergraph, as the representation structure for authorization information. It generates a Secure Authorization Certificate (SAC) in place of an Attribute Certificate (AC) to enable both Role-based Access Control (RBAC) and a delegation process for hiding authorization information. We have two contributions in this regard: (1) a value-based delegation scheme and (2) a pattern-based RBAC. A Secure Delegation Scheme is based on the hashing values generated with the quasirandom structure. With this scheme, the delegation process will greatly reduce the risk of sensitive authorization information leakage for applications. In the case of pattern-based access, we introduce a new hash function using quasirandom structure to make a fingerprint1 for RBAC. The quasirandom structure derived from k-Uniform Hypergraph has measurable uniformity, which is an advantage over traditional hash functions. Another advantage is that it does not need to access the entire message context to generate the fingerprint which is essential for traditional hash functions such as MD5, SHA-1, etc.
30|8||Estimating botnet virulence within mathematical models of botnet propagation dynamics|Mathematical models of botnet propagation dynamics are increasingly deemed to have potential for significant contribution to botnet mitigation. Botnet virulence, which comprises network vulnerability rate and network infection rate, is a key factor in those models. In this paper we discuss a practical approach that draws on epidemiological models in biology to estimate the botnet virulence in a network. Our research provides mathematical models of botnet propagation dynamics with concrete measures of botnet virulence, which make those models practical and hence employable in mitigation of real world botnets in a timely fashion. The approach is based on random sampling and follows a novel application of statistical learning and inference in a botnet-versus-network setting. We have implemented this research in the Matlab programming language. In this paper, we discuss an experimental evaluation of the effectiveness of this research with respect to botnet propagation dynamics realistically simulated in a GTNetS network simulation platform.
30|8||Cybercrime: Understanding and addressing the concerns of stakeholders|Cybercrime and cybercriminal activities continue to impact communities as the steady growth of electronic information systems enables more online business. The collective views of sixty-six computer users and organizations, that have an exposure to cybercrime, were analyzed using concept analysis and mapping techniques in order to identify the major issues and areas of concern, and provide useful advice. The findings of the study show that a range of computing stakeholders have genuine concerns about the frequency of information security breaches and malware incursions (including the emergence of dangerous security and detection avoiding malware), the need for e-security awareness and education, the roles played by law and law enforcement, and the installation of current security software and systems. While not necessarily criminal in nature, some stakeholders also expressed deep concerns over the use of computers for cyberbullying, particularly where younger and school aged users are involved. The government’s future directions and recommendations for the technical and administrative management of cybercriminal activity were generally observed to be consistent with stakeholder concerns, with some users also taking practical steps to reduce cybercrime risks.
30|8||E2VoIP2: Energy efficient voice over IP privacy|Due to the convergence of telecommunication technologies and pervasive computing, voice is increasingly being transmitted over IP networks, in what is commonly known as Voice over IP (VoIP). Despite many advantages offered by this technology, VoIP applications inherit many challenging characteristics from the underlying IP network related to quality of service and security concerns. Traditional ways to secure data over IP networks have negative effects on real-time applications and on power consumption, which is scarce in power-constrained handheld devices. In this work, a new codec-independent Energy Efficient Voice over IP Privacy (E2VoIP2) algorithm is devised to limit the overhead of the encryption process, without compromising the end-to-end confidentiality of the conversation. The design takes advantage of VoIP stream characteristics to encrypt selected packets using a secure algorithm, while relaxing the encryption procedure in-between these packets. We evaluated experimentally the difficulty of conducting known plaintext attacks on VoIP by demonstrating that a sound recorded simultaneously by different sources results in apparently random encoded files. Regarding E2VoIP2, experimental and simulation results show a substantial improvement in terms of the number of CPU cycles which results in a reduction of latency and a reduction in consumed power with respect to that of the SRTP. In addition, the proposed method is flexible in terms of the balance between security and power consumption.
30|8||An interactive mobile SMS confirmation method using secret sharing technique|
